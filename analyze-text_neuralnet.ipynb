{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing session descriptions\n",
    "- Preprocess descriptions to lowercase, remove punctuation, remove extra whitespaces, remove numbers\n",
    "- Train model to vectorize original session descriptions\n",
    "- Find similarities between my session notes and original descriptions\n",
    "- Train neural network with similarity scores and notes\n",
    "- Use neural network model to write summaries for sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic packages, data wrangling\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sklearn, machine learning\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# skipthoughts seq2seq vectorizer\n",
    "sys.path.append('C:/Users/ffarmer/AppData/Local/Continuum/Anaconda2/Lib/skip-thoughts')\n",
    "import skipthoughts\n",
    "\n",
    "# gensim modules, word vectorization\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# Keras, neural networks\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import TimeDistributedDense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>topic</th>\n",
       "      <th>descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Parallel SQL and analytics with Solr</td>\n",
       "      <td>Analytics has increasingly become a major focu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>JupyterLab: The evolution of the Jupyter Notebook</td>\n",
       "      <td>Project Jupyter provides building blocks for i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Designing a location intelligence platform for...</td>\n",
       "      <td>CartoDB has enabled hundreds of thousands of u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>The future of column-oriented data processing ...</td>\n",
       "      <td>In pursuit of speed and efficiency, big data p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Beyond Hadoop at Yahoo: Interactive analytics ...</td>\n",
       "      <td>Yahoo initially built Hadoop as an answer to a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "1  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "2  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "3  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "4  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "\n",
       "                                               topic  \\\n",
       "0               Parallel SQL and analytics with Solr   \n",
       "1  JupyterLab: The evolution of the Jupyter Notebook   \n",
       "2  Designing a location intelligence platform for...   \n",
       "3  The future of column-oriented data processing ...   \n",
       "4  Beyond Hadoop at Yahoo: Interactive analytics ...   \n",
       "\n",
       "                                        descriptions  \n",
       "0  Analytics has increasingly become a major focu...  \n",
       "1  Project Jupyter provides building blocks for i...  \n",
       "2  CartoDB has enabled hundreds of thousands of u...  \n",
       "3  In pursuit of speed and efficiency, big data p...  \n",
       "4  Yahoo initially built Hadoop as an answer to a...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in pickle file for strata session data\n",
    "sessions = pd.read_pickle('data_train/strata_sessions.pkl')\n",
    "\n",
    "# confirm results\n",
    "sessions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>topic</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Parallel SQL and analytics with Solr</td>\n",
       "      <td>Analytics has increasingly become a major focu...</td>\n",
       "      <td>analytics has increasingly become a major focu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>JupyterLab: The evolution of the Jupyter Notebook</td>\n",
       "      <td>Project Jupyter provides building blocks for i...</td>\n",
       "      <td>project jupyter provides building blocks for i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Designing a location intelligence platform for...</td>\n",
       "      <td>CartoDB has enabled hundreds of thousands of u...</td>\n",
       "      <td>cartodb has enabled hundreds of thousands of u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>The future of column-oriented data processing ...</td>\n",
       "      <td>In pursuit of speed and efficiency, big data p...</td>\n",
       "      <td>in pursuit of speed and efficiency big data pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Beyond Hadoop at Yahoo: Interactive analytics ...</td>\n",
       "      <td>Yahoo initially built Hadoop as an answer to a...</td>\n",
       "      <td>yahoo initially built hadoop as an answer to a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "1  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "2  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "3  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "4  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "\n",
       "                                               topic  \\\n",
       "0               Parallel SQL and analytics with Solr   \n",
       "1  JupyterLab: The evolution of the Jupyter Notebook   \n",
       "2  Designing a location intelligence platform for...   \n",
       "3  The future of column-oriented data processing ...   \n",
       "4  Beyond Hadoop at Yahoo: Interactive analytics ...   \n",
       "\n",
       "                                        descriptions  \\\n",
       "0  Analytics has increasingly become a major focu...   \n",
       "1  Project Jupyter provides building blocks for i...   \n",
       "2  CartoDB has enabled hundreds of thousands of u...   \n",
       "3  In pursuit of speed and efficiency, big data p...   \n",
       "4  Yahoo initially built Hadoop as an answer to a...   \n",
       "\n",
       "                                           documents  \n",
       "0  analytics has increasingly become a major focu...  \n",
       "1  project jupyter provides building blocks for i...  \n",
       "2  cartodb has enabled hundreds of thousands of u...  \n",
       "3  in pursuit of speed and efficiency big data pr...  \n",
       "4  yahoo initially built hadoop as an answer to a...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize list\n",
    "documents = []\n",
    "\n",
    "# pre-processing\n",
    "for doc in sessions['descriptions']:\n",
    "    # remove punction\n",
    "    no_punc = doc.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    \n",
    "    # convert to lower\n",
    "    low = no_punc.lower()\n",
    "    \n",
    "    # add to list\n",
    "    documents.append(low)\n",
    "\n",
    "# add to dataframe\n",
    "sessions['documents'] = documents \n",
    "\n",
    "# confirm results\n",
    "sessions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train, test = train_test_split(documents, test_size=0.3)\n",
    "\n",
    "# add to txt file for model\n",
    "with open('docs_train.txt', 'w') as f:\n",
    "    for d in train:\n",
    "        f.write(d + '\\n')\n",
    "with open('docs_test.txt', 'w') as f:\n",
    "    for d in test:\n",
    "        f.write(d + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train doc2vec vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# custom label line sentence class to read in mulitple document files\n",
    "# append prefix to labels for document titles\n",
    "# source: http://linanqiu.github.io/2015/10/07/word2vec-sentiment/\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data files\n",
    "sources = {'data_train/docs_train.txt':'train', 'data_train/docs_test.txt':'test'}\n",
    "\n",
    "# convert files\n",
    "data = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)\n",
    "\n",
    "# get model vocabulary\n",
    "model.build_vocab(data.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done training\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "for epoch in range(20):\n",
    "    model.train(data.sentences_perm())\n",
    "\n",
    "print 'done training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model, no need to retrain\n",
    "model.save('./strata.d2v')\n",
    "\n",
    "# load model\n",
    "model = Doc2Vec.load('./strata.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'sponsored', 0.5786522030830383),\n",
       " (u'telecommunication', 0.5548757314682007),\n",
       " (u'trusted', 0.5534477829933167),\n",
       " (u'law', 0.492511123418808),\n",
       " (u'hottest', 0.48926758766174316),\n",
       " (u'similarities', 0.4680299460887909),\n",
       " (u'units', 0.45595583319664),\n",
       " (u'comprehensive', 0.45538529753685),\n",
       " (u'refining', 0.4534202218055725),\n",
       " (u'reached', 0.4323597252368927)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how it works, look at similar words\n",
    "model.most_similar('government')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare notes to original summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data_strata/fastforwardlabs_unstrtxtsummary.md', 'r') as myfile:\n",
    "    new_doc_raw = myfile.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove punction\n",
    "no_punc = new_doc_raw.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    \n",
    "# convert to lower\n",
    "low = no_punc.lower()\n",
    "\n",
    "# remove numbers\n",
    "no_num = ''.join([i for i in low if not i.isdigit()])\n",
    "\n",
    "# remove extra whitespaces\n",
    "new_doc = re.sub(' +',' ',no_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('train_105', 0.3945472538471222), ('train_79', 0.26070916652679443), ('train_34', 0.253447026014328), ('train_29', 0.23216590285301208), ('train_134', 0.22700904309749603), ('test_35', 0.20641231536865234), ('train_120', 0.2033594399690628), ('train_113', 0.200046569108963), ('train_43', 0.19725462794303894), ('train_4', 0.1964893341064453)]\n"
     ]
    }
   ],
   "source": [
    "new_doc_vec = model.infer_vector(new_doc)\n",
    "\n",
    "best = model.docvecs.most_similar([new_doc_vec])\n",
    "print best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(articles, scores), (articles_test, scores_test) = function_that_loads_all_the_training_data()\n",
    "\n",
    "articles_vectors = skipthoughts.encode(articles)\n",
    "articles_vectors_test = skipthoughts.encode(articles_test)\n",
    "\n",
    "model = Model()\n",
    "model.add(LSTM(512, input_shape=(max_sentences, 4800), dropout_W=0.3, dropout_U=0.3))\n",
    "model.add(TimeDistributedDense(1))\n",
    "model.compile(loss='mean_absolute_error', optimizer='rmsprop')\n",
    "model.fit(articles_vectors, scores, validation_split=0.10)\n",
    "\n",
    "loss, acc = model.evaluate(articles_vectors_test, scores_test)\n",
    "print('Test loss / test accuracy = {} / {}'.format(loss, acc))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

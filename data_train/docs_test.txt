increasing demand for more and highergranularity data continues to push the boundaries of what is possible to process using big data technologies netflix’s big data platform team manages a highly organized and curated data warehouse in amazon s3 with over 40 petabytes of data at this scale we are reaching the limits of partitioning with thousands of tables and millions of partitions per table
one of the ways to drive enterprise adoption of big data in financial services is to have a central standardized reusable transparent and wellgoverned library of features or metrics that will empower data scientists and business analysts across a range of business problems this is the central idea behind a feature store—a library of documented features for various analyses based on a shared data model that spans a wide variety of data sources resident within a bank’s data lake
apache flink has seen incredible growth during the last year both in development and usage driven to a large extent by the fundamental shift in the enterprise from batch to stream processing a streamingfirst architecture enables continuous processing on data that is continuously produced as it is in most interesting datasets enabling realtime decisions but also a simplified architecture that can subsume batch processing kostas tzoumas dives into the benefits of using flink as the central piece of such architecture in addition kostas covers the latest developments in the project and the future roadmap such as the ability to query the state in the stream processor new libraries eg sql and cep dynamic scaling seamless application and flink updates and integration between batch and streaming which leads to radically simplified architecture and deployment kostas concludes with a sample of what production users of flink are currently achieving with the system
pinterest is a rapidly expanding product that acts as a catalogue of ideas for over 100 million people pinterest’s content contains over 1 billion boards curated from over 50 billion pins one of the roles data scientists fill at pinterest is to understand this rapidly changing user base and content corpus a handy tool for understanding large datasets is to reduce them to smaller datasets via clustering for this application the workflow of a data scientist is to
society is standing at the gates of what promises to be a profound transformation in the nature of work the role of data and the future of the world’s major industries intelligent machines will play a variety of roles in every sector of the economy from the energy supply chain to legal services and manufacturing
today metamarkets processes over 300 billion events per day representing over 100 tb going through a single pipeline built entirely on open source technologies including druid kafka and samza growing to such a scale presents engineering challenges on many levels not just in design but also with operations especially when downtime is not an option
the netflix data platform is constantly evolving but fundamentally it’s an allcloud platform at a massive scale 40 pb and over 700 billion new events per day focused on empowering developers kurt brown dives into the current technology landscape at netflix and offers some thoughts on what the future holds
hadoop platforms can be very effective and efficient at analyzing historical data at scale in minutes or smallerscale data in near real time data lakes provide largescale data processing and storage at low cost but often struggle to deliver realtime analytic response without significant investment in large clusters technologies like apache spark promise to take the hadoop stack beyond batch but even they rely on a “microbatch” approach instead of truly streaming in real time further the complexities associated with development and ongoing management of a data lake that aims to deliver realtime analytic response can be costly and overwhelming
the new world of data exploration using virtual augmented and mixed reality brings powerful new capabilities and challenges for data visualization engineers designers and professionals brad sarsfield demonstrates new input and output capabilities of holographic computing in relation to mixedreality data visualizations with examples that are both visually beautiful and deeply practical
spark’s efficiency and speed can help big data administrators reduce the total cost of ownership tco of their existing clusters this is because spark’s performance advantages allow it to complete processing in drastically shorter batch windows with higher performance per dollar raj krishnamurthy offers a detailed walkthrough of an alternating least squaresbased matrix factorization workload using this methodology raj has been able to improve runtimes by a factor of 222
mariusz gądarowski offers an overview of neptune deepsenseio’s new it platformbased machinelearning experiment management solution for data scientists deepsenseio uses technologies such as theano tensorflow lasagne scikitlearn and apache spark to carry out machinelearning tasks neptune seamlessly integrates with these technologies and makes them easier to use neptune enhances the management of machinelearning tasks such as dependent computational processes code versioning comparing achieved results monitoring tasks and progress sharing infrastructure among teammates and many others
bigquery provides petabytescale data warehousing with consistently high performance for all users however users coming from traditional enterprise data warehousing platforms often have questions about how best to adapt their workloads for bigquery chad jennings explores best practices and integration with bigquery with special emphasis on loading and transforming data for bigquery as well as how bigquery integrates with the rest of the google cloud platform—including apache spark on google cloud dataproc chad also discusses big query’s sql dialect and its ability to handle the industry’s most common queries
moving from batch to streaming involves changing how we think about time streaming data is neither bounded nor typically well ordered in time however to make streaming systems useful and deliver on the promise of lowlatency results we often want to know when we have all the data relevant to emitting a correct aggregation watermarks provide the foundation for making such decisions enabling streaming systems to emit timely correct results when processing outoforder data
businesses today are driven to adopt big data technologies for their analytics for a number of reasons there are new types of data sources that are not handled by the existing data warehouses there is a growth in data velocity and data volumes that becomes prohibitive to process using existing data warehouses or there are different types of analytics not supported by existing infrastructure to name a few
every industry has both proven and potential data lake use cases with enterprise data warehouses edws being rendered ever more inefficient when facing new business needs cloudbased data lakes have been gaining popularity with enterprises looking to cover the technology gap cloud data lakes are purposebuilt to meet the data management requirements of the evolving enterprise landscape
join xiangrui meng and ram sriharsha to discuss the state of spark
two of the hottest topics in analytics are data lakes and big data in the cloud enterprises like asurion services are using hadoop and informatica’s big data management solutions to deliver faster more flexible and more repeatable big data projects by adopting a big data management architecture on top of hadoop enterprises can quickly and flexibly ingest cleanse master govern secure and deliver all types of data onpremises or in the cloud for business data lake initiatives ranging from marketing effectiveness to fraud detection viral shah explains how enterprises like asurion services are leveraging big data management solutions to accelerate enterprise data lake initiatives for business value
using hadoop and other big data technologies the yp analytics application allows advertisers and media and advertising consultants to understand their digital presence and roi richard langlois explains how yellow pages yp used this expertise for an internal use case that delivers realtime analytics with tableau using olap on hadoop and enabled by its stack hdfs parquet hive impala and atscale
at strata  hadoop world 2012 amy o’connor and her daughter danielle dean shared how they learned and built data science skills at nokia at the time amy led the big data team at nokia where danielle was an intern still working on her phd in the past four years the landscape of data science has changed drastically in 2012 most data science skills had to be learned organically today there have been major advances in tools education and the general culture in organizations taking on data science work this year amy and danielle explore how the landscape in the world of data science has changed and explain how to be successful deriving value from data today along the way they outline the innovative methods they’ve used to find and build a data science skill set within their teams and for those in their customer base
jonathon whitton details how prgx is using talend and cloudera to load two million annual client flat files into a hadoop cluster and perform recovery audit services in order to help clients detect find and fix leakage in their procurement and payment processes jonathon also explores how prgx unzips and decrypts transactional data received from customers so it can analyze the data using a joblet in talend shaving hours of time off each job
the largest challenge for deep learning is scalability with a single gpu server it takes hours or days to finish training this doesn’t scale for production service eventually you’ll need distributed training in the cloud google has been working on a largescale neural network in the cloud for years and has now started sharing the power with developers
ai is moving from consumer applications to the enterprise and will soon affect all parts of operations from the customer to the product to the enterprise stephen pratt the ceo of noodleai and former head of watson for ibm gbs presents a shareholder value perspective on why enterprise artificial intelligence eai will be the single largest competitive differentiator in business over the next five years—and what you can do to end up on top
threequarters of firms tell forrester they aspire to be data driven yet less than a third are good at connecting insights to actions that really matter a new generation of digital competitors does not have this problem and stands poised to steal 12 trillion from traditional enterprises by 2020 many of them are presenting at strata but what does this new breed of competitors have in common beyond using technology like hadoop and spark
fifteen years ago webvan spectacularly failed to bring grocery delivery online speculation has been high that the current wave of ondemand grocery delivery startups will meet similar fates jeremy stanley explains why this time the story will be different—data science is the key innovations in mobile applications have paved the way but significant investments in algorithms to optimize efficiency will drive positive unit economics
voltdb promises full acid with strong serializability in a faulttolerant distributed sql platform as well as higher throughput than other systems that promise much less but why should users believe this
the need to quickly acquire process prepare store and analyze data has never been greater the need for performance crosses the big data ecosystem too—from the edge to the server to the analytics software speed matters raghunath nambiar shares a few use cases that have had significant organizational impact where performance was key
sponsored by 
creating productionready analytical pipelines can be a messy errorprone undertaking in the simplest case connecting a workflow of heterogeneous components such as databases feature enrichment and visualization tools programming languages and analytical engines requires maintaining connections between multiple tools and each of these tools is subject to its own development cycle in the case of projects involving big data or analytics over realtime streaming data the difficulties only increase
while traditional methods may be proficient to collect and analyze uniform data utilizing multiple structured and unstructured external data sources can be challenging joe caserta explains how one of the largest membership interests groups in the country makes sense of the influx of information from streaming external data sources this challenge is exciting because aside from collecting data from its 40 million members the  group also needs to monitor digital and traditional interactions cohesively to predict and optimize a member’s path to purchase
choice hotels international is in the midst of a multiyear transformation that is changing key elements of its it enterprise—replacing its monolithic central reservation system with a cloudbased microservicestyle architecture using cassandra as the backend a parallel project is replacing its enterprise data warehouse and reporting systems with an advanced analytics platform based on spark and kafka
birds of a feather bof discussions are a great way to informally network with people in similar industries or interested in the same topics
cartodb has enabled hundreds of thousands of users to visualize their data as beautiful maps to gain insights and share stories however these maps contain information that often can’t be uncovered by visualization alone by applying geospatial statistical methods and machine learning new stories and understanding can be extracted and predictions can be made
uma raghavan explains why you’re about to see companies whose business models depend on using their customers’ data like facebook google and many others scramble to keep up with the flood of new and evolving laws on data privacy whether using your customers’ data buying thirdparty data or mashing it up to make derivative data to better market to customers create better products and services or provide customer support you could be in violation of emerging data privacy laws from around the world that carry stiff fines up to 5 of revenue or even jail time for violations of the use of people’s personal data ultimately using customer data is a balance between what your business needs to do to run efficiently and effectively and what the brand regulatory and legal risks are if you get caught in violation of the law join uma to learn what’s needed to manage your data risk effectively
john akred stephen o’sullivan and julie steele will field a wide range of detailed questions about developing a modern data strategy architecting a data platform and best practices for cdo and its evolving role even if you don’t have a specific question join in to hear what others are asking
the best datadriven companies constantly utilize data at each function of the business one wellknown example uber brought a datadriven approach to the taxi industry using information about where customers are located changing its price based on demand and gathering customer feedback scores to improve customer satisfaction today more and more companies are accessing and operationalizing instant data in a similar way daniel mintz dives into case studies from three companies—thredup twilio and warby parker—that use data to generate sustainable competitive advantages in their industries these companies have three characteristics in common
in a streaming data processing system where data is generally unbounded triggers specify when each stage of computation should emit output with a small language of primitive conditions and multiple ways of combining them triggers provide the flexibility to tailor a streaming pipeline to a variety of use cases and data sources enabling a practitioner to achieve an appropriate balance between accuracy latency and cost some conditions under which one may choose to “fire”—aka trigger output—include after the system believes all data for the current window is processed after at least 1000 elements have arrived for processing when the first of trigger a and trigger b fires or according to trigger a until trigger b fires
in a few short years cloud has risen from an aspirational concept to an enterprise mandate as workloads move to the cloud a new set of concerns in the data center are emerging including data security portability and governance cloudera ceo tom reilly and james powell global cto of nielsen discuss the dynamics of hadoop in the cloud what to consider at the start of the journey and how to implement a solution that delivers flexibility while meeting key enterprise requirements
much of the success of deep learning in recent years can be attributed to scale—bigger datasets and more computing power—but scale can quickly become a problem distributed asynchronous computing in heterogenous environments is complex hard to debug and hard to profile and optimize martin wicke demonstrates how to automate or abstract away such complexity using tensorflow as an example martin covers the sources of complexity for largescale machinelearning systems explains how to mitigate such complexity and touches upon the future avenues for this work where unsurprisingly machine learning will be used to understand and improve machine learning
eharmony has been using machine learning for about eight years during this time eharmony has learned a number of lessons about how to implement machine learning at scale that allow it to rapidly address problems accurately recently more business units have needed datadriven models jonathan morra introduces aloha an open source project that allows the modeling group to quickly deploy typesafe accurate models to production and explores how eharmony creates models with apache spark and how it uses them
launched in late 2015 cigna’s enterprise data lake project is taking the company on a data governance journey using podium as the software platform for its data lake cigna’s data management and governance teams are eliminating silos of activity and creating a common thread of activity—based on a shared platform of metadata—to connect and align activities across technical teams and business processes
for a long time a substantial portion of the data processing that companies did ran as big batch jobs—csv files dumped out of databases log files collected at the end of the day etc but businesses operate in real time and the software they run is catching up rather than processing data only at the end of the day why not react to it continuously as the data arrives this is the emerging world of stream processing
machine data is growing at an exponential rate and a key driver for this growth is the internet of things iot revolution johan bjerke explains how to make use of and find value in the unstructured machine data that plays an important role in the new connected world
trends driving demand for automated machine learning aml include the growing availability of big data through hadoop architecture and the shortage of experienced data scientists successful big data projects require careful consideration of project definition success criteria organizational design and implementation and executives contribute vitally to this process jeremy achin teaches executives how to identify opportunities to optimize their business using machine learning this means radically reducing time to value the total cycle time from data to predictions and broadening the pool of people who can contribute to machinelearning projects without sacrificing quality jeremy also introduces datarobot aml software that supports and reflects these best practices and explains how datarobot interfaces with hdfs yarn apache spark and other key components in a hadoop cluster
to thrive as a datadriven enterprise organizations must train their eye for the best opportunities for analytic impact creative problemsolving through collaboration enables each individual working with data to utilize their strengths and specialized skills hypothesis testing for business impact through analytics requires finding ways to share hypotheses as well as the data assets that are used to test assumptions through data tools like data inventories data catalogs code repositories and data visualization tools
the electrical utility industry an industry accustomed to gathering customer usage data on a monthly basis now has access to a regular stream of data from smart meters and other smart sensors analyzing these new streams of data has given utilities the opportunity to understand their customer usage patterns perform preventative maintenance detect fraud exercise demand management and allocate resources more effectively
whether we’re talking about spam emails merging records or investigating clusters there are many times when having a measure of how alike things are makes them easier to work with you may have unstructured or vague data that isn’t incorporated into your data models eg information from subjectmatter experts who have a sense of whether something is good or bad similar or different melissa santos offers a practical approach to creating a distance metric and validating with business owners that it provides value—providing you with the tools to turn that expert information into numbers you can compare and use to quickly see structures in the data
most data centers and many cloud deployments are statically partitioned into siloed clusters dedicated to running individual datacenterscale applications including web services databases and batchstream processing this static partitioning model limits overall cluster utilization decreases flexibility and poses operational challenges there is an increasing need to integrate big data applications like apache hadoop and apache spark with other data center services like apache cassandra or apache kafka ideally colocating the data with the services that need it
business and franchise users need access to data to generate reports and dashboards perform analytics and create customercentric predictivepersonalization models that assist with managing demand at choice hotel properties but making data available in an accurate timely and reliable manner to anyone who is authorized to consume it is no easy task
defining the challenges outlining the goals identifying the use cases and tracking roi are always important considerations when building a big data strategy but what about greater behindthescenes challenges like security consumer privacy fraud detection governance and financial investment each impacts the business and its brand mastercard’s nick curcuru hosts an interactive fireside chat with anthony dina from dell to explore how the flexibility scalability and agility of hadoop big data solutions allow one of the world’s leading organizations to innovate enable and enhance the customer experience while still expanding emerging opportunities you’ll also have the chance to discuss the most important considerations for driving big data strategies and implementations
with rapid advances in the field of data science and the availability of realtime streaming data the specter of a datadriven dystopia looms larger than ever mainstream media civil rights advocates and watchdog groups of all political persuasions are increasingly questioning the legitimacy of proprietary predictive tools that are widely used in areas from law enforcement to healthcare
in the last decade fire services around the world have started to notice both the challenges and opportunities of our information society more and more information about their operating environment has been made available either as open data or by ingovernment datasharing initiatives which has made clear fire services’ poor information position as well as the resulting potential unnecessary risks their firefighters take the fear in the current information climate is that if a firefighter is injured or killed on the job the investigation might show that the fire service knew in advance but didn’t or couldn’t share vital information that could have prevented the incident
in pursuit of speed and efficiency big data processing is continuing its logical evolution toward columnar execution a number of key big data technologies including kudu ibis and drill have or will soon have inmemory columnar capabilities the solid foundation laid by apache arrow and apache parquet for a shared columnar representation across the ecosystem promises a great future modern cpus achieve higher throughput using simd instructions and vectorization on arrow’s columnar inmemory representation similarly parquet provides storage and io optimized columnar data access using statistics and appropriate encodings for interoperability rowbased encodings csv thrift avro combined with generalpurpose compression algorithms gzip lzo snappy are common but inefficient the arrow and parquet projects define standard columnar representations allowing interoperability without the usual cost of serialization
data science is a process of abstraction in order to explain or to predict a real phenomena the process starts with acquiring and refining the data it then moves between the three layers of abstraction transformations data abstraction visualizations visual abstraction and modeling symbolic abstraction all three layers of abstraction together build a truer or closer representation of the real phenomena
data wrangling has quickly become a hot topic and technology category within the big data analytics industry stakeholders across business and it are hungry to learn the right way to think about applying these new wrangling solutions to the data and analytics efforts of their organization as with any emerging technology the leading question from organizations still learning about data wrangling is “how are other organizations wrangling data and what are the benefits they are realizing” if this question sounds familiar then this is the session for you
finra ingests over 50 billion records of stock market trading data daily and its examiners compliance officers and analysts look at different slices of this data to draw powerful insights on market behavior and to identify anomalies in trading patterns janaki parameswaran and kishore ramachandran explain how finra technology integrates data feeds from disparate systems to provide analytics and visuals for regulating equities options and fixedincome markets
deep learning—the most significant innovation in data science in recent years—presents amazing improvements in the modeling results however most data scientists don’t yet use deep learning for several reasons the relative complexity of customizing deep learning models for their own problems challenges in installing and using the required frameworks and low performance of open source deep learning frameworks on standard cpus
during election season we’re tasked with considering the next four years and comparing platforms across candidates what’s good for the country is good for your data consider what the next four years will look like for your organization how will you lower costs and deliver innovation jack norris reviews the requirements for a winning data platform such as speed scale and agility
since its inception big data solutions have best been known for their ability to master the complexity of the volume variety and velocity of data but as we enter the era of data democratization there’s a new set of concerns to consider mike olson discusses the new dynamics of big data and how a renewed approach focused on where who and why can lead to cuttingedge solutions

analytics has increasingly become a major focus for apache solr the primary search engine in the hadoop stack yonik seeley explores recent apache solr features in the areas of faceting and analytics including parallel sql streaming expressions distributed join and distributed graph queries given the increasing number of apis and techniques that can be brought to bear yonik also covers the tradeoffs of different approaches and strategies for maximizing scalability
project jupyter provides building blocks for interactive and exploratory computing that make science and data science reproducible across over 40 programming languages python julia r etc central to the project is the jupyter notebook a webbased interactive computing platform that allows users to author data and codedriven narratives computational narratives that combine live code equations narrative text visualizations interactive dashboards and other media while the jupyter notebook has proved to be an incredibly productive way of working interactively with code and data it is helpful to decompose notebooks into more primitive building blocks kernels for code execution input areas for typing code markdown cells for composing narrative content output areas for showing results terminals etc
cartodb has enabled hundreds of thousands of users to visualize their data as beautiful maps to gain insights and share stories however these maps contain information that often can’t be uncovered by visualization alone by applying geospatial statistical methods and machine learning new stories and understanding can be extracted and predictions can be made
in pursuit of speed and efficiency big data processing is continuing its logical evolution toward columnar execution a number of key big data technologies including kudu ibis and drill have or will soon have inmemory columnar capabilities the solid foundation laid by apache arrow and apache parquet for a shared columnar representation across the ecosystem promises a great future modern cpus achieve higher throughput using simd instructions and vectorization on arrow’s columnar inmemory representation similarly parquet provides storage and io optimized columnar data access using statistics and appropriate encodings for interoperability rowbased encodings csv thrift avro combined with generalpurpose compression algorithms gzip lzo snappy are common but inefficient the arrow and parquet projects define standard columnar representations allowing interoperability without the usual cost of serialization
yahoo initially built hadoop as an answer to a very acute pain around efficiently storing and processing large volumes of data since yahoo open sourced hadoop it has become widely adopted in the technology world however time has taught us that when a system becomes extremely popular for solving one class of problems its limitations in solving other problems become more apparent himanshu gupta explains why yahoo has been increasingly investing in interactive analytics and how it leverages druid to power a variety of internal and externalfacing data applications
the netflix data platform is constantly evolving but fundamentally it’s an allcloud platform at a massive scale 40 pb and over 700 billion new events per day focused on empowering developers kurt brown dives into the current technology landscape at netflix and offers some thoughts on what the future holds
whether we’re talking about spam emails merging records or investigating clusters there are many times when having a measure of how alike things are makes them easier to work with you may have unstructured or vague data that isn’t incorporated into your data models eg information from subjectmatter experts who have a sense of whether something is good or bad similar or different melissa santos offers a practical approach to creating a distance metric and validating with business owners that it provides value—providing you with the tools to turn that expert information into numbers you can compare and use to quickly see structures in the data
the new world of data exploration using virtual augmented and mixed reality brings powerful new capabilities and challenges for data visualization engineers designers and professionals brad sarsfield demonstrates new input and output capabilities of holographic computing in relation to mixedreality data visualizations with examples that are both visually beautiful and deeply practical
finra ingests over 50 billion records of stock market trading data daily and its examiners compliance officers and analysts look at different slices of this data to draw powerful insights on market behavior and to identify anomalies in trading patterns janaki parameswaran and kishore ramachandran explain how finra technology integrates data feeds from disparate systems to provide analytics and visuals for regulating equities options and fixedincome markets
data lineage is critical to answering a wide range of questions about how data is being used within an organization which datasets and table columns are driving key performance indicators how is certain privacysensitive data being used where do errors or outliers arise and how do they propagate forward where are inefficient or unnecessary processing steps being taken tracking data lineage is also critical in realworld use cases such as regulatory reporting and compliance
which suppliers are most likely to have delivery or quality issues does service product placement or price make the biggest difference in customer sentiment finding the answers to these questions in structured data is often straightforward but can we answer them using the unstructured data free text in emails social media call center transcripts product reviews and other sources
visual analysis is changing in the era of gpu clusters now that scale compute is easier the bottleneck is mapping data to visualizations and intelligently interacting with them using datasets uploaded to graphistry leo meyerovich provides a glimpse into the emerging workflows for graph and linked event analysis and offers common tricks for success
kudu is redefining the big data ecosystem and opening doors to capabilities not available before comcast is moving in the direction of adopting kudu with impala and spark for several projects including realtime processing of events from xfinity devices sridhar alla and kiran muglurmath explain how realtime analytics on comcast xfinity settop boxes stbs help drive several customerfacing and internal datascienceoriented applications and how comcast uses kudu to fill the gaps in batch and realtime storage and computation needs allowing comcast to process the highspeed data without the elaborate solutions needed till now
for a long time a substantial portion of the data processing that companies did ran as big batch jobs—csv files dumped out of databases log files collected at the end of the day etc but businesses operate in real time and the software they run is catching up rather than processing data only at the end of the day why not react to it continuously as the data arrives this is the emerging world of stream processing
moving from batch to streaming involves changing how we think about time streaming data is neither bounded nor typically well ordered in time however to make streaming systems useful and deliver on the promise of lowlatency results we often want to know when we have all the data relevant to emitting a correct aggregation watermarks provide the foundation for making such decisions enabling streaming systems to emit timely correct results when processing outoforder data
in a streaming data processing system where data is generally unbounded triggers specify when each stage of computation should emit output with a small language of primitive conditions and multiple ways of combining them triggers provide the flexibility to tailor a streaming pipeline to a variety of use cases and data sources enabling a practitioner to achieve an appropriate balance between accuracy latency and cost some conditions under which one may choose to “fire”—aka trigger output—include after the system believes all data for the current window is processed after at least 1000 elements have arrived for processing when the first of trigger a and trigger b fires or according to trigger a until trigger b fires
time series and event data form the basis for realtime insights about the performance of businesses such as ecommerce the iot and web services but gaining these insights involves designing a learning system that scales to millions and billions of data streams ira cohen outlines a system that performs realtime machine learning and analytics on streams at massive scale
modern cars produce data lots of data and formula 1 cars produce more than their fair share ted dunning presents a demo of how data streaming can be applied to the analytics problems posed by modern motorsports although he won’t be bringing formula 1 cars to the talk ted demonstrates a highfidelity physicsbased automotive simulator to produce realistic data from simulated cars running on the spafrancorchamps track as ted moves data from the cars to the pits to the engineers back at hq the result is near realtime visualization and comparison of performance and a great exposition of how to move data using messaging systems like kafka
fifteen years ago webvan spectacularly failed to bring grocery delivery online speculation has been high that the current wave of ondemand grocery delivery startups will meet similar fates jeremy stanley explains why this time the story will be different—data science is the key innovations in mobile applications have paved the way but significant investments in algorithms to optimize efficiency will drive positive unit economics
to anticipate who will succeed and invest wisely investors spend a lot of time trying to understand the longerterm trends within an industry in a panel discussion toptier vcs look over the horizon and consider the big trends in big data explaining what they think the field will look like a few years or more down the road join us to hear about the trends that everyone is seeing and areas for investment that they find exciting
threequarters of firms tell forrester they aspire to be data driven yet less than a third are good at connecting insights to actions that really matter a new generation of digital competitors does not have this problem and stands poised to steal 12 trillion from traditional enterprises by 2020 many of them are presenting at strata but what does this new breed of competitors have in common beyond using technology like hadoop and spark
leading companies that are getting the most out of their data are not focusing on queries and data lakes they are actively integrating analytics into their operations with a datafirst application development approach realtime adjustments to improve revenues reduce costs or mitigate risk rely on applications that minimize latency on a variety of data sources jack norris reviews best practices for three use cases in admedia financial services and healthcare to show how customers develop deploy and dynamically update these applications and how this datafirst approach is fundamentally different from traditional applications
the best datadriven companies constantly utilize data at each function of the business one wellknown example uber brought a datadriven approach to the taxi industry using information about where customers are located changing its price based on demand and gathering customer feedback scores to improve customer satisfaction today more and more companies are accessing and operationalizing instant data in a similar way daniel mintz dives into case studies from three companies—thredup twilio and warby parker—that use data to generate sustainable competitive advantages in their industries these companies have three characteristics in common
at the start of the crimean war in 1853 britain’s royal navy needed 90 new gunboats ready to fight in the baltic in just 90 days assembling the boats was straightforward the challenge was to build all of the engine sets in time marine engineer john penn did an unusual thing he took a pair of reference engines disassembled them and distributed the pieces to the best machine shops across britain these workshops—latterday microservices—each built 90 sets of their allocated parts which were then assembled into the engines for the new gunboats ready for battle
enterprises that pursue datadriven operations and decisions are approaching the conclusion that graph analysis capabilities will yield critical competitive advantages however for this impact to be fully realized the results of any graph analysis must be available in real time to operational applications data scientists and developers across the enterprise
while all other industries have embraced the digital era healthcare seems to be still playing catchup in this kaiser permanente is an anomaly kaiser permanente is a leader in healthcare technology—technology has been at center stage in kaiser permanente since it first started using computing to improve healthcare results in the 1960s today kaiser permanente is an integrated health care delivery system with 10 million members and about 200000 employees
billions of visa cards are used around the world to make payments each payment transaction has a story getting payments from point a to point b is complex and the resulting data visa captures reflects this the scale and complexity of that data is a direct manifestation of the number variety and complexity of payment transactions processed by the visa network
last year linkedin embarked on an ambitious mission to completely revamp the mobile experience for its members this would mean a completely new mobile application reimagined user experiences and new interaction concepts as the team evaluated the impact of this big rewrite on the data analytics ecosystem they observed a few problems
the world is producing an everincreasing volume velocity and variety of big data consumers and businesses are demanding uptothesecond or even millisecond analytics on their fastmoving data in addition to classic batch processing the hadoop ecosystem and aws provide a plethora of tools for solving big data problems but what tools should you use why and how
today hadoop is deployed onpremises and in the public cloud with public cloud increasingly becoming more prevalent the cloud provides some unique abilities including ondemand infrastructure cluster elasticity persisted globally available object storage and payforuse pricing which enables even more flexible and costefficient deployment options for bi and sql analytic users of impala but brings in new challenges that need to be carefully considered to achieve optimal outcome
from data exploration to prototyping to final delivery anyone engaged in using data science for social good knows that the path from project kickoff to delivery is full of exciting twists and turns jeancarlo bonilla susan sun and caitlin augustin explore how datakind volunteer teams navigate the road to social impact by automating evidence collection for conservationists and helping expand the reach of mobile surveys so that more voices can be heard
every industry has both proven and potential data lake use cases with enterprise data warehouses edws being rendered ever more inefficient when facing new business needs cloudbased data lakes have been gaining popularity with enterprises looking to cover the technology gap cloud data lakes are purposebuilt to meet the data management requirements of the evolving enterprise landscape
as the momentum with which big data drives decisions continues to grow the relevant legal considerations relating to collection and use of such data will also increase and evolve understanding the legal frameworks applicable to collection and use of data for certain purposes is key to making compliancebased business decisions and maintaining longterm consumer trust in a brand
with rapid advances in the field of data science and the availability of realtime streaming data the specter of a datadriven dystopia looms larger than ever mainstream media civil rights advocates and watchdog groups of all political persuasions are increasingly questioning the legitimacy of proprietary predictive tools that are widely used in areas from law enforcement to healthcare
how are users meant to interpret the influence of big data and personalization in their targeted experiences what signals show how your data is used and how it improves or constrains your experience to what degree is this experience based on coarse demographics or the entire data profile of your browsing history sara watson explains that in order to develop normative opinions to shape policy and practice users need means to guide their experience—the personalization spectrum
enterprises are increasingly demanding realtime analytics and insights tony ng offers an overview of pulsar an open source realtime streaming system used at ebay which can scale to millions of events per second with 4gl sqllike language support pulsar provides realtime sessionization multidimensional metrics aggregation over time windows and custom stream creation through data enrichment filtering and stateful processing tony explains how pulsar integrates kafka kylin and druid to provide flexibility and scalability in event and metrics consumption
the landscape for storing your big data is quite complex with several competing formats and different implementations of each format picking the best data format depends on what kind of data you have and how you plan to use it depending on your use case different formats perform very differently although you can use a hammer to drive a screw it isn’t fast or easy to do so owen o’malley outlines the performance differences between formats in different use cases and offers an overview of the advantages and disadvantages of each to help you improve the performance of your applications
when it comes to sqlonhadoop it is easy to feel overwhelmed with the number of choices available in tools file formats schema design and configurations however in reality making good design choices when you start will help you avoid some of the common pitfalls marcel kornacker and mostafa mokhtar simplify the process and cover top performance optimizations for apache impala incubating from schema design and memory optimization to query tuning
most data centers and many cloud deployments are statically partitioned into siloed clusters dedicated to running individual datacenterscale applications including web services databases and batchstream processing this static partitioning model limits overall cluster utilization decreases flexibility and poses operational challenges there is an increasing need to integrate big data applications like apache hadoop and apache spark with other data center services like apache cassandra or apache kafka ideally colocating the data with the services that need it
ever since its creation hdfs has relied on data replication to shield against most failure scenarios however with the explosive growth in data volume replication is getting quite expensive the default 3x replication scheme incurs a 200 overhead in storage space and other resources eg network bandwidth when writing the data erasure coding ec uses far less storage space while still providing the same level of fault tolerance under typical configurations ec reduces the storage cost by 50 compared with 3x replication
several big data graph processing frameworks that have been built to run on large graph datasets have been proposed and are in use at large corporations for applications ranging from social network analysis to machine learning to the pagerank algorithm however these libraries can also be put to work to study the nature of cancer
we’ve seen significant progress in infrastructure for using data effectively in the last halfdecade but this hasn’t applied to all types of data equally unstructured text in particular has been slower to yield to the kinds of analysis that many businesses are starting to take for granted rather than being limited by what we can collect we are now constrained by the tools time and techniques to make good use of it but we are beginning to gain the ability to do remarkable things with unstructured text data
as healthcare data becomes more digitized the opportunity to leverage electronic medical records prescription data medical billings hospital and other healthcare datasets to help improve health outcomes and lower the cost of care for patients in near real time is becoming a possibility however processing terabytes and petabytes of deidentified healthcare data requires the application of complex and everchanging business rules this impacts the ability to generate nearrealtime insights and conduct research studies that could potentially influence how patients are treated
handling heterogeneous and concurrent query workloads in a multiuser environment is a common use scenario for bi analytics over sqlonhadoop systems properly deploying a sqlonhadoop cluster that provides the best performance in such an environment requires extensive knowledge of the workloads overall resource utilization database table design software stack configurations and hardware settings an improperly planned deployment can lead to an underutilized cluster wasting company assets or failing to meet performance requirements in one realworld example a company deployed an 80node cluster however their workloads and data volume required less than half of the nodes to meet their performance requirement this means more than half of the nodes sat doing nothing but waiting to be depreciated in another a company used more expensive ssds even though at the software level operations were single threaded and bottlenecked by cpu rather than io—thus hdds might have been a better choice for deployment of course in some scenarios ssds might improve the overall query execution time by more than 50 so there is really no onesizefitsall solution
running realtime dataintensive applications on apache hadoop requires complex architectures to store and query data typically involving multiple independent systems that are tied together through customengineered pipelines a common pattern is to use a nosql engine like apache hbase for caching and later transformations the results of which are periodically written to hdfs in one of the popular open columnar file formats as a prerequisite for querying by a sql engine
one of the ways to drive enterprise adoption of big data in financial services is to have a central standardized reusable transparent and wellgoverned library of features or metrics that will empower data scientists and business analysts across a range of business problems this is the central idea behind a feature store—a library of documented features for various analyses based on a shared data model that spans a wide variety of data sources resident within a bank’s data lake
zillow pioneered providing access to unprecedented information about the housing market long gone are the days when you needed an agent to get comparables and prior sale and listing data enter zillow the nation’s numberone real estate website and mobile app with more data data science has enabled more use cases jasjeet thind explores zillow’s big data platform discusses some of its core machinelearning algorithms and outlines best practices for scaling streaming data ingestion and data processing in spark
bas geerdink offers an overview of the evolution that the hadoop ecosystem has taken at ing since 2013 ing has invested heavily in a central data lake and data management practice bas shares historical lessons and best practices for enterprises that are incorporating hadoop into their infrastructure landscape
ram sriharsha reviews major developments in apache spark 20 and discusses future directions for the project to make spark faster and easier to use for a wider array of workloads with an emphasis on api evolution singlenode performance project tungsten phase 3 and structured streaming
in the world of distributed computing spark has simplified development and opened the doors for many to start writing distributed programs folks with little to no distributed coding experience can now write just a couple lines of code that will immediately get hundreds or thousands of machines working on creating business value
spark’s efficiency and speed can help big data administrators reduce the total cost of ownership tco of their existing clusters this is because spark’s performance advantages allow it to complete processing in drastically shorter batch windows with higher performance per dollar raj krishnamurthy offers a detailed walkthrough of an alternating least squaresbased matrix factorization workload using this methodology raj has been able to improve runtimes by a factor of 222
praveen murugesan explains how uber leverages hadoop and spark as the cornerstones of its data infrastructure praveen details the current data architecture at uber and outlines some of the unique challenges with data processing uber faced as well as its approach to solving some key issues in order to continue to power uber’s realtime marketplace
swisscom the leading mobile service provider in switzerland also provides datadriven intelligence through the analysis of the data created by its mobile network its mobility insights team works to help civil administrators tourism and marketing professionals and many others understand the flow of people through their locations of interest françois garillot outlines the platform tooling and choices that help achieve this service and some challenges the team has faced before exploring in depth the task of understanding the speeds of populations through a path of interest
apache spark has been growing in deployments for the past two years the increasing amount of data being analyzed and processed through the framework is massive and it continues to push the boundaries of the engine drawing on his experiences across 150 production deployments neelesh srinivas salian focuses on five common issues observed in a cluster environment setup with apache spark core streaming and sql to help you improve the usability and supportability of apache spark and avoid such issues in future deployments
despite widespread adoption machinelearning models remain mostly black boxes making it very difficult to understand the reasons behind a prediction such understanding is fundamentally important to assess trust in a model before we take actions based on a prediction or choose to deploy a new ml service such understanding further provides insights into the model which can be used to turn an untrustworthy model or prediction into a trustworthy one
eharmony has been using machine learning for about eight years during this time eharmony has learned a number of lessons about how to implement machine learning at scale that allow it to rapidly address problems accurately recently more business units have needed datadriven models jonathan morra introduces aloha an open source project that allows the modeling group to quickly deploy typesafe accurate models to production and explores how eharmony creates models with apache spark and how it uses them
pinterest is a rapidly expanding product that acts as a catalogue of ideas for over 100 million people pinterest’s content contains over 1 billion boards curated from over 50 billion pins one of the roles data scientists fill at pinterest is to understand this rapidly changing user base and content corpus a handy tool for understanding large datasets is to reduce them to smaller datasets via clustering for this application the workflow of a data scientist is to
predicting which stories will become popular is an invaluable tool for newsrooms but people access their news using a variety of different platforms and sites so identifying what stories are likely to be popular is a challenge very few studies have addressed the popularity of news articles specifically although others have looked at predicting the popularity of other types of online content like tweets and videos the reasons why a particular story becomes popular are varied and might involve contemporariness writing quality and other latent factors euihong han and shuguang wang explain how the washington post predicts what stories on its site will be popular with readers and share the challenges they faced in developing the tool and metrics on how they refined the tool to increase accuracy
recurrent neural networks rnn and related models represent the state of the art in language modeling trained on sufficiently large corpora rnns can learn to generate convincing text that obeys rules of syntax and even matches parentheses what is especially remarkable is that these models can synthesize text from diverse inputs text in another language for translation images and video for captioning and scores and categories for creation of personalized product reviews
much of the success of deep learning in recent years can be attributed to scale—bigger datasets and more computing power—but scale can quickly become a problem distributed asynchronous computing in heterogenous environments is complex hard to debug and hard to profile and optimize martin wicke demonstrates how to automate or abstract away such complexity using tensorflow as an example martin covers the sources of complexity for largescale machinelearning systems explains how to mitigate such complexity and touches upon the future avenues for this work where unsurprisingly machine learning will be used to understand and improve machine learning
creating productionready analytical pipelines can be a messy errorprone undertaking in the simplest case connecting a workflow of heterogeneous components such as databases feature enrichment and visualization tools programming languages and analytical engines requires maintaining connections between multiple tools and each of these tools is subject to its own development cycle in the case of projects involving big data or analytics over realtime streaming data the difficulties only increase
topics include
every day analysts at citi standard chartered bank scb and polaris analyze transactions and behaviors for indicators of money laundering fraud and human trafficking the most comprehensive investigation must leverage massive volumes of data from financial institutions law enforcement and government agencies social media sites telecommunication organizations and enterprises but it often comes with varied data quality standards or in unusable formats and nonstandard structures analysts ability to explore clean shape and integrate the data can’t be slowed down by timeconsuming user compute cycles or resourceintensive etl processes and the entire flow—from source data to usable information—must be auditable and fully trusted
trends driving demand for automated machine learning aml include the growing availability of big data through hadoop architecture and the shortage of experienced data scientists successful big data projects require careful consideration of project definition success criteria organizational design and implementation and executives contribute vitally to this process jeremy achin teaches executives how to identify opportunities to optimize their business using machine learning this means radically reducing time to value the total cycle time from data to predictions and broadening the pool of people who can contribute to machinelearning projects without sacrificing quality jeremy also introduces datarobot aml software that supports and reflects these best practices and explains how datarobot interfaces with hdfs yarn apache spark and other key components in a hadoop cluster
to thrive as a datadriven enterprise organizations must train their eye for the best opportunities for analytic impact creative problemsolving through collaboration enables each individual working with data to utilize their strengths and specialized skills hypothesis testing for business impact through analytics requires finding ways to share hypotheses as well as the data assets that are used to test assumptions through data tools like data inventories data catalogs code repositories and data visualization tools
hadoop platforms can be very effective and efficient at analyzing historical data at scale in minutes or smallerscale data in near real time data lakes provide largescale data processing and storage at low cost but often struggle to deliver realtime analytic response without significant investment in large clusters technologies like apache spark promise to take the hadoop stack beyond batch but even they rely on a “microbatch” approach instead of truly streaming in real time further the complexities associated with development and ongoing management of a data lake that aims to deliver realtime analytic response can be costly and overwhelming
massively parallel big data platforms are quickly becoming the industry standard for organizations looking to extract greater value from data as architectures have shifted application development paradigms have also changed to reflect growing needs for agility scale robustness efficiency and ease of collaboration on these new platforms
data is a company’s lifeblood and more data exists than ever before—in more disparate silos getting the insights you need sifting through data and answering new questions have all been complex hairy tasks that only data jocks have been able to do the entire process is slow and daunting and businesses are never satisfied with the outcome andrew yeung and scott anderson explore new ways to challenge the status quo through automated data blending and smart data discovery across diverse sources to speed insights for business users see and hear about real customer use cases and learn how to reinvent your organization’s analytics capability
when building your data stack the architecture could be your biggest challenge yet it could also be the best predictor for success with so many elements to consider and no proven playbook where do you begin to assemble best practices for a scalable data architecture ben sharma offers lessons learned from the field to get you started if you are concerned with building a data architecture that will serve you now and scale for the future this is a mustattend session
building running and governing a data lake and production data applications on hadoop is often a difficult process filled with slow development cycles and painful operations not only are traditional development tools and techniques missing from the hadoop ecosystem but mastering data ingestion and data integration as well as enterprise governance and security has become a formidable challenge when building big data solutions the challenge only increases as the hadoop ecosystem continues to grow use cases mature slas intensify and services become customer facing and revenue generating and while the it organization owns the task of mitigating these issues more importantly it also has an opportunity to enable the business to reduce time to insights and make better decisions faster by providing them with a modern selfservice environment for their data
more and more frequently owners of hadoop deployments find themselves facing the challenge of supporting data science ecosystems like python and r both adjacent to and within their hadoop infrastructure although these technologies promise powerful data science insights they can also be complex to manage and deploy as people build out data science sandboxes and production environments they discover a number of challenges ranging from basic package management and data lineage to reproducibility and governance of data science artifacts
the last 10 years have seen hadoop move from storage cost killer to contender for the next doordie platform in financial services fintech organizations have used hadoop for building advanced scientific operational data stores data warehousing and reporting consumer application development data visualization and realtime processing but what’s the point at which hadoop tips from a swissarmy knife of use cases to a new foundation that rearranges how the financial services marketplace turns data into profit and competitive advantage this panel of expert practitioners looks into the near future to see if the inflection point is at hand
the emerging industrial internet of things is giving rise to what is predicted to be a sweeping change that will fundamentally transform industries and reconfigure the technology landscape sensor data is expected to dwarf the data volumes that defined the first decade of big data and leading companies will be those that effectively derive value from this next wave of information and opportunity yet the challenges for enterprises remain formidable the information required to enable breakthrough insights is typically fragmented within the domains of information technology it and operational technology ot requiring both technical and cultural changes further organizations are realizing that analytics on sensor data is vastly more diverse and complex than analyzing traditional big datasets like weblogs cheryl wiebe explores how leading companies harness the iot by putting iot data in context fostering collaboration between it and ot and enabling a new breed of scalable analytics
two of the hottest topics in analytics are data lakes and big data in the cloud enterprises like asurion services are using hadoop and informatica’s big data management solutions to deliver faster more flexible and more repeatable big data projects by adopting a big data management architecture on top of hadoop enterprises can quickly and flexibly ingest cleanse master govern secure and deliver all types of data onpremises or in the cloud for business data lake initiatives ranging from marketing effectiveness to fraud detection viral shah explains how enterprises like asurion services are leveraging big data management solutions to accelerate enterprise data lake initiatives for business value
a study by hpe’s security unit found that 70 percent of popular consumer iot devices are easily hackable many of the simplest iot or machinetomachine devices lack adequate processing power and storage to host endpoint security software as a result attackers can exploit vulnerabilities in consumer devices and mobile applications to gain remote access to internal networks and expose users to maninthemiddle attacks
jonathon whitton details how prgx is using talend and cloudera to load two million annual client flat files into a hadoop cluster and perform recovery audit services in order to help clients detect find and fix leakage in their procurement and payment processes jonathon also explores how prgx unzips and decrypts transactional data received from customers so it can analyze the data using a joblet in talend shaving hours of time off each job
the trend of deploying hadoop on virtual infrastructure is rapidly increasing martin yip explores the benefits of virtualizing hadoop through the lens of three realworld examples and walks you through the basics of virtualizing hadoop the first step in providing hadoop on the public or private cloud
big data is being thrown around as a silverbullet solution to enable organizational agility and transformation all your data in one place sounds great on paper but is it really thomas place explores the big data journey of the world’s biggest payment processor which came dangerously close to building a data swamp before pivoting to embrace governance and qualityfirst patterns this case study includes patterns partners successes failures and lessons learned to date and reviews the journey ahead
big data and analytics is a team sport empowering companies of all kinds to achieve business outcomes faster and with greater levels of success dell emc has integrated all the key components for modern digital transformation taking you on a big data journey that focuses on analytics integration and infrastructure its portfolio provides the flexibility to buy or build your analytics ecosystem and offerings range from servers and data lakes to flexible analytics with a turnkey development platform carey james explains how the formation of dell technologies and dell emc can help you on your data analytics journey and how you can turn actionable insights into new business opportunities
the flux capacitor was the core component that made time travel possible in back to the future processing garbage as a power source did you know that you can achieve the same affect in machine learning ingo mierswa rapidminer’s cofounder and ceo offers a case study on how he took “garbage” data drawn from 250k data scientist rapidminer users and through machine learning turned it into wisdom of crowds which helps novice and expert data scientists alike accelerate the creation of their predictive models by delivering expert recommendations about what other scientists would do at every step in their predictive analytics process ingo covers the most frequently used machinelearning techniques what data preparation most experts perform before modeling and how those behaviors have changed over time along with other interesting patterns
a major challenge in today’s big data world is getting data into a data lake in a simple automated way many organizations use python or another language to code their way through these processes but when the number of data sources increases into the hundreds—or often thousands—coding scripts for each source becomes time consuming and extremely difficult to manage and maintain
defining the challenges outlining the goals identifying the use cases and tracking roi are always important considerations when building a big data strategy but what about greater behindthescenes challenges like security consumer privacy fraud detection governance and financial investment each impacts the business and its brand mastercard’s nick curcuru hosts an interactive fireside chat with anthony dina from dell to explore how the flexibility scalability and agility of hadoop big data solutions allow one of the world’s leading organizations to innovate enable and enhance the customer experience while still expanding emerging opportunities you’ll also have the chance to discuss the most important considerations for driving big data strategies and implementations
as software becomes more free and open it also is becoming more complex and expensive to operate how can we in the open source community clarify best practices and recommended operations to model complex interconnected services so users can focus on their ideas how can we as developers deliver recommended best practices in our applications so users are free to focus on the science on their choice of substrate eg laptop cloud or bare metalx86 arm ppc64el or s390x
customers are looking to extend the benefits beyond big data with the power of the deep learning and accelerated analytics ecosystems jim mchugh explains how customers are leveraging deep learning and accelerated analytics to turn insights into aidriven knowledge and covers the growing ecosystem of solutions and technologies that are delivering on this promise such as the nvidia dgx1 which integrates power of deep learning and accelerated analytics together in a single hardware and software system
inchip analytics blasted onto the scene a few years back impressing companies with its ability to significantly impact how companies handle complex data both large and disparate with less hardware while eliminating the data preparation nightmare but the real impact of inchip analytics is only now being realized as companies exploit inchip’s extensibility scaleability and flexibility to take business intelligence to the next level with new iot and ai technologies guy levyyurista explains the unexpected consequences of making big data processing significantly more agile than ever before and the impact it’s having on human insight consumption
legacy enterprise data warehouse edw architecture geared toward daytoday workloads associated with operational querying reporting and analytics is often illequipped to handle the volume of data or traffic and varied data types associated with modern ad hoc analytics platforms faced with the challenges of increasing pipeline speed aggregation and visualization in a simplified selfservice fashion organizations are considering newer technologies such as apache spark hadoop kafka and columnar databases as key enabling technologies to optimize their edw architectures
data wrangling has quickly become a hot topic and technology category within the big data analytics industry stakeholders across business and it are hungry to learn the right way to think about applying these new wrangling solutions to the data and analytics efforts of their organization as with any emerging technology the leading question from organizations still learning about data wrangling is “how are other organizations wrangling data and what are the benefits they are realizing” if this question sounds familiar then this is the session for you
machine data is growing at an exponential rate and a key driver for this growth is the internet of things iot revolution johan bjerke explains how to make use of and find value in the unstructured machine data that plays an important role in the new connected world
businesses are clamoring to capture all data possible and harness it as a revenue driver the challenge is bringing the data together companies that can capture and harness this data can benefit accordingly
businesses today are driven to adopt big data technologies for their analytics for a number of reasons there are new types of data sources that are not handled by the existing data warehouses there is a growth in data velocity and data volumes that becomes prohibitive to process using existing data warehouses or there are different types of analytics not supported by existing infrastructure to name a few
strata  hadoop world program chairs roger magoulas doug cutting and alistair croll welcome you to the first day of keynotes
since its inception big data solutions have best been known for their ability to master the complexity of the volume variety and velocity of data but as we enter the era of data democratization there’s a new set of concerns to consider mike olson discusses the new dynamics of big data and how a renewed approach focused on where who and why can lead to cuttingedge solutions
during election season we’re tasked with considering the next four years and comparing platforms across candidates what’s good for the country is good for your data consider what the next four years will look like for your organization how will you lower costs and deliver innovation jack norris reviews the requirements for a winning data platform such as speed scale and agility
susan woodward discusses venture outcomes—what fraction make lots of money which just barely return capital and which fraction fail completely susan uses updated figures on the fraction of entrepreneurs who succeed including some interesting details on female founders of venture companies
the power of artificial intelligence and advanced analytics emerges from the ability to analyze and compute large disparate datasets from varied devices and locations such as predictive medicine and automated cars at lightningfast speed these realtime insights require continued innovation to fuel the changing landscape of ai martin hall explains why collaboration and openness are the key elements driving innovation in ai
it is no surprise that reducing operational it expenditures and increasing software capabilities is a top priority for large enterprises given its advantages open source software has proliferated across the globe while there’s been much discussion on open source versus commercial cios and ctos at global 1000 enterprises are increasingly interested in solutions that blend the benefits of both however key challenges must be overcome enterprise architecture groups are now faced with the difficult task of selecting the right open source software components from an evergrowing set of options figuring out how to integrate them and ensuring they work together
how do we discover what we’re not looking for how can we become more serendipitous in the age of big data and bioinformatics such questions are more relevant than ever we develop new tools to help us spot clues in mountains of information and machines are getting better and better at aiding discovery and yet serendipity remains a very human art pagan kennedy discusses the origins of the word serendipity and qualities of mind that lead to successful searches in the deep unknown
it is clear that we’re at a critical inflection point in the industry—organizations are realizing that they must quickly adapt in order to keep pace in today’s ever changing digital economy data your most precious commodity is increasing at an alarming rate at the same time an emerging business imperative has made this data a component of your deepest insights allowing you to focus on your business outcomes patricia florissi explains why the recent formation of dell emc ensures that your analytics capabilities will be stronger than ever
healthcare a 3 trillion industry is ripe for disruption through data science however there are many challenges in the journey to make healthcare a truly transparent consumercentric datadriven industry sriram vishwanath explains where data science can have massive impact in healthcare and dispels myths where its apparent use contradicts realities within the healthcare ecosystem
american politics is adrift in a sea of polls this year that sea is deeper than ever before—and darker data science is upending the public opinion industry but to what end in a brief illustrated history of the field jill lepore demonstrates how pollsters rose to prominence by claiming that measuring public opinion is good for democracy and asks “but what if it’s bad”
quench your thirst with vendorhosted libations and snacks while you check out all the exhibitors in the expo hall it’s also a great time to meet and mingle with fellow attendees and strata  hadoop world speakers and authors
birds of a feather bof discussions are a great way to informally network with people in similar industries or interested in the same topics
calling all data enthusiasts
sponsored by 
increasing demand for more and highergranularity data continues to push the boundaries of what is possible to process using big data technologies netflix’s big data platform team manages a highly organized and curated data warehouse in amazon s3 with over 40 petabytes of data at this scale we are reaching the limits of partitioning with thousands of tables and millions of partitions per table
tyler akidau offers a whirlwind tour of the evolution of massivescale data processing at google from the original mapreduce paradigm to the highlevel pipelines of flume to the streaming approach of millwheel to the portable unified streamingbatch model of google cloud dataflow and apache beam incubating tyler examines in detail the basic architectural concepts that underlie these four models highlights their similarities contrasts their differences particularly regarding traditional batch versus streaming and provides insight into the use cases the drove the progression of the designs to what exists today he also highlights similarities and differences with related open source systems such as flink spark storm and gearpump calling out ways in which they’re converging on and diverging from the beam model and what that means when running beam pipelines on their respective runners
today metamarkets processes over 300 billion events per day representing over 100 tb going through a single pipeline built entirely on open source technologies including druid kafka and samza growing to such a scale presents engineering challenges on many levels not just in design but also with operations especially when downtime is not an option
in the last decade fire services around the world have started to notice both the challenges and opportunities of our information society more and more information about their operating environment has been made available either as open data or by ingovernment datasharing initiatives which has made clear fire services’ poor information position as well as the resulting potential unnecessary risks their firefighters take the fear in the current information climate is that if a firefighter is injured or killed on the job the investigation might show that the fire service knew in advance but didn’t or couldn’t share vital information that could have prevented the incident
cluster computing frameworks such as hadoop or spark are tremendously beneficial in processing and deriving insights from data however long query latencies make these frameworks suboptimal choices to power interactive applications organizations frequently rely on dedicated query layers such as relational databases and keyvalue stores for faster query latencies but these technologies suffer many drawbacks for analytic use cases
airbnb developed caravel to provide all employees with interactive access to data while minimizing friction caravel provides a quick way to intuitively visualize datasets by allowing users to create and share interactive dashboards a rich set of visualizations to analyze your data as well as a flexible way to extend the capabilities an extensible highgranularity security model allowing intricate rules on who can access which features and integration with major authentication providers database openid ldap oauth and remoteuser through flask appbuilder a simple semantic layer allowing you to control how data sources are displayed in the ui by defining which fields should show up in which dropdown and which aggregation and function metrics are made available to the user and deep integration with druid that allows for caravel to stay blazing fast while working with large realtime datasets
uma raghavan explains why you’re about to see companies whose business models depend on using their customers’ data like facebook google and many others scramble to keep up with the flood of new and evolving laws on data privacy whether using your customers’ data buying thirdparty data or mashing it up to make derivative data to better market to customers create better products and services or provide customer support you could be in violation of emerging data privacy laws from around the world that carry stiff fines up to 5 of revenue or even jail time for violations of the use of people’s personal data ultimately using customer data is a balance between what your business needs to do to run efficiently and effectively and what the brand regulatory and legal risks are if you get caught in violation of the law join uma to learn what’s needed to manage your data risk effectively
data science is a process of abstraction in order to explain or to predict a real phenomena the process starts with acquiring and refining the data it then moves between the three layers of abstraction transformations data abstraction visualizations visual abstraction and modeling symbolic abstraction all three layers of abstraction together build a truer or closer representation of the real phenomena
ai is moving from consumer applications to the enterprise and will soon affect all parts of operations from the customer to the product to the enterprise stephen pratt the ceo of noodleai and former head of watson for ibm gbs presents a shareholder value perspective on why enterprise artificial intelligence eai will be the single largest competitive differentiator in business over the next five years—and what you can do to end up on top
data should be something you can see feel hear taste and touch cameron turner brad sarsfield hanna kangbrown and evan macmillan cover the emerging field of sensory data visualization including data sonification in an anecdotal survey they explore reallife examples of solutions deployed to production in industries spanning from consumer goods to heavy industrial and largescale manufacturing to the iot that take advantage of auditory touch and other senses as alternative means of what has traditionally been called data visualization they then investigate the hypothesis that we might better consume information by moving beyond words numbers and pictures and start using sound smell and even taste as a means to better understand the state of the world topics will tie into cameron’s recent interview on the  which focused on data sonification extending these topics into the future of sensory data collection and consumption
agility is king in the world of finance and a messagedriven architecture is a mechanism for building and managing discrete business functionality to enable agility in order to accommodate rapid innovation data pipelines must evolve however implementing microservices can create management problems like the number of instances running in an environment
to manage the everincreasing volume and velocity of data within your company you may have successfully made the transition from single machines and oneoff solutions to large distributed stream infrastructures in your data center powered by apache kafka but what’s to be done if one data center is not enough
digital consumer companies are disrupting the old guard and changing the way we do business in fundamental ways for example uber airbnb and zipcar have disrupted the traditional businesses of taxis hotels and car rental companies by leveraging software capabilities to create new business models opportunities in the industrial world are expected to outpace consumer business cases time series data is growing exponentially as new machines around the world get connected venkatesh sivasubramanian and luis ramos explain how ge makes it faster and easier for systems to access using a common layer and perform analytics on a massive volume of time series data by taking what they’ve learned from apache arrow and applying it today for highly efficient time series storage using apache apex spark and kudu
iot and financial trading platforms share some commonality they intercept massive amounts of sensor or event data and must provide insights and actions in real time the technology challenge is huge since we need to combine fast event streams historical state and consistent data update transactions eg time series data and statistical aggregators with data science and machine learning and present results through realtime dashboards or drive immediate corrective actions
recent years have seen significant evolution of the internet of things it has become increasingly easy to connect devices to the internet and send sensory data to the public cloud  however the adoption of iot platforms and stream analytics within the enterprise is lagging and less prevalent an effect of the lack of skilled developers required to deploy an onpremises platform and the limited demonstration of high value in reallife use cases
the history of the digital age is being written in photographs today for better or worse everyone is a both a photographer and a subject we need to start thinking about visual content in a radically different way as both organizations and individuals to innovate in the visual age we have to crack the visual code this means learning as much as we can not only about how we see but also about how computers see so we can teach them to discover hidden opportunities and disregard hidden biases if we try hard enough maybe they’ll teach us to do the same
otto is the world’s secondlargest online retailer in a highly competitive market space superior customer experience in terms of higher empathy relevance and speed is key to positive customer experience and this is where ai comes into play rupert steffner explores the cornerstones retailers have to focus when building their customers’ experience on artificial intelligence it starts with having clear goals and a value system that finds the right balance between customer retention and revenue optimization even if ai is hereby built from the seller’s perspective retailers will need a “good ai” approach that treats consumers fairly and as partners to optimize longterm customer equity
at strata  hadoop world 2012 amy o’connor and her daughter danielle dean shared how they learned and built data science skills at nokia at the time amy led the big data team at nokia where danielle was an intern still working on her phd in the past four years the landscape of data science has changed drastically in 2012 most data science skills had to be learned organically today there have been major advances in tools education and the general culture in organizations taking on data science work this year amy and danielle explore how the landscape in the world of data science has changed and explain how to be successful deriving value from data today along the way they outline the innovative methods they’ve used to find and build a data science skill set within their teams and for those in their customer base
most people will agree that interviewing is one of the most difficult and least enjoyable professional activities given the recent demand for data analytics and data science skills it has become an increasingly daunting task for managers to adequately test and qualify candidates
the electrical utility industry an industry accustomed to gathering customer usage data on a monthly basis now has access to a regular stream of data from smart meters and other smart sensors analyzing these new streams of data has given utilities the opportunity to understand their customer usage patterns perform preventative maintenance detect fraud exercise demand management and allocate resources more effectively
machinelearning tools promise to help solve data curation problems while the principles are well understood the engineering details in configuring and deploying ml techniques are the biggest hurdle ihab ilyas explains why leveraging data semantics and domainspecific knowledge is key in delivering the optimizations necessary for truly scalable ml curation solutions
twitter generates billions and billions of events per day analyzing these events in real time presents a massive challenge karthik ramasamy offers an overview of the endtoend realtime stack twitter designed in order to meet this challenge consisting of distributedlog the distributed and replicated messaging system and heron the streaming system for realtime computation
many initiatives for running applications inside containers have been scoped to run on a single host using docker containers for largescale production environments poses interesting challenges especially when deploying distributed big data applications like apache hadoop and apache spark
the hearst corporation monitors trending content on all of its 300 sites worldwide providing metrics to editors and promoting crossplatform content sharing to facilitate this hearst has built a clickstream analytics pipeline entirely in the cloud that transmits and processes over 30 tb of data a day
society is standing at the gates of what promises to be a profound transformation in the nature of work the role of data and the future of the world’s major industries intelligent machines will play a variety of roles in every sector of the economy from the energy supply chain to legal services and manufacturing
according to the identity theft resource center database more than 169068506 records were exposed in 2015 that stolen data has a much longer shelf life than most realize and will be used to continue the cycle of theft deception and fraud through one of the fastestgrowing and most lucrative businesses for criminals account takeover ato attacks
traditional security tools like security information and event managers siems are struggling to keep up with the terabytes of event data 250m to 2b events being generated each day from an evergrowing number of devices cybersecurity has become a data problem and enterprises need to reply with scalable solutions to enable effective hunting and combat evolving attacks rethinking the cybersecurity problem as a datacentric problem led accenture labs’s cybersecurity team to use emerging big data tools along with new approaches such as graph databases and analysis to exploit the connected nature of the data to its advantage joshua patterson michael wendt and keith kraus explain how accenture labs’s cybersecurity team is using apache kafka spark and flink to stream data into blazegraph and datastax graph to accelerate cyber defense
kafka developed at linkedin in 2010 was originally an open system to encourage adoption developers could easily create new data streams add data to the pipeline and read data as it was created it succeeded brilliantly at encouraging developers to build new data applications improved the reliability of systems and applications and helped linkedin scale its logging and data infrastructure
hadoop in the cloud is becoming an increasingly common use case as the cloud provides rapid access to flexible and lowcost it resources similar to traditional onpremises hadoop clusters data authorization becomes more crucial than ever for the multitenant cloud a transparent solution that decouples compute and storage is required for a simple and smooth transition and since the underlying data is shared across the components a unified authorization policy should be enforced to adapt the flexibility of hadoop ecosystem
apache kudu was first announced as a public beta release at strata nyc 2015 and recently reached 10 this conference marks its one year anniversary as a public open source project todd lipcon offers a very brief refresher on the goals and feature set of the kudu storage engine covering the development that has taken place over the last year including new features such as improved support for time series workloads performance improvements spark integration and highly available replicated masters along the way todd explores realworld production deployments and some of the tools that have been built to help operators manage a kudu cluster he ends with a view of the road map of the kudu project for the upcoming year including plans for security and other new features
at pg the global business services organization delivers many shared services including the core data infrastructure and applications from data warehouses to business intelligence and advanced analytics terry mcfadden and priyank patel discuss procter and gamble’s threeyear journey to enable production applications with oncluster bi technology exploring in detail the architecture challenges and choices made by the team along this journey evaluation criteria and how the final choice arcadia fit with the big data infrastructure
since its introduction in spark 14 sparkr has received contributions from both the spark community and the r community xiangrui meng explores recent community efforts to extend sparkr for scalable advanced analytics—including summary statistics singlepass approximate algorithms and machinelearning algorithms ported from spark mllib—and shows how to integrate existing r packages with sparkr to accelerate existing r workflows
deep learning—the most significant innovation in data science in recent years—presents amazing improvements in the modeling results however most data scientists don’t yet use deep learning for several reasons the relative complexity of customizing deep learning models for their own problems challenges in installing and using the required frameworks and low performance of open source deep learning frameworks on standard cpus
many areas of applied machine learning require models optimized for rare occurrences such as class imbalances and users actively attempting to subvert the system adversaries the data innovation lab at capital one has explored advanced modeling techniques for just these challenges the lab’s use case necessitated that it survey the many related fields that deal with these issues and perform many of the suggested modeling techniques it has also introduced a few novel variations of its own
choice hotels international is in the midst of a multiyear transformation that is changing key elements of its it enterprise—replacing its monolithic central reservation system with a cloudbased microservicestyle architecture using cassandra as the backend a parallel project is replacing its enterprise data warehouse and reporting systems with an advanced analytics platform based on spark and kafka
modern data science is the creative application of scientific principles to design new tools and processes in areas where a scientific approach has been previously infeasible due to the difficulty or expense of collecting data that’s a mouthful but if you see data science that way we’re likely just at the beginning the people and things that are starting to be equipped with sensors will create data that will enable entirely new classes of problems to be approached more scientifically
data visualizations are interactive stories that can powerfully engage audiences giving them insight into the meanings and trends behind numbers they often begin with a massive spreadsheet of data that has no meaning to the average person building an effective visualization begins by asking how data can be transformed into a compelling narrative and a dynamic user experience
apache flink has seen incredible growth during the last year both in development and usage driven to a large extent by the fundamental shift in the enterprise from batch to stream processing a streamingfirst architecture enables continuous processing on data that is continuously produced as it is in most interesting datasets enabling realtime decisions but also a simplified architecture that can subsume batch processing kostas tzoumas dives into the benefits of using flink as the central piece of such architecture in addition kostas covers the latest developments in the project and the future roadmap such as the ability to query the state in the stream processor new libraries eg sql and cep dynamic scaling seamless application and flink updates and integration between batch and streaming which leads to radically simplified architecture and deployment kostas concludes with a sample of what production users of flink are currently achieving with the system
the largest challenge for deep learning is scalability with a single gpu server it takes hours or days to finish training this doesn’t scale for production service eventually you’ll need distributed training in the cloud google has been working on a largescale neural network in the cloud for years and has now started sharing the power with developers
amazon kinesis is a fully managed cloudbased service for realtime data processing over large distributed data streams customers who use amazon kinesis can continuously capture and process realtime data such as iot sensor data website clickstreams financial transactions social media feeds it logs locationtracking events and more roy benalta explores the amazon kinesis platform in detail and discusses best practices for scaling your core streaming data ingestion pipeline as well as realworld customer use cases and design pattern integration with amazon elasticsearch aws lambda and apache spark
structured streaming is a new effort in apache spark to make stream processing simple without the need to learn a new programming paradigm or system ram sriharsha offers an overview of structured streaming discussing its support for eventtime outoforderdelayed data sessionization and integration with the batch data stack to show how it simplifies building powerful continuous applications
there is a growing trend to use modern advanced technology in the finance industry information is often obtained on much larger scales in various modalities and from multiple dimensions which greatly enriches the profiles of financial entities and leads to a rapid increase in the complexity of financial analytics in the meantime there’s increasing demand for automating the process of data statistics feature engineering and model tuning
streaming machine learning is being integrated in spark 21 but you don’t need to wait holden karau and seth hendrickson demonstrate how to do streaming machine learning using spark’s new structured streaming and walk you through creating your own streaming model holden and seth will also cover how to use structured machinelearning algorithms if they are merged by the talk by the end of this session you’ll have a better understanding of spark’s structured streaming api as well as how machine learning works in spark
business and franchise users need access to data to generate reports and dashboards perform analytics and create customercentric predictivepersonalization models that assist with managing demand at choice hotel properties but making data available in an accurate timely and reliable manner to anyone who is authorized to consume it is no easy task
most people are surprised to know that spark works with java maybe they saw the initial java code that used anonymous classes and dismissed it as an ungainly mess they were right—plain java and spark are ugly together then java 8’s lambdas came along now instead of an ungainly mess we get the tight syntax of lambda expressions offering code that is readable and testable best of all it uses java
deep learning has taken us a few steps further toward achieving ai for a manmachine interface however deep learning technologies like speech recognition and natural language processing remain a mystery to many yishay carmiel reviews the history of deep learning the impact it’s made recent breakthroughs interesting solved and open problems and what’s in store for the future
a textmining system must go way beyond indexing and search to appear truly intelligent first it should understand language beyond keyword matching for example distinguishing between “jane has the flu” “jane may have the flu” “jane is concerned about the flu “jane’s sister has the flu but she doesn’t” or “jane had the flu when she was 9” is of critical importance this is a natural language processing problem second it should “read between the lines” and make likely inferences even if they’re not explicitly written for example if jane has had a fever a headache fatigue and a runny nose for three days not as part of an ongoing condition then she likely has the flu this is a semisupervised machinelearning problem third it should automatically learn the right contextual inferences to make for example learning on its own that fatigue is sometimes a flu symptom—only because it appears in many diagnosed patients—without a human ever explicitly stating that rule this is an associationmining problem which can be tackled via deep learning or via more guided machinelearning techniques
political analysts may once have depended entirely on subjective attributes such as ethics charisma and nonscientific impressions of the electorate to forecast elections but with the rise of data generated from human daily interaction with software systems it’s possible to add meaningful datadriven attributes to political forecasting alongside all of the demographic information available to today’s political consultants
predictive maintenance is about anticipating a failure and taking preemptive action with the recent advances in accessible machine learning and cloud storage there is tremendous opportunity to utilize the entire gamut of data coming from factories buildings machines and sensors to not only monitor the health of equipment but also predict when it is likely to malfunction or fail however as simple as it sounds in principle in reality the data required to actually make a prediction in advance and in a timely manner is hard to come by the data that is collected is often incomplete partial or just not enough making it unsuitable for modeling
for ma speculators intellectual property managers and attorneys the “holy grail” for patents is to be able to accurately assess the value of an individual invention and aggregate up to portfolios ignoring the softer side of a patent’s value ie its power to halt competitive development andor neutralize the threat of enforcement there are few liquidity events—specifically litigation damagessettlements portfolio acquisitions and licensing fees—that allow assignees to monetize individual patents none of these typically make the associated data available to the research community
martin wicke and josh gordon field questions related to their tutorial deep learning with tensorflow ask them about tensorflow machine learning and deep learning in particular deploying ml in products and their favorite resources to learn more about ml or any other question you might have
mark grover jonathan seidman and ted malaska the authors of emhadoop application architecturesem participate in an open qa session on considerations and recommendations for the architecture and design of applications using hadoop come with questions about your use case and its big data architecture or just listen in on the conversation
join apache beam and google cloud dataflow engineers to ask all of your questions about stream processing they’ll answer everything from general streaming questions about concepts semantics capabilities limitations etc to questions specifically related to apache beam google cloud dataflow and other common streaming systems flink spark storm etc
join xiangrui meng and ram sriharsha to discuss the state of spark
john akred stephen o’sullivan and julie steele will field a wide range of detailed questions about developing a modern data strategy architecting a data platform and best practices for cdo and its evolving role even if you don’t have a specific question join in to hear what others are asking
voltdb promises full acid with strong serializability in a faulttolerant distributed sql platform as well as higher throughput than other systems that promise much less but why should users believe this
using hadoop and other big data technologies the yp analytics application allows advertisers and media and advertising consultants to understand their digital presence and roi richard langlois explains how yellow pages yp used this expertise for an internal use case that delivers realtime analytics with tableau using olap on hadoop and enabled by its stack hdfs parquet hive impala and atscale
we all hear stories about the potential value of big data analytics but it’s important to understand that big data analytics is a journey of introspection operational excellence and new winning strategies john morrell leads a panel of practitioners from dell national instruments and citi—companies that are gaining real value from big data analytics—as they explore their companies’ big data journeys offering lessons learned and best practices join john to hear realworld stories from organizations that are seeing concrete results and learn how analytics can answer groundbreaking new questions about business and create a path to becoming a datadriven organization
haoyuan li offers an overview of alluxio formerly tachyon a memoryspeed virtual distributed storage system the alluxio open source community is one of the fastest growing open source communities in big data history with more than 250 developers from over 50 organizations around the world and the alluxio system has been deployed at a number of companies including alibaba baidu barclays intel huawei and qunar in some of these deployments alluxio has been running in production for over a year managing pbs of data
operational data stores ods serve as a data staging area between transactional databases and data warehouses data from multiple sources are integrated cleansed and prepped in the ods before populating a data warehouse for longterm storage and analytics traditional ods systems encounter severe challenges when it comes to dealing with the wide variety and massive volume of data common to data warehouses built on top of the hadoop platform it’s time to rethink the requirements and the architecture for the next generation of an ods on top of hadoop starting from first principles vinayak borkar defines the requirements for a modern operational data store and explores some possible architectures to support those requirements
the amount of cuttingedge technology that azure puts at your fingertips is incredible tasks like building a web server or integration workflow that used to take weeks can now be accomplished in seconds and bundling azure services allows developers to create solutions that even a few months ago would have seemed out of reach artificial intelligence is no exception azure enables sophisticated capabilities in artificial intelligence machine learning deep learning cognitive services and advanced analytics rimma nehme explains why azure is the next ai supercomputer and how this vision is being implemented in reality along the way rimma explains how to use azure to process data at any scale and how to compose tools such as spark and r and do scaleout querying on demand at massive scale
bigquery provides petabytescale data warehousing with consistently high performance for all users however users coming from traditional enterprise data warehousing platforms often have questions about how best to adapt their workloads for bigquery chad jennings explores best practices and integration with bigquery with special emphasis on loading and transforming data for bigquery as well as how bigquery integrates with the rest of the google cloud platform—including apache spark on google cloud dataproc chad also discusses big query’s sql dialect and its ability to handle the industry’s most common queries
organizations from small startups to large enterprises are increasingly using open source frameworks such as apache hadoop spark and presto to address a broad range of analytic use cases including business intelligence stream processing and machine learning however with any big data project comes the risk of uncapped costs delayed timelines expensive infrastructure and difficult choices about where to focus in the open source toolset
 
jonathan fritz explains how organizations are deploying these and other big data frameworks with amazon web services aws and how you too can quickly and securely run spark and presto on aws jonathan demonstrates how to lower costs and accelerate deployment of big data applications using amazon emr to easily create a hadoop cluster running spark and presto and querying data in amazon s3 using ansi sql jonathan then explores how you can use amazon s3 as a highly scalable durable and secure data lake by decoupling compute from storage before outlining best practices to lower costs using amazon ec2 spot instances and discussing how to secure your clusters using aws’s extensive security capabilities
join max shron former consultant on data science and current head of warby parker’s data science team for a qa all about data science consulting bring your questions about getting into the data science consulting business or your questions about how to transition from consulting to something new even if you don’t have questions join in to hear what others are asking
join apache kafka cocreator and pmc chair jun rao and apache kafka committer and architect of kafka connect ewen cheslackpostava for a qa session about apache kafka bring your questions about kafka internals or key considerations for developing your data pipeline and architecture designing your applications and running in production with kafka even if you don’t have a specific question join in to hear what others are asking
rajesh shroff reviews the big data and analytics landscape lessons learned in enterprise over the last few years and some of the key considerations while designing a big data systems rajesh also highlights differentiations in the cisco unified computing systems ucs platform outlines cisco’s big data ecosystem partnerships and certified ciscovalidated designs cvds and demonstrates what ucs director express can bring to managing your big data environment by significantly increasing productivity and reducing operational cost
big data and hadoop are a critical part of the data fabric of companies as such proper information governance is key in order to support datadriven applications that extend lineofbusiness processes radically transforming industryspecific solutions with big data and hadoop a generalpurpose reusable resource developers and administrators need to meet the critical enterprise adoption criteria of correctness quality consistency compliance and traceability big data solutions and the quality of data in data lakes should not generate additional risk to the business or be a roadblock to application development and user adoption these solutions must meet the highest levels of enterprise information governance compliance and regulation without stifling the democratization agility and openness promised by big data
in this new world order data collection must come with a corporate responsibility to protect data sometimes this is a legal requirement as in the eu’s data protection regulation aka gdpr russia’s federal law on personal data and germany’s bundesdatenschutzgesetz bdsg but many times it’s only a social responsibility a quite complicated and gray area—it’s all about what you feel is “right”
join dj patil and lynn overmann to ask your questions about data science at the white house
with so much variance across hadoop distributions odpi was established to create standards for both hadoop components and testing applications on those components the linux foundation’s john mertic offers an overview of odpi and its benefits before ibm’s berni schiefer discusses applications such as ibm big sql that have been designed to work with odpicompliant hadoop distributions join in to learn how application developers and companies considering hadoop can benefit from odpi
big data offers the possibility of deep insights into marketing programs and business operations as well as a 360degree view of customers and competitors for many enterprises being able to achieve such results means combining new and traditional data sources and modernizing etl and data warehouse applications these tasks seem easy during research and proofofconcept phases but complexity grows exponentially when moving toward enterprisegrade implementations
scott gnau provides unique insights into the tipping point for data how enterprises are now rethinking everything from their it architecture and software strategies to data governance and security and the cultural shifts cios must grapple with when supporting a business using realtime data to scale and grow
ready to take a deeper look at how hadoop and its ecosystem has a widespread impact on analytics douglas liming explains where sas fits into the open ecosystem why you no longer have to choose between analytics languages like python r or sas and how a single unified open analytics architecture empowers you to literally have it all
launched in late 2015 cigna’s enterprise data lake project is taking the company on a data governance journey using podium as the software platform for its data lake cigna’s data management and governance teams are eliminating silos of activity and creating a common thread of activity—based on a shared platform of metadata—to connect and align activities across technical teams and business processes
while traditional methods may be proficient to collect and analyze uniform data utilizing multiple structured and unstructured external data sources can be challenging joe caserta explains how one of the largest membership interests groups in the country makes sense of the influx of information from streaming external data sources this challenge is exciting because aside from collecting data from its 40 million members the  group also needs to monitor digital and traditional interactions cohesively to predict and optimize a member’s path to purchase
mariusz gądarowski offers an overview of neptune deepsenseio’s new it platformbased machinelearning experiment management solution for data scientists deepsenseio uses technologies such as theano tensorflow lasagne scikitlearn and apache spark to carry out machinelearning tasks neptune seamlessly integrates with these technologies and makes them easier to use neptune enhances the management of machinelearning tasks such as dependent computational processes code versioning comparing achieved results monitoring tasks and progress sharing infrastructure among teammates and many others
strata  hadoop world program chairs roger magoulas doug cutting and alistair croll welcome you to the second day of keynotes
in a few short years cloud has risen from an aspirational concept to an enterprise mandate as workloads move to the cloud a new set of concerns in the data center are emerging including data security portability and governance cloudera ceo tom reilly and james powell global cto of nielsen discuss the dynamics of hadoop in the cloud what to consider at the start of the journey and how to implement a solution that delivers flexibility while meeting key enterprise requirements
when hollywood portrays artificial intelligence it’s either a demon or a savior but the reality is that ai is far more likely to be an extension of ourselves strata program chair alistair croll looks at the sometimes surprising ways that machine learning is insinuating itself into our everyday lives
will machine learning give us better eyesight join joseph sirosh for a surprising story about how machine learning population data and the cloud are coming together to fundamentally reimagine eye care in one of the world’s most populous countries india
the panama papers investigation revealed the offshore holdings and connections of dozens of politicians and prominent public figures around the world and led to highprofile resignations police raids and official investigations almost 500 journalists coordinated by the international consortium of investigative journalists and süddeutsche zeitung had to sift through 26 terabytes of data—the biggest leak in the history of journalism mar cabra explains how technology made it all possible
the need to quickly acquire process prepare store and analyze data has never been greater the need for performance crosses the big data ecosystem too—from the edge to the server to the analytics software speed matters raghunath nambiar shares a few use cases that have had significant organizational impact where performance was key
chad w jennings demonstrates the power of bigquery through an exciting demo and announces several new features that will make bigquery a better home for your enterprise big data workloads
keynote by dj patil and lynn overmann
data has long stopped being structured and flat but the results of our analysis are still rendered as flat bar charts and scatter plots we live in a 3d world and we need to be able to enable data interaction from all perspectives robert thomas offers an overview of immersive visualization—integrated with notebooks and powered by spark—which helps bring insights to life
we have more data than ever before by many orders of magnitude yet “strong” artificial intelligence remains elusive some notorious difficult problems like speech recognition and the game of go have recently seen spectacular advances yet no machine can understand language as well as threeyearold child gary marcus explores the gap between what machines do well and what people do well and what needs to happen before machines can match the flexibility and power of human cognition
hadoop and its ecosystem mean new possibilities for analytics we’ve shifted from samplebased assessments to allinclusive investigations the result deeper insights and more relevant actions paul kent offers an overview of sas’s participation in open platforms and introduces sas viya a new unified and open analytics architecture that lets you scale analytics in the cloud and code as you choose
although 2016 is a highly unusual political year elections and public opinion follow predictable statistical properties sam wang explains how the presidential senate and house races can be tracked and forecast from freely available polling data using tools from statistics and machine learning these approaches offer a deeper understanding of the us political scene even under extreme circumstances
birds of a feather bof discussions are a great way to informally network with people in similar industries or interested in the same topics

the landscape for storing your big data is quite complex with several competing formats and different implementations of each format picking the best data format depends on what kind of data you have and how you plan to use it depending on your use case different formats perform very differently although you can use a hammer to drive a screw it isn’t fast or easy to do so owen o’malley outlines the performance differences between formats in different use cases and offers an overview of the advantages and disadvantages of each to help you improve the performance of your applications
structured streaming is a new effort in apache spark to make stream processing simple without the need to learn a new programming paradigm or system ram sriharsha offers an overview of structured streaming discussing its support for eventtime outoforderdelayed data sessionization and integration with the batch data stack to show how it simplifies building powerful continuous applications
deep learning has taken us a few steps further toward achieving ai for a manmachine interface however deep learning technologies like speech recognition and natural language processing remain a mystery to many yishay carmiel reviews the history of deep learning the impact it’s made recent breakthroughs interesting solved and open problems and what’s in store for the future
join apache beam and google cloud dataflow engineers to ask all of your questions about stream processing they’ll answer everything from general streaming questions about concepts semantics capabilities limitations etc to questions specifically related to apache beam google cloud dataflow and other common streaming systems flink spark storm etc
strata  hadoop world program chairs roger magoulas doug cutting and alistair croll welcome you to the second day of keynotes
leading companies that are getting the most out of their data are not focusing on queries and data lakes they are actively integrating analytics into their operations with a datafirst application development approach realtime adjustments to improve revenues reduce costs or mitigate risk rely on applications that minimize latency on a variety of data sources jack norris reviews best practices for three use cases in admedia financial services and healthcare to show how customers develop deploy and dynamically update these applications and how this datafirst approach is fundamentally different from traditional applications
the panama papers investigation revealed the offshore holdings and connections of dozens of politicians and prominent public figures around the world and led to highprofile resignations police raids and official investigations almost 500 journalists coordinated by the international consortium of investigative journalists and süddeutsche zeitung had to sift through 26 terabytes of data—the biggest leak in the history of journalism mar cabra explains how technology made it all possible
modern cars produce data lots of data and formula 1 cars produce more than their fair share ted dunning presents a demo of how data streaming can be applied to the analytics problems posed by modern motorsports although he won’t be bringing formula 1 cars to the talk ted demonstrates a highfidelity physicsbased automotive simulator to produce realistic data from simulated cars running on the spafrancorchamps track as ted moves data from the cars to the pits to the engineers back at hq the result is near realtime visualization and comparison of performance and a great exposition of how to move data using messaging systems like kafka
recurrent neural networks rnn and related models represent the state of the art in language modeling trained on sufficiently large corpora rnns can learn to generate convincing text that obeys rules of syntax and even matches parentheses what is especially remarkable is that these models can synthesize text from diverse inputs text in another language for translation images and video for captioning and scores and categories for creation of personalized product reviews
tyler akidau offers a whirlwind tour of the evolution of massivescale data processing at google from the original mapreduce paradigm to the highlevel pipelines of flume to the streaming approach of millwheel to the portable unified streamingbatch model of google cloud dataflow and apache beam incubating tyler examines in detail the basic architectural concepts that underlie these four models highlights their similarities contrasts their differences particularly regarding traditional batch versus streaming and provides insight into the use cases the drove the progression of the designs to what exists today he also highlights similarities and differences with related open source systems such as flink spark storm and gearpump calling out ways in which they’re converging on and diverging from the beam model and what that means when running beam pipelines on their respective runners
the last 10 years have seen hadoop move from storage cost killer to contender for the next doordie platform in financial services fintech organizations have used hadoop for building advanced scientific operational data stores data warehousing and reporting consumer application development data visualization and realtime processing but what’s the point at which hadoop tips from a swissarmy knife of use cases to a new foundation that rearranges how the financial services marketplace turns data into profit and competitive advantage this panel of expert practitioners looks into the near future to see if the inflection point is at hand
in the world of distributed computing spark has simplified development and opened the doors for many to start writing distributed programs folks with little to no distributed coding experience can now write just a couple lines of code that will immediately get hundreds or thousands of machines working on creating business value
customers are looking to extend the benefits beyond big data with the power of the deep learning and accelerated analytics ecosystems jim mchugh explains how customers are leveraging deep learning and accelerated analytics to turn insights into aidriven knowledge and covers the growing ecosystem of solutions and technologies that are delivering on this promise such as the nvidia dgx1 which integrates power of deep learning and accelerated analytics together in a single hardware and software system
the power of artificial intelligence and advanced analytics emerges from the ability to analyze and compute large disparate datasets from varied devices and locations such as predictive medicine and automated cars at lightningfast speed these realtime insights require continued innovation to fuel the changing landscape of ai martin hall explains why collaboration and openness are the key elements driving innovation in ai
running realtime dataintensive applications on apache hadoop requires complex architectures to store and query data typically involving multiple independent systems that are tied together through customengineered pipelines a common pattern is to use a nosql engine like apache hbase for caching and later transformations the results of which are periodically written to hdfs in one of the popular open columnar file formats as a prerequisite for querying by a sql engine
political analysts may once have depended entirely on subjective attributes such as ethics charisma and nonscientific impressions of the electorate to forecast elections but with the rise of data generated from human daily interaction with software systems it’s possible to add meaningful datadriven attributes to political forecasting alongside all of the demographic information available to today’s political consultants
yahoo initially built hadoop as an answer to a very acute pain around efficiently storing and processing large volumes of data since yahoo open sourced hadoop it has become widely adopted in the technology world however time has taught us that when a system becomes extremely popular for solving one class of problems its limitations in solving other problems become more apparent himanshu gupta explains why yahoo has been increasingly investing in interactive analytics and how it leverages druid to power a variety of internal and externalfacing data applications
apache spark has been growing in deployments for the past two years the increasing amount of data being analyzed and processed through the framework is massive and it continues to push the boundaries of the engine drawing on his experiences across 150 production deployments neelesh srinivas salian focuses on five common issues observed in a cluster environment setup with apache spark core streaming and sql to help you improve the usability and supportability of apache spark and avoid such issues in future deployments
mark grover jonathan seidman and ted malaska the authors of emhadoop application architecturesem participate in an open qa session on considerations and recommendations for the architecture and design of applications using hadoop come with questions about your use case and its big data architecture or just listen in on the conversation
businesses are clamoring to capture all data possible and harness it as a revenue driver the challenge is bringing the data together companies that can capture and harness this data can benefit accordingly
operational data stores ods serve as a data staging area between transactional databases and data warehouses data from multiple sources are integrated cleansed and prepped in the ods before populating a data warehouse for longterm storage and analytics traditional ods systems encounter severe challenges when it comes to dealing with the wide variety and massive volume of data common to data warehouses built on top of the hadoop platform it’s time to rethink the requirements and the architecture for the next generation of an ods on top of hadoop starting from first principles vinayak borkar defines the requirements for a modern operational data store and explores some possible architectures to support those requirements
the flux capacitor was the core component that made time travel possible in back to the future processing garbage as a power source did you know that you can achieve the same affect in machine learning ingo mierswa rapidminer’s cofounder and ceo offers a case study on how he took “garbage” data drawn from 250k data scientist rapidminer users and through machine learning turned it into wisdom of crowds which helps novice and expert data scientists alike accelerate the creation of their predictive models by delivering expert recommendations about what other scientists would do at every step in their predictive analytics process ingo covers the most frequently used machinelearning techniques what data preparation most experts perform before modeling and how those behaviors have changed over time along with other interesting patterns
despite widespread adoption machinelearning models remain mostly black boxes making it very difficult to understand the reasons behind a prediction such understanding is fundamentally important to assess trust in a model before we take actions based on a prediction or choose to deploy a new ml service such understanding further provides insights into the model which can be used to turn an untrustworthy model or prediction into a trustworthy one
scott gnau provides unique insights into the tipping point for data how enterprises are now rethinking everything from their it architecture and software strategies to data governance and security and the cultural shifts cios must grapple with when supporting a business using realtime data to scale and grow
modern data science is the creative application of scientific principles to design new tools and processes in areas where a scientific approach has been previously infeasible due to the difficulty or expense of collecting data that’s a mouthful but if you see data science that way we’re likely just at the beginning the people and things that are starting to be equipped with sensors will create data that will enable entirely new classes of problems to be approached more scientifically
streaming machine learning is being integrated in spark 21 but you don’t need to wait holden karau and seth hendrickson demonstrate how to do streaming machine learning using spark’s new structured streaming and walk you through creating your own streaming model holden and seth will also cover how to use structured machinelearning algorithms if they are merged by the talk by the end of this session you’ll have a better understanding of spark’s structured streaming api as well as how machine learning works in spark
we have more data than ever before by many orders of magnitude yet “strong” artificial intelligence remains elusive some notorious difficult problems like speech recognition and the game of go have recently seen spectacular advances yet no machine can understand language as well as threeyearold child gary marcus explores the gap between what machines do well and what people do well and what needs to happen before machines can match the flexibility and power of human cognition
we all hear stories about the potential value of big data analytics but it’s important to understand that big data analytics is a journey of introspection operational excellence and new winning strategies john morrell leads a panel of practitioners from dell national instruments and citi—companies that are gaining real value from big data analytics—as they explore their companies’ big data journeys offering lessons learned and best practices join john to hear realworld stories from organizations that are seeing concrete results and learn how analytics can answer groundbreaking new questions about business and create a path to becoming a datadriven organization
american politics is adrift in a sea of polls this year that sea is deeper than ever before—and darker data science is upending the public opinion industry but to what end in a brief illustrated history of the field jill lepore demonstrates how pollsters rose to prominence by claiming that measuring public opinion is good for democracy and asks “but what if it’s bad”
join max shron former consultant on data science and current head of warby parker’s data science team for a qa all about data science consulting bring your questions about getting into the data science consulting business or your questions about how to transition from consulting to something new even if you don’t have questions join in to hear what others are asking
predictive maintenance is about anticipating a failure and taking preemptive action with the recent advances in accessible machine learning and cloud storage there is tremendous opportunity to utilize the entire gamut of data coming from factories buildings machines and sensors to not only monitor the health of equipment but also predict when it is likely to malfunction or fail however as simple as it sounds in principle in reality the data required to actually make a prediction in advance and in a timely manner is hard to come by the data that is collected is often incomplete partial or just not enough making it unsuitable for modeling
bas geerdink offers an overview of the evolution that the hadoop ecosystem has taken at ing since 2013 ing has invested heavily in a central data lake and data management practice bas shares historical lessons and best practices for enterprises that are incorporating hadoop into their infrastructure landscape
topics include
otto is the world’s secondlargest online retailer in a highly competitive market space superior customer experience in terms of higher empathy relevance and speed is key to positive customer experience and this is where ai comes into play rupert steffner explores the cornerstones retailers have to focus when building their customers’ experience on artificial intelligence it starts with having clear goals and a value system that finds the right balance between customer retention and revenue optimization even if ai is hereby built from the seller’s perspective retailers will need a “good ai” approach that treats consumers fairly and as partners to optimize longterm customer equity
which suppliers are most likely to have delivery or quality issues does service product placement or price make the biggest difference in customer sentiment finding the answers to these questions in structured data is often straightforward but can we answer them using the unstructured data free text in emails social media call center transcripts product reviews and other sources
agility is king in the world of finance and a messagedriven architecture is a mechanism for building and managing discrete business functionality to enable agility in order to accommodate rapid innovation data pipelines must evolve however implementing microservices can create management problems like the number of instances running in an environment
while all other industries have embraced the digital era healthcare seems to be still playing catchup in this kaiser permanente is an anomaly kaiser permanente is a leader in healthcare technology—technology has been at center stage in kaiser permanente since it first started using computing to improve healthcare results in the 1960s today kaiser permanente is an integrated health care delivery system with 10 million members and about 200000 employees
swisscom the leading mobile service provider in switzerland also provides datadriven intelligence through the analysis of the data created by its mobile network its mobility insights team works to help civil administrators tourism and marketing professionals and many others understand the flow of people through their locations of interest françois garillot outlines the platform tooling and choices that help achieve this service and some challenges the team has faced before exploring in depth the task of understanding the speeds of populations through a path of interest
analytics has increasingly become a major focus for apache solr the primary search engine in the hadoop stack yonik seeley explores recent apache solr features in the areas of faceting and analytics including parallel sql streaming expressions distributed join and distributed graph queries given the increasing number of apis and techniques that can be brought to bear yonik also covers the tradeoffs of different approaches and strategies for maximizing scalability
time series and event data form the basis for realtime insights about the performance of businesses such as ecommerce the iot and web services but gaining these insights involves designing a learning system that scales to millions and billions of data streams ira cohen outlines a system that performs realtime machine learning and analytics on streams at massive scale
healthcare a 3 trillion industry is ripe for disruption through data science however there are many challenges in the journey to make healthcare a truly transparent consumercentric datadriven industry sriram vishwanath explains where data science can have massive impact in healthcare and dispels myths where its apparent use contradicts realities within the healthcare ecosystem
inchip analytics blasted onto the scene a few years back impressing companies with its ability to significantly impact how companies handle complex data both large and disparate with less hardware while eliminating the data preparation nightmare but the real impact of inchip analytics is only now being realized as companies exploit inchip’s extensibility scaleability and flexibility to take business intelligence to the next level with new iot and ai technologies guy levyyurista explains the unexpected consequences of making big data processing significantly more agile than ever before and the impact it’s having on human insight consumption
big data and analytics is a team sport empowering companies of all kinds to achieve business outcomes faster and with greater levels of success dell emc has integrated all the key components for modern digital transformation taking you on a big data journey that focuses on analytics integration and infrastructure its portfolio provides the flexibility to buy or build your analytics ecosystem and offerings range from servers and data lakes to flexible analytics with a turnkey development platform carey james explains how the formation of dell technologies and dell emc can help you on your data analytics journey and how you can turn actionable insights into new business opportunities
we’ve seen significant progress in infrastructure for using data effectively in the last halfdecade but this hasn’t applied to all types of data equally unstructured text in particular has been slower to yield to the kinds of analysis that many businesses are starting to take for granted rather than being limited by what we can collect we are now constrained by the tools time and techniques to make good use of it but we are beginning to gain the ability to do remarkable things with unstructured text data
it is clear that we’re at a critical inflection point in the industry—organizations are realizing that they must quickly adapt in order to keep pace in today’s ever changing digital economy data your most precious commodity is increasing at an alarming rate at the same time an emerging business imperative has made this data a component of your deepest insights allowing you to focus on your business outcomes patricia florissi explains why the recent formation of dell emc ensures that your analytics capabilities will be stronger than ever
building running and governing a data lake and production data applications on hadoop is often a difficult process filled with slow development cycles and painful operations not only are traditional development tools and techniques missing from the hadoop ecosystem but mastering data ingestion and data integration as well as enterprise governance and security has become a formidable challenge when building big data solutions the challenge only increases as the hadoop ecosystem continues to grow use cases mature slas intensify and services become customer facing and revenue generating and while the it organization owns the task of mitigating these issues more importantly it also has an opportunity to enable the business to reduce time to insights and make better decisions faster by providing them with a modern selfservice environment for their data
data should be something you can see feel hear taste and touch cameron turner brad sarsfield hanna kangbrown and evan macmillan cover the emerging field of sensory data visualization including data sonification in an anecdotal survey they explore reallife examples of solutions deployed to production in industries spanning from consumer goods to heavy industrial and largescale manufacturing to the iot that take advantage of auditory touch and other senses as alternative means of what has traditionally been called data visualization they then investigate the hypothesis that we might better consume information by moving beyond words numbers and pictures and start using sound smell and even taste as a means to better understand the state of the world topics will tie into cameron’s recent interview on the  which focused on data sonification extending these topics into the future of sensory data collection and consumption
data is a company’s lifeblood and more data exists than ever before—in more disparate silos getting the insights you need sifting through data and answering new questions have all been complex hairy tasks that only data jocks have been able to do the entire process is slow and daunting and businesses are never satisfied with the outcome andrew yeung and scott anderson explore new ways to challenge the status quo through automated data blending and smart data discovery across diverse sources to speed insights for business users see and hear about real customer use cases and learn how to reinvent your organization’s analytics capability
many initiatives for running applications inside containers have been scoped to run on a single host using docker containers for largescale production environments poses interesting challenges especially when deploying distributed big data applications like apache hadoop and apache spark
today hadoop is deployed onpremises and in the public cloud with public cloud increasingly becoming more prevalent the cloud provides some unique abilities including ondemand infrastructure cluster elasticity persisted globally available object storage and payforuse pricing which enables even more flexible and costefficient deployment options for bi and sql analytic users of impala but brings in new challenges that need to be carefully considered to achieve optimal outcome
a textmining system must go way beyond indexing and search to appear truly intelligent first it should understand language beyond keyword matching for example distinguishing between “jane has the flu” “jane may have the flu” “jane is concerned about the flu “jane’s sister has the flu but she doesn’t” or “jane had the flu when she was 9” is of critical importance this is a natural language processing problem second it should “read between the lines” and make likely inferences even if they’re not explicitly written for example if jane has had a fever a headache fatigue and a runny nose for three days not as part of an ongoing condition then she likely has the flu this is a semisupervised machinelearning problem third it should automatically learn the right contextual inferences to make for example learning on its own that fatigue is sometimes a flu symptom—only because it appears in many diagnosed patients—without a human ever explicitly stating that rule this is an associationmining problem which can be tackled via deep learning or via more guided machinelearning techniques
there is a growing trend to use modern advanced technology in the finance industry information is often obtained on much larger scales in various modalities and from multiple dimensions which greatly enriches the profiles of financial entities and leads to a rapid increase in the complexity of financial analytics in the meantime there’s increasing demand for automating the process of data statistics feature engineering and model tuning
predicting which stories will become popular is an invaluable tool for newsrooms but people access their news using a variety of different platforms and sites so identifying what stories are likely to be popular is a challenge very few studies have addressed the popularity of news articles specifically although others have looked at predicting the popularity of other types of online content like tweets and videos the reasons why a particular story becomes popular are varied and might involve contemporariness writing quality and other latent factors euihong han and shuguang wang explain how the washington post predicts what stories on its site will be popular with readers and share the challenges they faced in developing the tool and metrics on how they refined the tool to increase accuracy
most people are surprised to know that spark works with java maybe they saw the initial java code that used anonymous classes and dismissed it as an ungainly mess they were right—plain java and spark are ugly together then java 8’s lambdas came along now instead of an ungainly mess we get the tight syntax of lambda expressions offering code that is readable and testable best of all it uses java
data visualizations are interactive stories that can powerfully engage audiences giving them insight into the meanings and trends behind numbers they often begin with a massive spreadsheet of data that has no meaning to the average person building an effective visualization begins by asking how data can be transformed into a compelling narrative and a dynamic user experience
airbnb developed caravel to provide all employees with interactive access to data while minimizing friction caravel provides a quick way to intuitively visualize datasets by allowing users to create and share interactive dashboards a rich set of visualizations to analyze your data as well as a flexible way to extend the capabilities an extensible highgranularity security model allowing intricate rules on who can access which features and integration with major authentication providers database openid ldap oauth and remoteuser through flask appbuilder a simple semantic layer allowing you to control how data sources are displayed in the ui by defining which fields should show up in which dropdown and which aggregation and function metrics are made available to the user and deep integration with druid that allows for caravel to stay blazing fast while working with large realtime datasets
hadoop in the cloud is becoming an increasingly common use case as the cloud provides rapid access to flexible and lowcost it resources similar to traditional onpremises hadoop clusters data authorization becomes more crucial than ever for the multitenant cloud a transparent solution that decouples compute and storage is required for a simple and smooth transition and since the underlying data is shared across the components a unified authorization policy should be enforced to adapt the flexibility of hadoop ecosystem
the trend of deploying hadoop on virtual infrastructure is rapidly increasing martin yip explores the benefits of virtualizing hadoop through the lens of three realworld examples and walks you through the basics of virtualizing hadoop the first step in providing hadoop on the public or private cloud
calling all data enthusiasts
ready to take a deeper look at how hadoop and its ecosystem has a widespread impact on analytics douglas liming explains where sas fits into the open ecosystem why you no longer have to choose between analytics languages like python r or sas and how a single unified open analytics architecture empowers you to literally have it all
haoyuan li offers an overview of alluxio formerly tachyon a memoryspeed virtual distributed storage system the alluxio open source community is one of the fastest growing open source communities in big data history with more than 250 developers from over 50 organizations around the world and the alluxio system has been deployed at a number of companies including alibaba baidu barclays intel huawei and qunar in some of these deployments alluxio has been running in production for over a year managing pbs of data
when building your data stack the architecture could be your biggest challenge yet it could also be the best predictor for success with so many elements to consider and no proven playbook where do you begin to assemble best practices for a scalable data architecture ben sharma offers lessons learned from the field to get you started if you are concerned with building a data architecture that will serve you now and scale for the future this is a mustattend session
it is no surprise that reducing operational it expenditures and increasing software capabilities is a top priority for large enterprises given its advantages open source software has proliferated across the globe while there’s been much discussion on open source versus commercial cios and ctos at global 1000 enterprises are increasingly interested in solutions that blend the benefits of both however key challenges must be overcome enterprise architecture groups are now faced with the difficult task of selecting the right open source software components from an evergrowing set of options figuring out how to integrate them and ensuring they work together
massively parallel big data platforms are quickly becoming the industry standard for organizations looking to extract greater value from data as architectures have shifted application development paradigms have also changed to reflect growing needs for agility scale robustness efficiency and ease of collaboration on these new platforms
cluster computing frameworks such as hadoop or spark are tremendously beneficial in processing and deriving insights from data however long query latencies make these frameworks suboptimal choices to power interactive applications organizations frequently rely on dedicated query layers such as relational databases and keyvalue stores for faster query latencies but these technologies suffer many drawbacks for analytic use cases
according to the identity theft resource center database more than 169068506 records were exposed in 2015 that stolen data has a much longer shelf life than most realize and will be used to continue the cycle of theft deception and fraud through one of the fastestgrowing and most lucrative businesses for criminals account takeover ato attacks
praveen murugesan explains how uber leverages hadoop and spark as the cornerstones of its data infrastructure praveen details the current data architecture at uber and outlines some of the unique challenges with data processing uber faced as well as its approach to solving some key issues in order to continue to power uber’s realtime marketplace
recent years have seen significant evolution of the internet of things it has become increasingly easy to connect devices to the internet and send sensory data to the public cloud  however the adoption of iot platforms and stream analytics within the enterprise is lagging and less prevalent an effect of the lack of skilled developers required to deploy an onpremises platform and the limited demonstration of high value in reallife use cases
for ma speculators intellectual property managers and attorneys the “holy grail” for patents is to be able to accurately assess the value of an individual invention and aggregate up to portfolios ignoring the softer side of a patent’s value ie its power to halt competitive development andor neutralize the threat of enforcement there are few liquidity events—specifically litigation damagessettlements portfolio acquisitions and licensing fees—that allow assignees to monetize individual patents none of these typically make the associated data available to the research community
kudu is redefining the big data ecosystem and opening doors to capabilities not available before comcast is moving in the direction of adopting kudu with impala and spark for several projects including realtime processing of events from xfinity devices sridhar alla and kiran muglurmath explain how realtime analytics on comcast xfinity settop boxes stbs help drive several customerfacing and internal datascienceoriented applications and how comcast uses kudu to fill the gaps in batch and realtime storage and computation needs allowing comcast to process the highspeed data without the elaborate solutions needed till now
many areas of applied machine learning require models optimized for rare occurrences such as class imbalances and users actively attempting to subvert the system adversaries the data innovation lab at capital one has explored advanced modeling techniques for just these challenges the lab’s use case necessitated that it survey the many related fields that deal with these issues and perform many of the suggested modeling techniques it has also introduced a few novel variations of its own
big data and hadoop are a critical part of the data fabric of companies as such proper information governance is key in order to support datadriven applications that extend lineofbusiness processes radically transforming industryspecific solutions with big data and hadoop a generalpurpose reusable resource developers and administrators need to meet the critical enterprise adoption criteria of correctness quality consistency compliance and traceability big data solutions and the quality of data in data lakes should not generate additional risk to the business or be a roadblock to application development and user adoption these solutions must meet the highest levels of enterprise information governance compliance and regulation without stifling the democratization agility and openness promised by big data
birds of a feather bof discussions are a great way to informally network with people in similar industries or interested in the same topics
a study by hpe’s security unit found that 70 percent of popular consumer iot devices are easily hackable many of the simplest iot or machinetomachine devices lack adequate processing power and storage to host endpoint security software as a result attackers can exploit vulnerabilities in consumer devices and mobile applications to gain remote access to internal networks and expose users to maninthemiddle attacks
apache kudu was first announced as a public beta release at strata nyc 2015 and recently reached 10 this conference marks its one year anniversary as a public open source project todd lipcon offers a very brief refresher on the goals and feature set of the kudu storage engine covering the development that has taken place over the last year including new features such as improved support for time series workloads performance improvements spark integration and highly available replicated masters along the way todd explores realworld production deployments and some of the tools that have been built to help operators manage a kudu cluster he ends with a view of the road map of the kudu project for the upcoming year including plans for security and other new features
to anticipate who will succeed and invest wisely investors spend a lot of time trying to understand the longerterm trends within an industry in a panel discussion toptier vcs look over the horizon and consider the big trends in big data explaining what they think the field will look like a few years or more down the road join us to hear about the trends that everyone is seeing and areas for investment that they find exciting
machinelearning tools promise to help solve data curation problems while the principles are well understood the engineering details in configuring and deploying ml techniques are the biggest hurdle ihab ilyas explains why leveraging data semantics and domainspecific knowledge is key in delivering the optimizations necessary for truly scalable ml curation solutions
digital consumer companies are disrupting the old guard and changing the way we do business in fundamental ways for example uber airbnb and zipcar have disrupted the traditional businesses of taxis hotels and car rental companies by leveraging software capabilities to create new business models opportunities in the industrial world are expected to outpace consumer business cases time series data is growing exponentially as new machines around the world get connected venkatesh sivasubramanian and luis ramos explain how ge makes it faster and easier for systems to access using a common layer and perform analytics on a massive volume of time series data by taking what they’ve learned from apache arrow and applying it today for highly efficient time series storage using apache apex spark and kudu
how are users meant to interpret the influence of big data and personalization in their targeted experiences what signals show how your data is used and how it improves or constrains your experience to what degree is this experience based on coarse demographics or the entire data profile of your browsing history sara watson explains that in order to develop normative opinions to shape policy and practice users need means to guide their experience—the personalization spectrum
strata  hadoop world program chairs roger magoulas doug cutting and alistair croll welcome you to the first day of keynotes
organizations from small startups to large enterprises are increasingly using open source frameworks such as apache hadoop spark and presto to address a broad range of analytic use cases including business intelligence stream processing and machine learning however with any big data project comes the risk of uncapped costs delayed timelines expensive infrastructure and difficult choices about where to focus in the open source toolset
 
jonathan fritz explains how organizations are deploying these and other big data frameworks with amazon web services aws and how you too can quickly and securely run spark and presto on aws jonathan demonstrates how to lower costs and accelerate deployment of big data applications using amazon emr to easily create a hadoop cluster running spark and presto and querying data in amazon s3 using ansi sql jonathan then explores how you can use amazon s3 as a highly scalable durable and secure data lake by decoupling compute from storage before outlining best practices to lower costs using amazon ec2 spot instances and discussing how to secure your clusters using aws’s extensive security capabilities
join apache kafka cocreator and pmc chair jun rao and apache kafka committer and architect of kafka connect ewen cheslackpostava for a qa session about apache kafka bring your questions about kafka internals or key considerations for developing your data pipeline and architecture designing your applications and running in production with kafka even if you don’t have a specific question join in to hear what others are asking
the history of the digital age is being written in photographs today for better or worse everyone is a both a photographer and a subject we need to start thinking about visual content in a radically different way as both organizations and individuals to innovate in the visual age we have to crack the visual code this means learning as much as we can not only about how we see but also about how computers see so we can teach them to discover hidden opportunities and disregard hidden biases if we try hard enough maybe they’ll teach us to do the same
enterprises that pursue datadriven operations and decisions are approaching the conclusion that graph analysis capabilities will yield critical competitive advantages however for this impact to be fully realized the results of any graph analysis must be available in real time to operational applications data scientists and developers across the enterprise
more and more frequently owners of hadoop deployments find themselves facing the challenge of supporting data science ecosystems like python and r both adjacent to and within their hadoop infrastructure although these technologies promise powerful data science insights they can also be complex to manage and deploy as people build out data science sandboxes and production environments they discover a number of challenges ranging from basic package management and data lineage to reproducibility and governance of data science artifacts
most people will agree that interviewing is one of the most difficult and least enjoyable professional activities given the recent demand for data analytics and data science skills it has become an increasingly daunting task for managers to adequately test and qualify candidates
visual analysis is changing in the era of gpu clusters now that scale compute is easier the bottleneck is mapping data to visualizations and intelligently interacting with them using datasets uploaded to graphistry leo meyerovich provides a glimpse into the emerging workflows for graph and linked event analysis and offers common tricks for success
as healthcare data becomes more digitized the opportunity to leverage electronic medical records prescription data medical billings hospital and other healthcare datasets to help improve health outcomes and lower the cost of care for patients in near real time is becoming a possibility however processing terabytes and petabytes of deidentified healthcare data requires the application of complex and everchanging business rules this impacts the ability to generate nearrealtime insights and conduct research studies that could potentially influence how patients are treated
in this new world order data collection must come with a corporate responsibility to protect data sometimes this is a legal requirement as in the eu’s data protection regulation aka gdpr russia’s federal law on personal data and germany’s bundesdatenschutzgesetz bdsg but many times it’s only a social responsibility a quite complicated and gray area—it’s all about what you feel is “right”
every day analysts at citi standard chartered bank scb and polaris analyze transactions and behaviors for indicators of money laundering fraud and human trafficking the most comprehensive investigation must leverage massive volumes of data from financial institutions law enforcement and government agencies social media sites telecommunication organizations and enterprises but it often comes with varied data quality standards or in unusable formats and nonstandard structures analysts ability to explore clean shape and integrate the data can’t be slowed down by timeconsuming user compute cycles or resourceintensive etl processes and the entire flow—from source data to usable information—must be auditable and fully trusted
as the momentum with which big data drives decisions continues to grow the relevant legal considerations relating to collection and use of such data will also increase and evolve understanding the legal frameworks applicable to collection and use of data for certain purposes is key to making compliancebased business decisions and maintaining longterm consumer trust in a brand
ever since its creation hdfs has relied on data replication to shield against most failure scenarios however with the explosive growth in data volume replication is getting quite expensive the default 3x replication scheme incurs a 200 overhead in storage space and other resources eg network bandwidth when writing the data erasure coding ec uses far less storage space while still providing the same level of fault tolerance under typical configurations ec reduces the storage cost by 50 compared with 3x replication
handling heterogeneous and concurrent query workloads in a multiuser environment is a common use scenario for bi analytics over sqlonhadoop systems properly deploying a sqlonhadoop cluster that provides the best performance in such an environment requires extensive knowledge of the workloads overall resource utilization database table design software stack configurations and hardware settings an improperly planned deployment can lead to an underutilized cluster wasting company assets or failing to meet performance requirements in one realworld example a company deployed an 80node cluster however their workloads and data volume required less than half of the nodes to meet their performance requirement this means more than half of the nodes sat doing nothing but waiting to be depreciated in another a company used more expensive ssds even though at the software level operations were single threaded and bottlenecked by cpu rather than io—thus hdds might have been a better choice for deployment of course in some scenarios ssds might improve the overall query execution time by more than 50 so there is really no onesizefitsall solution
at the start of the crimean war in 1853 britain’s royal navy needed 90 new gunboats ready to fight in the baltic in just 90 days assembling the boats was straightforward the challenge was to build all of the engine sets in time marine engineer john penn did an unusual thing he took a pair of reference engines disassembled them and distributed the pieces to the best machine shops across britain these workshops—latterday microservices—each built 90 sets of their allocated parts which were then assembled into the engines for the new gunboats ready for battle
the emerging industrial internet of things is giving rise to what is predicted to be a sweeping change that will fundamentally transform industries and reconfigure the technology landscape sensor data is expected to dwarf the data volumes that defined the first decade of big data and leading companies will be those that effectively derive value from this next wave of information and opportunity yet the challenges for enterprises remain formidable the information required to enable breakthrough insights is typically fragmented within the domains of information technology it and operational technology ot requiring both technical and cultural changes further organizations are realizing that analytics on sensor data is vastly more diverse and complex than analyzing traditional big datasets like weblogs cheryl wiebe explores how leading companies harness the iot by putting iot data in context fostering collaboration between it and ot and enabling a new breed of scalable analytics
quench your thirst with vendorhosted libations and snacks while you check out all the exhibitors in the expo hall it’s also a great time to meet and mingle with fellow attendees and strata  hadoop world speakers and authors
zillow pioneered providing access to unprecedented information about the housing market long gone are the days when you needed an agent to get comparables and prior sale and listing data enter zillow the nation’s numberone real estate website and mobile app with more data data science has enabled more use cases jasjeet thind explores zillow’s big data platform discusses some of its core machinelearning algorithms and outlines best practices for scaling streaming data ingestion and data processing in spark
how do we discover what we’re not looking for how can we become more serendipitous in the age of big data and bioinformatics such questions are more relevant than ever we develop new tools to help us spot clues in mountains of information and machines are getting better and better at aiding discovery and yet serendipity remains a very human art pagan kennedy discusses the origins of the word serendipity and qualities of mind that lead to successful searches in the deep unknown
join dj patil and lynn overmann to ask your questions about data science at the white house
twitter generates billions and billions of events per day analyzing these events in real time presents a massive challenge karthik ramasamy offers an overview of the endtoend realtime stack twitter designed in order to meet this challenge consisting of distributedlog the distributed and replicated messaging system and heron the streaming system for realtime computation
the hearst corporation monitors trending content on all of its 300 sites worldwide providing metrics to editors and promoting crossplatform content sharing to facilitate this hearst has built a clickstream analytics pipeline entirely in the cloud that transmits and processes over 30 tb of data a day
keynote by dj patil and lynn overmann
will machine learning give us better eyesight join joseph sirosh for a surprising story about how machine learning population data and the cloud are coming together to fundamentally reimagine eye care in one of the world’s most populous countries india
chad w jennings demonstrates the power of bigquery through an exciting demo and announces several new features that will make bigquery a better home for your enterprise big data workloads
big data is being thrown around as a silverbullet solution to enable organizational agility and transformation all your data in one place sounds great on paper but is it really thomas place explores the big data journey of the world’s biggest payment processor which came dangerously close to building a data swamp before pivoting to embrace governance and qualityfirst patterns this case study includes patterns partners successes failures and lessons learned to date and reviews the journey ahead
the world is producing an everincreasing volume velocity and variety of big data consumers and businesses are demanding uptothesecond or even millisecond analytics on their fastmoving data in addition to classic batch processing the hadoop ecosystem and aws provide a plethora of tools for solving big data problems but what tools should you use why and how
susan woodward discusses venture outcomes—what fraction make lots of money which just barely return capital and which fraction fail completely susan uses updated figures on the fraction of entrepreneurs who succeed including some interesting details on female founders of venture companies
hadoop and its ecosystem mean new possibilities for analytics we’ve shifted from samplebased assessments to allinclusive investigations the result deeper insights and more relevant actions paul kent offers an overview of sas’s participation in open platforms and introduces sas viya a new unified and open analytics architecture that lets you scale analytics in the cloud and code as you choose
to manage the everincreasing volume and velocity of data within your company you may have successfully made the transition from single machines and oneoff solutions to large distributed stream infrastructures in your data center powered by apache kafka but what’s to be done if one data center is not enough
kafka developed at linkedin in 2010 was originally an open system to encourage adoption developers could easily create new data streams add data to the pipeline and read data as it was created it succeeded brilliantly at encouraging developers to build new data applications improved the reliability of systems and applications and helped linkedin scale its logging and data infrastructure
from data exploration to prototyping to final delivery anyone engaged in using data science for social good knows that the path from project kickoff to delivery is full of exciting twists and turns jeancarlo bonilla susan sun and caitlin augustin explore how datakind volunteer teams navigate the road to social impact by automating evidence collection for conservationists and helping expand the reach of mobile surveys so that more voices can be heard
although 2016 is a highly unusual political year elections and public opinion follow predictable statistical properties sam wang explains how the presidential senate and house races can be tracked and forecast from freely available polling data using tools from statistics and machine learning these approaches offer a deeper understanding of the us political scene even under extreme circumstances
rajesh shroff reviews the big data and analytics landscape lessons learned in enterprise over the last few years and some of the key considerations while designing a big data systems rajesh also highlights differentiations in the cisco unified computing systems ucs platform outlines cisco’s big data ecosystem partnerships and certified ciscovalidated designs cvds and demonstrates what ucs director express can bring to managing your big data environment by significantly increasing productivity and reducing operational cost
enterprises are increasingly demanding realtime analytics and insights tony ng offers an overview of pulsar an open source realtime streaming system used at ebay which can scale to millions of events per second with 4gl sqllike language support pulsar provides realtime sessionization multidimensional metrics aggregation over time windows and custom stream creation through data enrichment filtering and stateful processing tony explains how pulsar integrates kafka kylin and druid to provide flexibility and scalability in event and metrics consumption
when it comes to sqlonhadoop it is easy to feel overwhelmed with the number of choices available in tools file formats schema design and configurations however in reality making good design choices when you start will help you avoid some of the common pitfalls marcel kornacker and mostafa mokhtar simplify the process and cover top performance optimizations for apache impala incubating from schema design and memory optimization to query tuning
since its introduction in spark 14 sparkr has received contributions from both the spark community and the r community xiangrui meng explores recent community efforts to extend sparkr for scalable advanced analytics—including summary statistics singlepass approximate algorithms and machinelearning algorithms ported from spark mllib—and shows how to integrate existing r packages with sparkr to accelerate existing r workflows
several big data graph processing frameworks that have been built to run on large graph datasets have been proposed and are in use at large corporations for applications ranging from social network analysis to machine learning to the pagerank algorithm however these libraries can also be put to work to study the nature of cancer
legacy enterprise data warehouse edw architecture geared toward daytoday workloads associated with operational querying reporting and analytics is often illequipped to handle the volume of data or traffic and varied data types associated with modern ad hoc analytics platforms faced with the challenges of increasing pipeline speed aggregation and visualization in a simplified selfservice fashion organizations are considering newer technologies such as apache spark hadoop kafka and columnar databases as key enabling technologies to optimize their edw architectures
billions of visa cards are used around the world to make payments each payment transaction has a story getting payments from point a to point b is complex and the resulting data visa captures reflects this the scale and complexity of that data is a direct manifestation of the number variety and complexity of payment transactions processed by the visa network
with so much variance across hadoop distributions odpi was established to create standards for both hadoop components and testing applications on those components the linux foundation’s john mertic offers an overview of odpi and its benefits before ibm’s berni schiefer discusses applications such as ibm big sql that have been designed to work with odpicompliant hadoop distributions join in to learn how application developers and companies considering hadoop can benefit from odpi
as software becomes more free and open it also is becoming more complex and expensive to operate how can we in the open source community clarify best practices and recommended operations to model complex interconnected services so users can focus on their ideas how can we as developers deliver recommended best practices in our applications so users are free to focus on the science on their choice of substrate eg laptop cloud or bare metalx86 arm ppc64el or s390x
project jupyter provides building blocks for interactive and exploratory computing that make science and data science reproducible across over 40 programming languages python julia r etc central to the project is the jupyter notebook a webbased interactive computing platform that allows users to author data and codedriven narratives computational narratives that combine live code equations narrative text visualizations interactive dashboards and other media while the jupyter notebook has proved to be an incredibly productive way of working interactively with code and data it is helpful to decompose notebooks into more primitive building blocks kernels for code execution input areas for typing code markdown cells for composing narrative content output areas for showing results terminals etc
iot and financial trading platforms share some commonality they intercept massive amounts of sensor or event data and must provide insights and actions in real time the technology challenge is huge since we need to combine fast event streams historical state and consistent data update transactions eg time series data and statistical aggregators with data science and machine learning and present results through realtime dashboards or drive immediate corrective actions
big data offers the possibility of deep insights into marketing programs and business operations as well as a 360degree view of customers and competitors for many enterprises being able to achieve such results means combining new and traditional data sources and modernizing etl and data warehouse applications these tasks seem easy during research and proofofconcept phases but complexity grows exponentially when moving toward enterprisegrade implementations
when hollywood portrays artificial intelligence it’s either a demon or a savior but the reality is that ai is far more likely to be an extension of ourselves strata program chair alistair croll looks at the sometimes surprising ways that machine learning is insinuating itself into our everyday lives
martin wicke and josh gordon field questions related to their tutorial deep learning with tensorflow ask them about tensorflow machine learning and deep learning in particular deploying ml in products and their favorite resources to learn more about ml or any other question you might have
ram sriharsha reviews major developments in apache spark 20 and discusses future directions for the project to make spark faster and easier to use for a wider array of workloads with an emphasis on api evolution singlenode performance project tungsten phase 3 and structured streaming
the amount of cuttingedge technology that azure puts at your fingertips is incredible tasks like building a web server or integration workflow that used to take weeks can now be accomplished in seconds and bundling azure services allows developers to create solutions that even a few months ago would have seemed out of reach artificial intelligence is no exception azure enables sophisticated capabilities in artificial intelligence machine learning deep learning cognitive services and advanced analytics rimma nehme explains why azure is the next ai supercomputer and how this vision is being implemented in reality along the way rimma explains how to use azure to process data at any scale and how to compose tools such as spark and r and do scaleout querying on demand at massive scale
at pg the global business services organization delivers many shared services including the core data infrastructure and applications from data warehouses to business intelligence and advanced analytics terry mcfadden and priyank patel discuss procter and gamble’s threeyear journey to enable production applications with oncluster bi technology exploring in detail the architecture challenges and choices made by the team along this journey evaluation criteria and how the final choice arcadia fit with the big data infrastructure
amazon kinesis is a fully managed cloudbased service for realtime data processing over large distributed data streams customers who use amazon kinesis can continuously capture and process realtime data such as iot sensor data website clickstreams financial transactions social media feeds it logs locationtracking events and more roy benalta explores the amazon kinesis platform in detail and discusses best practices for scaling your core streaming data ingestion pipeline as well as realworld customer use cases and design pattern integration with amazon elasticsearch aws lambda and apache spark
data has long stopped being structured and flat but the results of our analysis are still rendered as flat bar charts and scatter plots we live in a 3d world and we need to be able to enable data interaction from all perspectives robert thomas offers an overview of immersive visualization—integrated with notebooks and powered by spark—which helps bring insights to life
data lineage is critical to answering a wide range of questions about how data is being used within an organization which datasets and table columns are driving key performance indicators how is certain privacysensitive data being used where do errors or outliers arise and how do they propagate forward where are inefficient or unnecessary processing steps being taken tracking data lineage is also critical in realworld use cases such as regulatory reporting and compliance
last year linkedin embarked on an ambitious mission to completely revamp the mobile experience for its members this would mean a completely new mobile application reimagined user experiences and new interaction concepts as the team evaluated the impact of this big rewrite on the data analytics ecosystem they observed a few problems
a major challenge in today’s big data world is getting data into a data lake in a simple automated way many organizations use python or another language to code their way through these processes but when the number of data sources increases into the hundreds—or often thousands—coding scripts for each source becomes time consuming and extremely difficult to manage and maintain
traditional security tools like security information and event managers siems are struggling to keep up with the terabytes of event data 250m to 2b events being generated each day from an evergrowing number of devices cybersecurity has become a data problem and enterprises need to reply with scalable solutions to enable effective hunting and combat evolving attacks rethinking the cybersecurity problem as a datacentric problem led accenture labs’s cybersecurity team to use emerging big data tools along with new approaches such as graph databases and analysis to exploit the connected nature of the data to its advantage joshua patterson michael wendt and keith kraus explain how accenture labs’s cybersecurity team is using apache kafka spark and flink to stream data into blazegraph and datastax graph to accelerate cyber defense

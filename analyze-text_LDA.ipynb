{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing sessions with topic modeling\n",
    "Develop topic summaries from original session descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.lda\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>topic</th>\n",
       "      <th>descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Parallel SQL and analytics with Solr</td>\n",
       "      <td>Analytics has increasingly become a major focu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>JupyterLab: The evolution of the Jupyter Notebook</td>\n",
       "      <td>Project Jupyter provides building blocks for i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Designing a location intelligence platform for...</td>\n",
       "      <td>CartoDB has enabled hundreds of thousands of u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>The future of column-oriented data processing ...</td>\n",
       "      <td>In pursuit of speed and efficiency, big data p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Beyond Hadoop at Yahoo: Interactive analytics ...</td>\n",
       "      <td>Yahoo initially built Hadoop as an answer to a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "1  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "2  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "3  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "4  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "\n",
       "                                               topic  \\\n",
       "0               Parallel SQL and analytics with Solr   \n",
       "1  JupyterLab: The evolution of the Jupyter Notebook   \n",
       "2  Designing a location intelligence platform for...   \n",
       "3  The future of column-oriented data processing ...   \n",
       "4  Beyond Hadoop at Yahoo: Interactive analytics ...   \n",
       "\n",
       "                                        descriptions  \n",
       "0  Analytics has increasingly become a major focu...  \n",
       "1  Project Jupyter provides building blocks for i...  \n",
       "2  CartoDB has enabled hundreds of thousands of u...  \n",
       "3  In pursuit of speed and efficiency, big data p...  \n",
       "4  Yahoo initially built Hadoop as an answer to a...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sessions = pd.read_pickle('data_train/strata_sessions.pkl')\n",
    "\n",
    "sessions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 584\n",
      "That's an average of 3 sentences per session description.\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i in sessions['descriptions']:\n",
    "    sent = i.split('. ')\n",
    "    for s in sent:\n",
    "        data.append(s.decode('utf-8'))\n",
    "        \n",
    "print 'Number of sentences:', len(data)\n",
    "print 'That\\'s an average of ' + str(len(data)/193) + \\\n",
    "    ' sentences per session description.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 584\n",
      "INFO:lda:vocab_size: 3020\n",
      "INFO:lda:n_words: 8227\n",
      "INFO:lda:n_topics: 100\n",
      "INFO:lda:n_iter: 2000\n",
      "INFO:lda:<0> log likelihood: -112354\n",
      "INFO:lda:<10> log likelihood: -84502\n",
      "INFO:lda:<20> log likelihood: -83686\n",
      "INFO:lda:<30> log likelihood: -82837\n",
      "INFO:lda:<40> log likelihood: -82626\n",
      "INFO:lda:<50> log likelihood: -82702\n",
      "INFO:lda:<60> log likelihood: -82598\n",
      "INFO:lda:<70> log likelihood: -82115\n",
      "INFO:lda:<80> log likelihood: -82687\n",
      "INFO:lda:<90> log likelihood: -82706\n",
      "INFO:lda:<100> log likelihood: -82425\n",
      "INFO:lda:<110> log likelihood: -82368\n",
      "INFO:lda:<120> log likelihood: -82297\n",
      "INFO:lda:<130> log likelihood: -82506\n",
      "INFO:lda:<140> log likelihood: -82366\n",
      "INFO:lda:<150> log likelihood: -82481\n",
      "INFO:lda:<160> log likelihood: -82717\n",
      "INFO:lda:<170> log likelihood: -82349\n",
      "INFO:lda:<180> log likelihood: -82233\n",
      "INFO:lda:<190> log likelihood: -82095\n",
      "INFO:lda:<200> log likelihood: -82556\n",
      "INFO:lda:<210> log likelihood: -82285\n",
      "INFO:lda:<220> log likelihood: -82244\n",
      "INFO:lda:<230> log likelihood: -82317\n",
      "INFO:lda:<240> log likelihood: -82652\n",
      "INFO:lda:<250> log likelihood: -82486\n",
      "INFO:lda:<260> log likelihood: -82147\n",
      "INFO:lda:<270> log likelihood: -82587\n",
      "INFO:lda:<280> log likelihood: -82374\n",
      "INFO:lda:<290> log likelihood: -82592\n",
      "INFO:lda:<300> log likelihood: -82430\n",
      "INFO:lda:<310> log likelihood: -82253\n",
      "INFO:lda:<320> log likelihood: -82352\n",
      "INFO:lda:<330> log likelihood: -82444\n",
      "INFO:lda:<340> log likelihood: -82552\n",
      "INFO:lda:<350> log likelihood: -82427\n",
      "INFO:lda:<360> log likelihood: -82705\n",
      "INFO:lda:<370> log likelihood: -82311\n",
      "INFO:lda:<380> log likelihood: -82377\n",
      "INFO:lda:<390> log likelihood: -82688\n",
      "INFO:lda:<400> log likelihood: -82623\n",
      "INFO:lda:<410> log likelihood: -82737\n",
      "INFO:lda:<420> log likelihood: -82323\n",
      "INFO:lda:<430> log likelihood: -82129\n",
      "INFO:lda:<440> log likelihood: -82237\n",
      "INFO:lda:<450> log likelihood: -82332\n",
      "INFO:lda:<460> log likelihood: -82491\n",
      "INFO:lda:<470> log likelihood: -82312\n",
      "INFO:lda:<480> log likelihood: -82509\n",
      "INFO:lda:<490> log likelihood: -82088\n",
      "INFO:lda:<500> log likelihood: -82388\n",
      "INFO:lda:<510> log likelihood: -82265\n",
      "INFO:lda:<520> log likelihood: -82203\n",
      "INFO:lda:<530> log likelihood: -82432\n",
      "INFO:lda:<540> log likelihood: -82528\n",
      "INFO:lda:<550> log likelihood: -82451\n",
      "INFO:lda:<560> log likelihood: -82761\n",
      "INFO:lda:<570> log likelihood: -82299\n",
      "INFO:lda:<580> log likelihood: -82456\n",
      "INFO:lda:<590> log likelihood: -82049\n",
      "INFO:lda:<600> log likelihood: -82480\n",
      "INFO:lda:<610> log likelihood: -82386\n",
      "INFO:lda:<620> log likelihood: -82246\n",
      "INFO:lda:<630> log likelihood: -82330\n",
      "INFO:lda:<640> log likelihood: -82486\n",
      "INFO:lda:<650> log likelihood: -81962\n",
      "INFO:lda:<660> log likelihood: -82304\n",
      "INFO:lda:<670> log likelihood: -82636\n",
      "INFO:lda:<680> log likelihood: -82204\n",
      "INFO:lda:<690> log likelihood: -82411\n",
      "INFO:lda:<700> log likelihood: -82459\n",
      "INFO:lda:<710> log likelihood: -82124\n",
      "INFO:lda:<720> log likelihood: -82397\n",
      "INFO:lda:<730> log likelihood: -82264\n",
      "INFO:lda:<740> log likelihood: -82056\n",
      "INFO:lda:<750> log likelihood: -82246\n",
      "INFO:lda:<760> log likelihood: -82394\n",
      "INFO:lda:<770> log likelihood: -82397\n",
      "INFO:lda:<780> log likelihood: -82338\n",
      "INFO:lda:<790> log likelihood: -82445\n",
      "INFO:lda:<800> log likelihood: -82289\n",
      "INFO:lda:<810> log likelihood: -82296\n",
      "INFO:lda:<820> log likelihood: -82521\n",
      "INFO:lda:<830> log likelihood: -82697\n",
      "INFO:lda:<840> log likelihood: -82356\n",
      "INFO:lda:<850> log likelihood: -82593\n",
      "INFO:lda:<860> log likelihood: -82370\n",
      "INFO:lda:<870> log likelihood: -82468\n",
      "INFO:lda:<880> log likelihood: -82802\n",
      "INFO:lda:<890> log likelihood: -82451\n",
      "INFO:lda:<900> log likelihood: -82313\n",
      "INFO:lda:<910> log likelihood: -82386\n",
      "INFO:lda:<920> log likelihood: -82309\n",
      "INFO:lda:<930> log likelihood: -82350\n",
      "INFO:lda:<940> log likelihood: -82095\n",
      "INFO:lda:<950> log likelihood: -82764\n",
      "INFO:lda:<960> log likelihood: -82318\n",
      "INFO:lda:<970> log likelihood: -82712\n",
      "INFO:lda:<980> log likelihood: -82478\n",
      "INFO:lda:<990> log likelihood: -82454\n",
      "INFO:lda:<1000> log likelihood: -82382\n",
      "INFO:lda:<1010> log likelihood: -82563\n",
      "INFO:lda:<1020> log likelihood: -82267\n",
      "INFO:lda:<1030> log likelihood: -82427\n",
      "INFO:lda:<1040> log likelihood: -82685\n",
      "INFO:lda:<1050> log likelihood: -82364\n",
      "INFO:lda:<1060> log likelihood: -82205\n",
      "INFO:lda:<1070> log likelihood: -82340\n",
      "INFO:lda:<1080> log likelihood: -82397\n",
      "INFO:lda:<1090> log likelihood: -82204\n",
      "INFO:lda:<1100> log likelihood: -82445\n",
      "INFO:lda:<1110> log likelihood: -82864\n",
      "INFO:lda:<1120> log likelihood: -82615\n",
      "INFO:lda:<1130> log likelihood: -82856\n",
      "INFO:lda:<1140> log likelihood: -82522\n",
      "INFO:lda:<1150> log likelihood: -82044\n",
      "INFO:lda:<1160> log likelihood: -82270\n",
      "INFO:lda:<1170> log likelihood: -82341\n",
      "INFO:lda:<1180> log likelihood: -82306\n",
      "INFO:lda:<1190> log likelihood: -82353\n",
      "INFO:lda:<1200> log likelihood: -82414\n",
      "INFO:lda:<1210> log likelihood: -82647\n",
      "INFO:lda:<1220> log likelihood: -82660\n",
      "INFO:lda:<1230> log likelihood: -82433\n",
      "INFO:lda:<1240> log likelihood: -82739\n",
      "INFO:lda:<1250> log likelihood: -82485\n",
      "INFO:lda:<1260> log likelihood: -82050\n",
      "INFO:lda:<1270> log likelihood: -82119\n",
      "INFO:lda:<1280> log likelihood: -82250\n",
      "INFO:lda:<1290> log likelihood: -82686\n",
      "INFO:lda:<1300> log likelihood: -82581\n",
      "INFO:lda:<1310> log likelihood: -82345\n",
      "INFO:lda:<1320> log likelihood: -82450\n",
      "INFO:lda:<1330> log likelihood: -82092\n",
      "INFO:lda:<1340> log likelihood: -82439\n",
      "INFO:lda:<1350> log likelihood: -82622\n",
      "INFO:lda:<1360> log likelihood: -82359\n",
      "INFO:lda:<1370> log likelihood: -82268\n",
      "INFO:lda:<1380> log likelihood: -82170\n",
      "INFO:lda:<1390> log likelihood: -82182\n",
      "INFO:lda:<1400> log likelihood: -82394\n",
      "INFO:lda:<1410> log likelihood: -82074\n",
      "INFO:lda:<1420> log likelihood: -82663\n",
      "INFO:lda:<1430> log likelihood: -82245\n",
      "INFO:lda:<1440> log likelihood: -82441\n",
      "INFO:lda:<1450> log likelihood: -82163\n",
      "INFO:lda:<1460> log likelihood: -82221\n",
      "INFO:lda:<1470> log likelihood: -82053\n",
      "INFO:lda:<1480> log likelihood: -82357\n",
      "INFO:lda:<1490> log likelihood: -82411\n",
      "INFO:lda:<1500> log likelihood: -82232\n",
      "INFO:lda:<1510> log likelihood: -82376\n",
      "INFO:lda:<1520> log likelihood: -82390\n",
      "INFO:lda:<1530> log likelihood: -82413\n",
      "INFO:lda:<1540> log likelihood: -82364\n",
      "INFO:lda:<1550> log likelihood: -82500\n",
      "INFO:lda:<1560> log likelihood: -82566\n",
      "INFO:lda:<1570> log likelihood: -82929\n",
      "INFO:lda:<1580> log likelihood: -82733\n",
      "INFO:lda:<1590> log likelihood: -82358\n",
      "INFO:lda:<1600> log likelihood: -82183\n",
      "INFO:lda:<1610> log likelihood: -82082\n",
      "INFO:lda:<1620> log likelihood: -82505\n",
      "INFO:lda:<1630> log likelihood: -82412\n",
      "INFO:lda:<1640> log likelihood: -82349\n",
      "INFO:lda:<1650> log likelihood: -82636\n",
      "INFO:lda:<1660> log likelihood: -82625\n",
      "INFO:lda:<1670> log likelihood: -82353\n",
      "INFO:lda:<1680> log likelihood: -82574\n",
      "INFO:lda:<1690> log likelihood: -82732\n",
      "INFO:lda:<1700> log likelihood: -82244\n",
      "INFO:lda:<1710> log likelihood: -82192\n",
      "INFO:lda:<1720> log likelihood: -82000\n",
      "INFO:lda:<1730> log likelihood: -81945\n",
      "INFO:lda:<1740> log likelihood: -82452\n",
      "INFO:lda:<1750> log likelihood: -82352\n",
      "INFO:lda:<1760> log likelihood: -82729\n",
      "INFO:lda:<1770> log likelihood: -82671\n",
      "INFO:lda:<1780> log likelihood: -82286\n",
      "INFO:lda:<1790> log likelihood: -82243\n",
      "INFO:lda:<1800> log likelihood: -82303\n",
      "INFO:lda:<1810> log likelihood: -82408\n",
      "INFO:lda:<1820> log likelihood: -82067\n",
      "INFO:lda:<1830> log likelihood: -82384\n",
      "INFO:lda:<1840> log likelihood: -82418\n",
      "INFO:lda:<1850> log likelihood: -82364\n",
      "INFO:lda:<1860> log likelihood: -82285\n",
      "INFO:lda:<1870> log likelihood: -81942\n",
      "INFO:lda:<1880> log likelihood: -82701\n",
      "INFO:lda:<1890> log likelihood: -82235\n",
      "INFO:lda:<1900> log likelihood: -82433\n",
      "INFO:lda:<1910> log likelihood: -82143\n",
      "INFO:lda:<1920> log likelihood: -82216\n",
      "INFO:lda:<1930> log likelihood: -82395\n",
      "INFO:lda:<1940> log likelihood: -82213\n",
      "INFO:lda:<1950> log likelihood: -82560\n",
      "INFO:lda:<1960> log likelihood: -82200\n",
      "INFO:lda:<1970> log likelihood: -82759\n",
      "INFO:lda:<1980> log likelihood: -82235\n",
      "INFO:lda:<1990> log likelihood: -82467\n",
      "INFO:lda:<1999> log likelihood: -82187\n"
     ]
    }
   ],
   "source": [
    "# http://mike.place/talks/pygotham/#p1\n",
    "documents = data\n",
    "\n",
    "vec = CountVectorizer(stop_words='english', binary=True)\n",
    "\n",
    "X = vec.fit_transform(documents)\n",
    "\n",
    "vocab = sorted(vec.vocabulary_, key=vec.vocabulary_.get)  # you'll need this later\n",
    "\n",
    "lda_model = lda.LDA(n_topics=100)\n",
    "\n",
    "model = lda_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topn_indices(arr, n):\n",
    "    '''Indices of the top n elements in arr, sorted by value'''\n",
    "    return np.argsort(-arr)[:n]\n",
    "def print_lda(lda_model, n_words=10):\n",
    "    '''Print top n_words words for all topics of an LDA model'''\n",
    "    for i, _ in enumerate(lda_model.topic_word_):\n",
    "        print_topic(model, i)\n",
    "def print_topic(lda_model, i, n_words=10):\n",
    "    '''Print top n_words words from topic i'''\n",
    "    top_words = topn_indices(lda_model.topic_word_[i], n_words)\n",
    "    words = np.array(vocab)[top_words]\n",
    "    print('*{}* {}'.format(i, ' '.join(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0* company architectures high manage ted requires value promise hdfs emerging\n",
      "*1* code interactive visualizations python computing web jupyter choices building output\n",
      "*2* information quickly scale problem years levels odpi greater ot integrates\n",
      "*3* open source single organizations project difficult introduces rapid billions architecture\n",
      "*4* time real challenge stack used case metrics druid custom caravel\n",
      "*5* data complex science getting project design languages considering exploration mature\n",
      "*6* models text unstructured making big significant applied deliver meng start\n",
      "*7* data world enable come 2015 required narrative nature results figures\n",
      "*8* central customer ll metrics need requirements interoperability dashboards historical general\n",
      "*9* industry use technology healthcare parallel internal better solr scores enforcement\n",
      "*10* data permanente gain connected kaiser today explain concept latency enabled\n",
      "*11* critical language days similar compliance matter spot concerned session 40\n",
      "*12* algorithms creating right numbers means insight consume activities file options\n",
      "*13* use operational deeper ready trends political hardware contribute reducing investigations\n",
      "*14* increasing big number center used using history lineage driving attacks\n",
      "*15* presents thousands data way calling agility millions execution science hundreds\n",
      "*16* using good biggest brief turns reliability contrasts executives predict twists\n",
      "*17* techniques using modeling perform covers flink given kostas innovation state\n",
      "*18* data driven new insights business year expert visa payments beautiful\n",
      "*19* best practices explores discusses john lessons scalable deep pattern ben\n",
      "*20* data management lake governance fraud important marketing point compliance deliver\n",
      "*21* applications hadoop production operations deployments running needed holden man likely\n",
      "*22* time real insights near examples data patients interesting designing approach\n",
      "*23* growing like effectively warehouses engineering leading decade collaboration uma rate\n",
      "*24* analytics platform increasingly range wide questions choose managers integrate aws\n",
      "*25* analysis graph technologies businesses reach host intensive linkedin workflow plots\n",
      "*26* organizations emerging technology solving spark leads leading cover massive cybersecurity\n",
      "*27* new set streams required including difficult popular news received hours\n",
      "*28* power platform people looking uses agility shifted io shared experiment\n",
      "*29* provide processes kafka order pipeline enabling written talend flexibility druid\n",
      "*30* predictive compute resource users power delivering knowledge age database affect\n",
      "*31* data like infrastructure services types run sharing initiatives developers privacy\n",
      "*32* optimize focus transformation jeremy emit formula allowing did ec ingo\n",
      "*33* analytics business intelligence advanced artificial organization years companies ai level\n",
      "*34* science today landscape learned achieve case changed amy time systems\n",
      "*35* using software security approach grow dj house overmann lynn ways\n",
      "*36* spark apache streaming processing batch stream structured core including simplified\n",
      "*37* devices iot internet fast based things industrial industries consumer exploit\n",
      "*38* cost storage databases demonstrates table processing standard flexibility enrichment csv\n",
      "*39* journey opportunities easy dell recent help emc hearst expected offering\n",
      "*40* cloud google related beam model increasingly pipelines dataflow transactions focuses\n",
      "*41* faster easier reduce explain makes providing discusses having step according\n",
      "*42* data big adoption lakes developers enterprise meet power taking research\n",
      "*43* learning machine deep problems scientists recent statistics statistical frameworks tensorflow\n",
      "*44* distributed patterns analysts cloud payment writing britain environments daily different\n",
      "*45* year story members varied cost processed accuracy coming competitors care\n",
      "*46* current delivery exciting conditions ml changing order 000 grocery share\n",
      "*47* columnar including kudu process foundation arrow parquet apache memory impala\n",
      "*48* data effective adapt enterprises smaller years authorization terabytes doors companies\n",
      "*49* service enterprises opportunity based reviews quality likely does deploy investigation\n",
      "*50* process human platforms model analyzing value coding highly standard assess\n",
      "*51* customer revenue ve product market methods free ideas partners allows\n",
      "*52* large processing bring project kafka working systems require jane lines\n",
      "*53* speed role daunting failure democratization virtual challenge pipeline luis quench\n",
      "*54* new make enable managing higher experience user teach inferences breed\n",
      "*55* amazon applications costs create lower risk jonathan s3 problems expensive\n",
      "*56* available benefits competitive feature end innovation approaches replication containers time\n",
      "*57* just ability outcomes typically individual means approach 300 detailed beginning\n",
      "*58* built day events entirely emerging evolving time ranging todd component\n",
      "*59* data running netflix cars datasets 50 workloads curated produce accordingly\n",
      "*60* hadoop enterprises traditional premises cloud using generated prevalent deployed fields\n",
      "*61* data modern environment highlights differences similarities computing sensory 50 number\n",
      "*62* business architecture changing considerations key uber security frameworks hotels example\n",
      "*63* enterprise software analyze major analytics content services automated choice predicting\n",
      "*64* drive industries create advantages machines marketplace fail feather leveraging studies\n",
      "*65* challenges results infrastructure faced hadoop traditional exploring triggers criteria doing\n",
      "*66* areas social volume law needs velocity records gaining times legal\n",
      "*67* help improve able content strong monitors fix liming processed srinivas\n",
      "*68* users simple create discuss need entire profile fully generate datasets\n",
      "*69* variety sources streaming complexity external ways balance replicated country combining\n",
      "*70* strata hadoop world skills croll cutting program alistair share chairs\n",
      "*71* tools explains data impact event components used share visualization significantly\n",
      "*72* need new organizations layers means growth different better customer brings\n",
      "*73* provides access deploying flexible extend datasets teams used automating choices\n",
      "*74* world building integration explores kudu series cornerstones start development processor\n",
      "*75* data big solutions companies explore systems harness capture personalization shape\n",
      "*76* hadoop ecosystem understand using seen number increases touch learn create\n",
      "*77* analytics join questions learn hear don answer specific ask asking\n",
      "*78* offers overview work engine support sessionization cycles scalability hall wang\n",
      "*79* big team reporting final successful associated openness nokia accenture years\n",
      "*80* way make people topics great similar engineers high informally birds\n",
      "*81* key different million continues flat collecting expanding solution challenges facing\n",
      "*82* platforms online operational pinterest technologies handle behaviors connecting continuing chip\n",
      "*83* information financial actions customers sensor platform massive media fundamentally develop\n",
      "*84* evolution example efficiency investment problem goals flu requirement administrators fatigue\n",
      "*85* network mobile understand space 3x positive public wave widespread providing\n",
      "*86* spark explains hadoop common technologies formats apache like taking yahoo\n",
      "*87* long consumer decisions task based term inefficient winning candidates advantage\n",
      "*88* scale clusters large feel challenges especially moves scientist ui ec2\n",
      "*89* data storage analytics volume centric hard ods increase larger maybe\n",
      "*90* value enterprise moving projects quality specifically community allow total accelerate\n",
      "*91* insights visualization public meet outlines understanding end james build computation\n",
      "*92* application development cycle visualization architectures rapidly subject columnar yp damages\n",
      "*93* use cases real services analytic value edge tracking small healthcare\n",
      "*94* second path implementations quite tasks 40 responsibility brought models predict\n",
      "*95* field cloud flexibility better martin elements consider dynamics service start\n",
      "*96* performance future sql cluster systems query workloads common deployment design\n",
      "*97* digital architecture services multiple businesses store features iot hypothesis visual\n",
      "*98* possible people java study data uses know garbage helps products\n",
      "*99* capabilities key ai ecosystem reality azure access brad wicke mixed\n"
     ]
    }
   ],
   "source": [
    "print_lda(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

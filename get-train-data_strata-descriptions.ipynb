{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup(domain, path):\n",
    "    '''\n",
    "    Function to pass in url (domain + path) to get \n",
    "    html via Requests, and parse via Beautiful Soup\n",
    "    '''\n",
    "    # use requests to grab page via url\n",
    "    url = domain + path\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # store html for parsing via soup\n",
    "    html = r.text\n",
    "    \n",
    "    # create soup\n",
    "    soup = bs4.BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# webpage domain\n",
    "domain = 'http://conferences.oreilly.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get parsed html of wed and thurs schedule pages\n",
    "soup_wed = get_soup(domain,'/strata/hadoop-big-data-ny/public/schedule/grid/public/2016-09-28?view=list')\n",
    "soup_thurs = get_soup(domain,'/strata/hadoop-big-data-ny/public/schedule/grid/public/2016-09-29?view=list')\n",
    "\n",
    "# confirm results\n",
    "print soup_wed.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all URLs from page\n",
    "topic_wed = soup_wed.find_all('a', href=re.compile('schedule/detail/'))\n",
    "topic_thurs = soup_thurs.find_all('a', href=re.compile('schedule/detail/'))\n",
    "\n",
    "# combine links from wed and thurs schedule pages\n",
    "topic_links = topic_wed + topic_thurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Parallel SQL and analytics with Solr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>JupyterLab: The evolution of the Jupyter Notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Designing a location intelligence platform for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>The future of column-oriented data processing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Beyond Hadoop at Yahoo: Interactive analytics ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "1  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "2  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "3  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "4  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "\n",
       "                                               topic  \n",
       "0               Parallel SQL and analytics with Solr  \n",
       "1  JupyterLab: The evolution of the Jupyter Notebook  \n",
       "2  Designing a location intelligence platform for...  \n",
       "3  The future of column-oriented data processing ...  \n",
       "4  Beyond Hadoop at Yahoo: Interactive analytics ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize dict\n",
    "link_dict = {\n",
    "    'link' : [],\n",
    "    'topic': []\n",
    "}\n",
    "\n",
    "# create dictionary of text and links\n",
    "for link in topic_links:\n",
    "    link_dict['link'].append(link.get('href'))\n",
    "    link_dict['topic'].append(link.text)\n",
    "    \n",
    "# convert dict to dataframe\n",
    "sessions_df = pd.DataFrame(link_dict)\n",
    "sessions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Analytics has increasingly become a major focus for Apache Solr, the primary search engine in the Hadoop stack. Yonik Seeley explores recent Apache Solr features in the areas of faceting and analytics, including parallel <span class=\"caps\">SQL</span>, streaming expressions, distributed join, and distributed graph queries. Given the increasing number of APIs and techniques that can be brought to bear, Yonik also covers the trade-offs of different approaches and strategies for maximizing scalability.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Project Jupyter provides building blocks for interactive and exploratory computing that make science and data science reproducible across over 40 programming languages (Python, Julia, R, etc.). Central to the project is the Jupyter Notebook, a web-based, interactive computing platform that allows users to author data- and code-driven narratives (computational narratives) that combine live code, equations, narrative text, visualizations, interactive dashboards, and other media. While the Jupyter Notebook has proved to be an incredibly productive way of working interactively with code and data, it is helpful to decompose notebooks into more primitive building blocks: kernels for code execution, input areas for typing code, markdown cells for composing narrative content, output areas for showing results, terminals, etc.</p>\\n<p>Brian Granger, Sylvain Corlay, and Jason Grout offer an overview of JupyterLab, the next-generation user interface for Project Jupyter that puts Jupyter Notebooks within a powerful user interface that allows the building blocks of interactive computing to be assembled to support the wide range of interactive workflows used in data science. The fundamental idea of JupyterLab is to offer a user interface that allows users to assemble these building blocks in different ways to support interactive workflows that include, but go far beyond, Jupyter Notebooks. JupyterLab accomplishes this by providing a modular and extensible user interface that exposes these building blocks in the context of a powerful work space that allows users to arrange multiple notebooks, text editors, terminals, and output areas on a single page with multiple panels, tabs, splitters, and collapsible sidebars with a file browser, command palette, and integrated help system. The codebase and UI of JupyterLab is based on a flexible plugin system that makes it easy to extend with new components. Brian, Sylvain, and Jason demonstrate the JupyterLab interface and describe how it fits within the overall roadmap of the project.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>CartoDB has enabled hundreds of thousands of users to visualize their data as beautiful maps to gain insights and share stories. However, these maps contain information that often can\\u2019t be uncovered by visualization alone. By applying geospatial statistical methods and machine learning, new stories and understanding can be extracted and predictions can be made.</p>\\n<p>Applying these methods to geospatial data is powerful but also highly technical and comes with a series of caveats and complexities that are not necessarily understood by our target users. So how do you enable diverse users to access advanced methods? CartoDB\\u2019s solution has been to tightly pair analysis, data, and cartography inside an easy-to-use user interface.</p>\\n<p>Stuart Lynn and Andy Eschbacher offer an overview of CartoDB\\u2019s platform, which combines an intuitive visual language to build analysis chains, novel, beautiful cartography that is tailored to communicate the results of these analyses, and an exploration interface that allows insights to be discovered in data through a simple iterative approach. Any one of these components alone is not sufficient; the interplay between them is essential to helping users understand more from geospatial data.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>In pursuit of speed and efficiency, big data processing is continuing its logical evolution toward columnar execution. A number of key big data technologies, including Kudu, Ibis, and Drill, have or will soon have in-memory columnar capabilities. The solid foundation laid by Apache Arrow and Apache Parquet for a shared columnar representation across the ecosystem promises a great future. Modern CPUs achieve higher throughput using <span class=\"caps\">SIMD</span> instructions and vectorization on Arrow\\u2019s columnar in-memory representation. Similarly, Parquet provides storage and I/O optimized columnar data access using statistics and appropriate encodings. For interoperability, row-based encodings (<span class=\"caps\">CSV</span>, Thrift, Avro) combined with general-purpose compression algorithms (GZip, <span class=\"caps\">LZO</span>, Snappy) are common but inefficient. The Arrow and Parquet projects define standard columnar representations allowing interoperability without the usual cost of serialization.</p>\\n<p>Jacques Nadeau, vice president of Apache Arrow, and Julien Le Dem, vice president of Apache Parquet, discuss the future of columnar data processing and the hardware trends it takes advantage of. Arrow-based interconnection between the various big data tools (<span class=\"caps\">SQL</span>, UDFs, machine learning, big data frameworks, etc.) enable them to be used together seamlessly and efficiently without overhead: when collocated on the same processing node, read-only shared memory and <span class=\"caps\">IPC</span> avoid communication overhead; when remote, scatter-gather I/O sends the memory representation directly to the socket, avoiding serialization costs, and soon <span class=\"caps\">RDMA</span> will allow exposing data remotely. As in-memory processing becomes more popular, the traditional tiering of <span class=\"caps\">RAM</span> as working space and <span class=\"caps\">HDD</span> as persistent storage is outdated. More tiers are now available like SSDs and nonvolatile memory, which provide much higher data density and achieve a latency close to <span class=\"caps\">RAM</span> at a fraction of the cost. Execution engines can take advantage of more granular tiering and avoid the traditional spilling to disk, which impacts performance by an order of magnitude when the working dataset does not fit in main memory.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Yahoo initially built Hadoop as an answer to a very acute pain around efficiently storing and processing large volumes of data. Since Yahoo open sourced Hadoop, it has become widely adopted in the technology world. However, time has taught us that when a system becomes extremely popular for solving one class of problems, its limitations in solving other problems become more apparent. Himanshu Gupta explains why Yahoo has been increasingly investing in interactive analytics and how it leverages Druid to power a variety of internal- and external-facing data applications.</p>\\n<p>Millions of users around the globe interact with Yahoo through their web browsers and mobile devices, and these interactions generate billions of events every day. As Yahoo\\u2019s data volumes have grown, it\\u2019s faced increasing demand to make the data more accessible, both to internal users and to its customers. Not all of Yahoo\\u2019s end users are backend analysts, and many have no prior experience with traditional analytic tools, so Yahoo wanted to build simple, interactive data applications that anyone could use to derive insights from data. To support these use cases, Yahoo elected to invest in the Druid open source project.</p>\\n<p>Today, Yahoo has multiple Druid clusters to support analytics for a variety of use cases, such as application performance, user activity, ads metrics, and many more. Each demands that Yahoo\\u2019s data applications update in real time and handle interactive ad hoc querying at a very high scale. Himanshu explores Yahoo\\u2019s use cases with Druid, shares the lessons learned from scaling Druid deployment, monitoring clusters, and ingesting data, and offers strategies for accelerating queries by leveraging approximate sketch-based algorithms.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>The Netflix data platform is constantly evolving, but fundamentally it\\u2019s an all-cloud platform at a massive scale (40+ PB and over 700 billion new events per day) focused on empowering developers. Kurt Brown dives into the current technology landscape at Netflix and offers some thoughts on what the future holds.</p>\\n<p>Kurt covers key technologies, such as Spark, Presto, Docker, and Jupyter, along with many broader data ecosystem facets (metadata, insights into jobs run, visualizing big data, etc.) and considerations (such as accessibility and necessary compromises). You\\u2019ll leave with greater insight into how things work at Netflix and be inspired to re-envision your data platform.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Whether we\\u2019re talking about spam emails, merging records, or investigating clusters, there are many times when having a measure of how alike things are makes them easier to work with. You may have unstructured or vague data that isn\\u2019t incorporated into your data models (e.g., information from subject-matter experts who have a sense of whether something is good or bad, similar or different). Melissa Santos offers a practical approach to creating a distance metric and validating with business owners that it provides value\\u2014providing you with the tools to turn that expert information into numbers you can compare and use to quickly see structures in the data.</p>\\n<p>Melissa walks you through setting expectations for a distance, creating distance metrics, iterating with experts to check expectations, validating the distance on a large chunk of the dataset, and then circling back to add more complexity and shares some real-world examples, such as distance from usual emails from a domain, quality scores for geographic data, and merging person records if they are sufficiently similar.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>What is a distance?</li>\\n<li>Turning expert opinion into training data</li>\\n<li>Making a very basic model</li>\\n<li>Why your model is wrong</li>\\n<li>Making it better</li>\\n<li>Working with experts and stakeholders to validate usefulness</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>The new world of data exploration using virtual, augmented, and mixed reality brings powerful new capabilities and challenges for data visualization engineers, designers, and professionals. Brad Sarsfield demonstrates new input and output capabilities of holographic computing in relation to mixed-reality data visualizations with examples that are both visually beautiful and deeply practical.</p>\\n<p>Holograms let us make smarter decisions and explore ideas faster by inspecting every vantage point of our data and interacting with it in new, more personal and human ways. There are new rules for the new world. Join Brad as he explores and experiments with the possibilities of the next generation of data visualization experiences.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>How holograms trick your brain into thinking they are real</li>\\n<li>New data visualization capabilities: Seeing 3D in 3D</li>\\n<li>How to use gaze, gestures, and voice to manipulate data</li>\\n<li>How 3D spatial sound can become a part of data visualization</li>\\n<li>Virtual there: Collaboration and exploration with others through virtual presence</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p><span class=\"caps\">FINRA</span> ingests over 50 billion records of stock market trading data daily, and its examiners, compliance officers, and analysts look at different slices of this data to draw powerful insights on market behavior and to identify anomalies in trading patterns. Janaki Parameswaran and Kishore Ramachandran explain how <span class=\"caps\">FINRA</span> technology integrates data feeds from disparate systems to provide analytics and visuals for regulating equities, options, and fixed-income markets.</p>\\n<p>A secure, cloud-based approach to store and process data has opened up an array of options for analyzing large volumes of data. <span class=\"caps\">FINRA</span> technology empowers market analysts with tools that enable cascading inquiries across the entire ecosystem of data. <span class=\"caps\">FINRA</span> is building a collaborative analytics platform to support the analytical and statistical needs of market analysts; technologists and analysts can leverage this collaborative platform to prioritize and deploy effective data visualization frameworks and efficient drill-down solutions.</p>\\n<p>Janaki and Kishore share actionable strategies for creating a secure ecosystem of tools that range from simple to complex predictive analytics and from simple heat maps to complex data models, all unified to draw on each other\\u2019s strengths and yet adaptive to address changing business analytic needs.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Data lineage is critical to answering a wide range of questions about how data is being used within an organization. Which datasets and table columns are driving key performance indicators? How is certain privacy-sensitive data being used? Where do errors or outliers arise, and how do they propagate forward? Where are inefficient or unnecessary processing steps being taken? Tracking data lineage is also critical in real-world use cases such as regulatory reporting and compliance.</p>\\n<p>Sean Kandel presents novel interactive visualizations for exploring data lineage across multiple levels of detail. From high-level overviews of input-output relationships to fine-grained column dependency tracking, Sean explains how analysts can rapidly navigate lineage data and formulate provenance queries to gain insight into how data is being processed and transformed. By incorporating summary statistics, distributions, and data quality metrics, these visualizations can further augment lineage views to jointly inspect schema-level metadata and the results of large-scale data processing.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Which suppliers are most likely to have delivery or quality issues? Does service, product placement, or price make the biggest difference in customer sentiment? Finding the answers to these questions in structured data is often straightforward, but can we answer them using the unstructured data (free text) in emails, social media, call center transcripts, product reviews, and other sources?</p>\\n<p>Mark Turner explains how to clearly see the associations between any two variables in text data by combining large-scale in-database text analytics and the bipartite graph visualization technique. Mark describes which text analytics methods to use for various operational business questions and how to show the associations clearly in a bipartite graph, offering insight into which associations are strongest and which are weakest. This powerful combination of methods gives operational value to the increasingly huge amounts of text data, in which customers express their likes, dislikes, preferences, and issues.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Visual analysis is changing in the era of <span class=\"caps\">GPU</span> clusters. Now that scale compute is easier, the bottleneck is mapping data to visualizations and intelligently interacting with them. Using datasets uploaded to Graphistry, Leo Meyerovich provides a glimpse into the emerging workflows for graph and linked event analysis and offers common tricks for success.</p>\\n<p>Leo first outlines the next generation of visual graph analysis. The driving force is the availability of scalable tools: the old bottlenecks are disappearing. Whether exploring billions of attack events or decades of hip hop collaborations, the act of graph analysis at scale is finally becoming accessible and interactive. Leo explains what that looks like in practice.</p>\\n<p>Leo then offers an overview of how the scaling bottleneck is being eliminated and explores the tools and techniques early adopters are now spending more time on: mapping data into useful graph representations and focusing on more quickly interacting with it. Previously, much of a project would have been spent massaging big datasets, designing sophisticated algorithms, and engineering a user interface. Scalable compute tools like Spark\\u2019s GraphX and Systap\\u2019s <span class=\"caps\">GPU</span>-accelerated Blazegraph are solving the first two problems, and for the visual tier, Graphistry\\u2019s embeddable <span class=\"caps\">GPU</span> cloud visual toolkit has unlocked magnitudes more performance with little work. The combined result is that scaling end-to-end interactive workflows has become a problem of the past.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Kudu is redefining the big data ecosystem and opening doors to capabilities not available before. Comcast is moving in the direction of adopting Kudu with Impala and Spark for several projects, including real-time processing of events from Xfinity devices. Sridhar Alla and Kiran Muglurmath explain how real-time analytics on Comcast Xfinity set-top boxes (STBs) help drive several customer-facing and internal data-science-oriented applications and how Comcast uses Kudu to fill the gaps in batch and real-time storage and computation needs, allowing Comcast to process the high-speed data without the elaborate solutions needed till now.</p>\\n<p>Sridhar and Kiran showcase the platform Comcast is testing using Kudu: real-time <span class=\"caps\">STB</span> events (tunes) are streamed from Kafka to Spark, which updates Kudu tables with high speed (~5,000 eps) while also sessionizing and maintaining state for tens of millions of devices in Kudu. While the Spark platform updates the transactions in real time directly on <span class=\"caps\">HDFS</span>, the middle tier accesses Kudu tables (through Impala) to generate subsecond real-time dashboards while still having the power of Hadoop to deliver batch analytics and integrations with other platforms. This is key to the success of the platform as previously Comcast had to rely on variety of multitiered architectures to both provide fast storage and be able to update just like NoSQL engines\\u2014but without the slowness caused by several thousand updates per second. Sridhar and Kiran also explore how Comcast stores half-a-trillion events using Kudu and still gets great performance analyzing the data.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>For a long time, a substantial portion of the data processing that companies did ran as big batch jobs\\u2014CSV files dumped out of databases, log files collected at the end of the day, etc. But businesses operate in real time, and the software they run is catching up. Rather than processing data only at the end of the day, why not react to it continuously as the data arrives? This is the emerging world of stream processing.</p>\\n<p>But stream processing only becomes possible when the fundamental data capture is done in a streaming fashion; after all, you can\\u2019t process a daily batch of <span class=\"caps\">CSV</span> dumps as a stream. This shift toward stream processing has driven the popularity of Apache Kafka. Making all an organization\\u2019s data available centrally as free-flowing streams enables business logic to be represented as stream processing operations. Essentially, applications are stream processors in this new world of stream processing.</p>\\n<p>Neha Narkhede explains how Apache Kafka serves as a foundation to streaming data applications that consume and process real-time data streams and introduces Kafka Connect, a system for capturing continuous data streams, and Kafka Streams, a lightweight stream processing library. Neha also describes the lessons companies like LinkedIn learned building massive streaming data architectures.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Moving from batch to streaming involves changing how we think about time. Streaming data is neither bounded nor typically well ordered in time. However, to make streaming systems useful and deliver on the promise of low-latency results, we often want to know when we have all the data relevant to emitting a correct aggregation. Watermarks provide the foundation for making such decisions, enabling streaming systems to emit timely, correct results when processing out-of-order data.</p>\\n<p>Given the trend toward out-of-order processing in existing streaming systems, understanding watermarks is an increasingly important skill when designing pipelines. This methodology, first discussed in the <a href=\"http://research.google.com/pubs/pub41378.html\">MillWheel paper</a> and further explored in the <a href=\"http://research.google.com/pubs/pub43864.html\">Dataflow model paper</a>, is now referred to as the Beam model. This approach is not limited to just Google\\u2019s stream processing efforts; rather, it is a solution to a general problem that must be addressed by any system that wishes to provide timely out-of-order distributed stream processing and has since been pursued by others such as <a href=\"http://data-artisans.com/high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink/\">Flink</a> and Qubit (which built a watermark tracking system on top of Spark Streaming for its own internal use).</p>\\n<p>Based on his experience developing and using watermarks at Google, Slava Chernyak discusses details of how watermarks are applied, as well as their strengths and limitations, and explores real-world use cases, providing a practical set of tools for understanding watermarks and time in out-of-order stream processing pipelines. Along the way, Slava also outlines some of the implementation challenges for computing watermarks with low latency in a highly distributed system.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>In a streaming data processing system, where data is generally unbounded, triggers specify when each stage of computation should emit output. With a small language of primitive conditions and multiple ways of combining them, triggers provide the flexibility to tailor a streaming pipeline to a variety of use cases and data sources, enabling a practitioner to achieve an appropriate balance between accuracy, latency, and cost. (Some conditions under which one may choose to \\u201cfire\\u201d\\u2014aka trigger output\\u2014include after the system believes all data for the current window is processed, after at least 1,000 elements have arrived for processing, when the first of trigger A and trigger B fires, or according to trigger A until trigger B fires.)</p>\\n<p>To support the variety of streaming systems in existence today and yet to come, as well as the variability built into each one, a foundational semantics for triggers must be based on fundamental aspects of stream processing. Since we also aim to maintain the unified batch/streaming programming model, trigger semantics must remain consistent across a number of dimensions, including reordering and/or delay of data, small bundles of data where an operation may buffer data until a trigger fires, large bundles of data where an operation processes it all before firing the result to the next stage, arbitrarily good (or bad) approximations of event time, and retrying a computation (for example, when processing time and event time may both have progressed, and more data may have arrived, and we\\u2019d like to process it all together in large bundles for performance).</p>\\n<p>Drawing on important real-world use cases, Kenneth Knowles delves into the details of language- and runner-independent semantics for triggers in Apache Beam and explores real-world implementations in Google Cloud Dataflow.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Time series and event data form the basis for real-time insights about the performance of businesses such as ecommerce, the IoT, and web services, but gaining these insights involves designing a learning system that scales to millions and billions of data streams. Ira Cohen outlines a system that performs real-time machine learning and analytics on streams at massive scale.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Modern cars produce data. Lots of data. And Formula 1 cars produce more than their fair share. Ted Dunning presents a demo of how data streaming can be applied to the analytics problems posed by modern motorsports. Although he won\\u2019t be bringing Formula 1 cars to the talk, Ted demonstrates a high-fidelity, physics-based automotive simulator to produce realistic data from simulated cars running on the Spa-Francorchamps track. As Ted moves data from the cars to the pits to the engineers back at HQ, the result is near real-time visualization and comparison of performance and a great exposition of how to move data using messaging systems like Kafka.</p>\\n<p>The code from this talk will be made available as open source.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Fifteen years ago, Webvan spectacularly failed to bring grocery delivery online. Speculation has been high that the current wave of on-demand grocery delivery startups will meet similar fates. Jeremy Stanley explains why this time the story will be different\\u2014data science is the key. Innovations in mobile applications have paved the way, but significant investments in algorithms to optimize efficiency will drive positive unit economics.</p>\\n<p>Jeremy explores how Instacart has used data science to optimize last-mile delivery and balance supply and demand to drive big gains in efficiency that are transforming unit economics in this competitive space. He describes how improvements in predicting outcomes, batching algorithms, forecasting, and staffing have contributed to big improvements in delivery efficiency and outlines some of the multiple competing objectives Instacart is optimizing for in order to provide a great customer and shopper experience. Jeremy ends by talking about how data science is organized at Instacart and how it collaborates with product, engineering, and field operators to make rapid innovation possible in a complex ecosystem.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>To anticipate who will succeed and invest wisely, investors spend a lot of time trying to understand the longer-term trends within an industry. In a panel discussion, top-tier VCs look over the horizon and consider the big trends in big data, explaining what they think the field will look like a few years (or more) down the road. Join us to hear about the trends that everyone is seeing and areas for investment that they find exciting.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Three-quarters of firms tell Forrester they aspire to be data driven, yet less than a third are good at connecting insights to actions that really matter. A new generation of digital competitors does not have this problem and stands poised to steal $1.2 trillion from traditional enterprises by 2020. Many of them are presenting at Strata, but what does this new breed of competitors have in common beyond using technology like Hadoop and Spark?</p>\\n<p>Forrester\\u2019s Brian Hopkins presents the results of two years of research into what is different about these insight-driven competitors and describes how they are able to sustain extremely high growth. The key is that they systematically harness data and connect it to action in closed-loop systems of insight. Brian breaks down what a system of insight is, demonstrates how to use one to become insight driven, and explains how you can selectively embrace emerging big data technology innovations to get there.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Leading companies that are getting the most out of their data are not focusing on queries and data lakes; they are actively integrating analytics into their operations with a data-first application development approach. Real-time adjustments to improve revenues, reduce costs, or mitigate risk rely on applications that minimize latency on a variety of data sources. Jack Norris reviews best practices for three use cases in ad/media, financial services, and healthcare to show how customers develop, deploy, and dynamically update these applications and how this data-first approach is fundamentally different from traditional applications.</p>\\n<p>Along the way, Jack covers examples of how customers identified ways to simplify data streams in a publish-and-subscribe framework (for example, how focusing on a stream of electronic medical records simplified the deployment of real-time applications for hospitals, clinics, and insurance companies). Jack also details how a data-first approach can lead to rapid deployment of additional real-time applications as well as centralize and simplify many data management and administration tasks.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>The best data-driven companies constantly utilize data at each function of the business. One well-known example, Uber, brought a data-driven approach to the taxi industry, using information about where customers are located, changing its price based on demand, and gathering customer feedback scores to improve customer satisfaction. Today more and more companies are accessing and operationalizing instant data in a similar way. Daniel Mintz dives into case studies from three companies\\u2014ThredUp, Twilio, and Warby Parker\\u2014that use data to generate sustainable competitive advantages in their industries. These companies have three characteristics in common.</p>\\n<p>First, they are changing the way they are managed, from the way they run meetings to the way they run their teams, make decisions, and collaborate. These companies use data to inform teams regarding decisions about whether customers will like a new feature, whether or not to run a marketing campaign, or even how to price an item. No longer are teams segmented by function or decisions made from the top down. Product, marketing, sales, and recruiting teams are all collaborating and acting based on the data at hand.</p>\\n<p>Second, companies are creating functional data supply chains so that data-starved employees can stop relying on data teams to push often fragmented information across disparate teams. Companies are now using centralized, scalable databases usable by any employee across the organization.</p>\\n<p>Finally, companies are creating a common data language\\u2014a universal set of metrics within the organization. Sales and marketing teams have the same definition of a lead, while recruiting teams record metrics such as offer acceptance rate. This consistency of data-literacy allows teams to work more efficiently together.</p>\\n<p>You\\u2019ll leave with an understanding of how to set up processes in your own businesses that help each function become better at using data to make informed decisions.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>At the start of the Crimean War in 1853, Britain\\u2019s Royal Navy needed 90 new gunboats ready to fight in the Baltic in just 90 days. Assembling the boats was straightforward. The challenge was to build all of the engine sets in time. Marine engineer John Penn did an unusual thing: he took a pair of reference engines, disassembled them, and distributed the pieces to the best machine shops across Britain. These workshops\\u2014latter-day microservices\\u2014each built 90 sets of their allocated parts, which were then assembled into the engines for the new gunboats, ready for battle.</p>\\n<p>This was the nineteenth century. How could the admiralty be certain that the parts from all these independent workshops would come together to form 90 high-powered engines? The answer lay in a crucial piece of standardization: the Whitworth thread, the world\\u2019s first national screw thread standard, devised by Sir Joseph Whitworth in 1841. By the time the Royal Navy came knocking, this standard had been adopted by workshops across Britain, so John Penn could be confident that engine parts built by any workshop to the Whitworth standard would fit together.</p>\\n<p>Snowplow\\u2019s Alexander Dean uses the story of the Crimean War gunboats to argue that our data-processing architectures urgently require a standardization of their own, in the form of schema registries. Like the Whitworth screw thread, a schema registry, such as Confluent Schema Registry or Snowplow\\u2019s own Iglu, allows enterprises to standardize on a set of business entities which can be used throughout their batch and stream processing architectures. Like the artisanal workshops in 1850s Britain, microservices can work on narrowly defined data processing tasks, confident that their inputs and outputs will be compatible with their peers.</p>\\n<p>Alexander outlines the rationale for putting a schema registry at the heart of your business, before moving on to the practicalities of an implementation, offering a side-by-side comparison of the available registries, best practices about schema versioning, and strategies around schema federation across different companies, including Snowplow\\u2019s own Iglu Central.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Enterprises that pursue data-driven operations and decisions are approaching the conclusion that graph analysis capabilities will yield critical competitive advantages. However, for this impact to be fully realized, the results of any graph analysis must be available, in real time, to operational applications, data scientists, and developers across the enterprise.</p>\\n<p>Monsanto previously attempted graph analysis using both <span class=\"caps\">RDBMS</span>-based and offline batch processing techniques. In the process, Monsanto found that some couldn\\u2019t drill sufficiently deeply to result in the necessary insights; others were limited in their expressibility and therefore general usefulness outside of the data science lab; and still others weren\\u2019t able to provide answers in a short enough amount of time to be useful to the business. Monsanto finally selected a graph database used alongside a broader tech stack that includes Apache Kafka, Spark, and Oracle. This stack allows Monsanto to not just derive but also operationalize insights that have allowed it to shorten R&amp;D cycles, better understand the dynamics of its business, and carry out certain of types of science in silico.</p>\\n<p>Tim Williamson and Emil Eifrem draw on Monsanto\\u2019s real-world experience to explain how organizations can use graph databases to operationalize insights from big data. Tim and Emil discuss Monsanto\\u2019s big data stack, using examples from Monsanto\\u2019s substantial experience with graphs, and describe the service-oriented graph architecture that has already handled over one billion requests and is available to over 150 developers, data scientists, and applications throughout Monsanto.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>While all other industries have embraced the digital era, healthcare seems to be still playing catch-up. In this, Kaiser Permanente is an anomaly. Kaiser Permanente is a leader in healthcare technology\\u2014technology has been at center stage in Kaiser Permanente since it first started using computing to improve healthcare results in the 1960s. Today, Kaiser Permanente is an integrated health care delivery system with 10 million members and about 200,000 employees.</p>\\n<p>Kaiser rolled out the first electronic medical records (<span class=\"caps\">EMR</span>) system in the inpatient setting in 2005, and it was fully deployed in all regions and medical centers by 2010. Electronic medical records at Kaiser Permanente include not only the health plan view of the data but also pharmacy, insurance claims, hospital, and provider views. In addition, Kaiser Permanente manages many transactions in a member-centric online web portal available at kp.org.</p>\\n<p>To leverage the immense amount of <span class=\"caps\">EMR</span> data, domain-specific data warehouses have been developed over the last decade. However, with the increase in volume, velocity, variety of data (clinical, genetic, behavioral, social, environmental, and device), a new way of storing, cataloguing, searching and provisioning was recently developed. Running analytical workloads on this platform works wonders. However, the challenge is to make this 200,000-employee company data driven, break the silos, and develop a collaborative culture.</p>\\n<p>Taposh Roy, Rajiv Synghal, and Sabrina Dahlgren offer an overview of Kaiser\\u2019s big data strategy and explain how other organizations can adopt similar strategies.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Billions of Visa cards are used around the world to make payments. Each payment transaction has a story. Getting payments from point A to point B is complex, and the resulting data Visa captures reflects this. The scale and complexity of that data is a direct manifestation of the number, variety, and complexity of payment transactions processed by the Visa network.</p>\\n<p>As enterprises go, Visa is among the more cautious. Visa systems have stringent availability requirements to ensure payments never fail, designed around proven ideas and technologies. Visa is almost never an early adopter of technologies. Instead, it waits for technologies to harden and prove themselves.</p>\\n<p>Nandu Jayakumar explores the adoption of big data practices at Visa and explains how Visa is transforming the way it manages data: database appliances are giving way to Hadoop and HBase; proprietary <span class=\"caps\">ETL</span> technologies are being replaced by Spark; and enterprise warehouse data models will be complemented by flexible data schemas. Due to the nature of its business, regulatory compliance and secure management of data are central tenets at Visa. Many open source and less mature technologies tend to fall short in these aspects. Embracing big data technologies has also required a culture change in the engineering teams and the ultimate users of data in the company. Nandu also discusses Visa\\u2019s experience with Impala supporting a large reporting setup.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Choosing commodity hardware and scale-out architectures as a way to achieve cost-efficient scaling</li>\\n<li>Trading off agility of data publishing and usage against security and compliance requirements</li>\\n<li>Moving beyond traditional <span class=\"caps\">EDW</span> and BI to data science and near-real time data applications at scale</li>\\n<li>Introduction of agile data that is defined less strictly as an important technique for handling rapidly evolving enterprise data needs</li>\\n<li>Visa\\u2019s decision to build a hosted, multitenant data lake across the company to reduce the time and effort it takes to build data applications</li>\\n<li>The decision matrix that was used to pick particular technologies, including Impala, Spark, HBase, Chef, and Kafka</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Last year, LinkedIn embarked on an ambitious mission to completely revamp the mobile experience for its members. This would mean a completely new mobile application, reimagined user experiences, and new interaction concepts. As the team evaluated the impact of this big rewrite on the data analytics ecosystem, they observed a few problems.</p>\\n<p>Over the past few years, LinkedIn has become extremely good at incrementally changing the site one mini-feature at a time, often in conjunction with hundreds of other incremental changes. LinkedIn\\u2019s experimentation platform ensures that it is always monitoring a wide gamut of impacted metrics with every change before rolling fully forward. However, when it comes to rolling out a big change like this, different challenges crop up. You have to rollout the entire application all at once; the new experience means that you have no baseline on new metrics; and existing metrics may see double digit changes just because of the new experience or because the metric\\u2019s logic is no longer accurate\\u2014the challenge is in figuring out which is which.</p>\\n<p>Shirshanka Das and Yael Garten describe how LinkedIn redesigned its data analytics ecosystem in the face of a significant product rewrite, covering the infrastructure changes that enable LinkedIn to roll out future product innovations with minimal downstream impact. Shirshanka and Yael explore the motivations and the building blocks for this reimagined data analytics ecosystem, the technical details of LinkedIn\\u2019s new client-side tracking infrastructure, its unified reporting platform, and its data virtualization layer on top of Hadoop and share lessons learned from data producers and consumers that are participating in this governance model. Along the way, they offer some anecdotal evidence during the rollout that validated some of their decisions and are also shaping the future roadmap of these efforts.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>The world is producing an ever-increasing volume, velocity, and variety of big data. Consumers and businesses are demanding up-to-the-second (or even millisecond) analytics on their fast-moving data, in addition to classic batch processing. The Hadoop ecosystem and <span class=\"caps\">AWS</span> provide a plethora of tools for solving big data problems. But what tools should you use, why, and how?</p>\\n<p>Siva Raghupathy demonstrates how to use Hadoop innovations in conjunction with Amazon Web Services innovations, showing how to simplify big data processing as a data bus comprising various stages: collect, store, process/analyze, and consume. Siva then discusses how to choose the right technology in each stage based on criteria such as data structure, query latency, cost, request rate, item size, data volume, durability, and so on before providing reference architecture, design patterns, and best practices for assembling these technologies to solve your big data problems at the right cost.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Today, Hadoop is deployed on-premises and in the public cloud, with public cloud increasingly becoming more prevalent. The cloud provides some unique abilities, including on-demand infrastructure, cluster elasticity, persisted globally available object storage, and pay-for-use pricing, which enables even more flexible and cost-efficient deployment options for BI and <span class=\"caps\">SQL</span> analytic users of Impala but brings in new challenges that need to be carefully considered to achieve optimal outcome.</p>\\n<p>Henry Robinson and Justin Erickson explain how to best take advantage of the flexibility and cost-effectiveness of the cloud with your BI and <span class=\"caps\">SQL</span> analytic workloads using Apache Hadoop and Apache Impala (incubating) to provide the same great functionality, partner ecosystem, and flexibility of on-premises. Henry and Justin cover the architectural considerations, best practices, tuning, and functionality available when deploying or migrating BI and <span class=\"caps\">SQL</span> analytic workloads to the cloud.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>On-premises versus the cloud: What\\u2019s the same and what\\u2019s different</li>\\n<li>What kind of instance types to consider for best performance</li>\\n<li>How Impala can read/write on cloud-based object storage (S3)</li>\\n<li>How to understand workloads in terms transient, hybrid, and permanent cloud clusters and what workloads are cost effective to run on the cloud versus on-premises</li>\\n<li>Tuning and best practices for storage and instance choices that will help you effectively architect Impala clusters</li>\\n<li>BI and <span class=\"caps\">SQL</span> analytics with Hadoop in the cloud</li>\\n<li>How Impala performs in the cloud compared to alternatives</li>\\n<li>The roadmap for what\\u2019s next</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>From data exploration to prototyping to final delivery, anyone engaged in using data science for social good knows that the path from project kickoff to delivery is full of exciting twists and turns. JeanCarlo Bonilla, Susan Sun, and Caitlin Augustin explore how DataKind volunteer teams navigate the road to social impact by automating evidence collection for conservationists and helping expand the reach of mobile surveys so that more voices can be heard.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Every industry has both proven and potential data lake use cases. With enterprise data warehouses (EDWs) being rendered ever more inefficient when facing new business needs, cloud-based data lakes have been gaining popularity with enterprises looking to cover the technology gap. Cloud data lakes are purpose-built to meet the data management requirements of the evolving enterprise landscape.</p>\\n<p>Alex Bordei walks you through the steps required to build a data lake in the cloud and connect it to on-premises environments, covering best practices in architecting cloud data lakes and key aspects such as performance, security, data lineage, and data maintenance. The technologies presented range from basic <span class=\"caps\">HDFS</span> storage to real-time processing with Spark Streaming.</p>\\n<p>Topics include:<br/>\\n<strong>Why enterprises should build data lakes in the cloud</strong><br/>\\nThe main drivers for enterprise adoption of the data lake have been the need for agility and custom, enterprise-wide access to datasets, data streams, and data analysis tools. However, more and more companies have started using cloud data lakes as prototyping workbenches and have embraced the researcher mindset in order to build fully functional data laboratories in the cloud. Apart from offering an extremely convenient method of bypassing the tedious integration and configuration of big data applications and the costly acquisition and tuning of the underlying on-premises infrastructure, using a cloud data lake offers the opportunity to experiment with an ever growing array of big data technologies.</p>\\n<p><strong>Solutions for securely extending the on-premises network in the cloud</strong><br/>\\nTo protect against unauthorized access, the data lake uses computer network authentication protocols, such as Kerberos, and it encrypts data both when transmitted across networks and while at rest. Security measures suited to cloud data lakes must also cover efficient backup protocols. Ideally, replication is configured on a per-file basis so users can decide the extent to which the most sensitive data is safeguarded against loss.</p>\\n<p><strong>Integration solutions for multiple Active Directory domains and multiple secure Hadoop environments</strong><br/>\\nThe data lake should easily integrate any corporate Active Directory (<span class=\"caps\">LDAP</span>) or third-party authentication method. Identity services integration is crucial when building a data lake in the cloud. As data provisioning, management, and governance become easier and safer, cloud-based Hadoop architectures better mirror and seamlessly integrate with on-premises architectures.</p>\\n<p><strong>Solutions for increasing performance</strong><br/>\\nDespite its impact on the IT landscape and its enthusiastic adoption across industry sectors, the virtualized cloud is far from being the best underlying architecture solution for data lakes\\u2014and for big data projects in general. A fairly new breed of cloud, the bare-metal cloud, offers a much better environment in terms of performance, isolation, and flexibility. Platforms offering such environments provide the high computation power and security of bare metal with the full flexibility of the cloud.</p>\\n<p><strong>Software solutions typically used for data lakes</strong><br/>\\nFrom concept to deployment, creating a production-ready enterprise cloud data lake should take minutes, not months. Every hardware connection should be software-defined, and every software component should be ready for deployment, scaling, and connecting to a data source. Along with the data lake\\u2019s powerful processing capabilities, its software stack is the main aspect that differentiates a cloud data lake from large-scale storage repositories such as an enterprise data warehouse.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>As the momentum with which big data drives decisions continues to grow, the relevant legal considerations relating to collection and use of such data will also increase and evolve. Understanding the legal frameworks applicable to collection and use of data for certain purposes is key to making compliance-based business decisions and maintaining long-term consumer trust in a brand.</p>\\n<p>Attorneys Kristi Wolff and Crystal Skelton address the legal considerations for using big data as it relates to specific industries and is governed by specific laws, such as the Federal Trade Commission Act, the Health Insurance Portability and Accountability Act, the Gramm-Leach-Bliley Act, the Fair Credit Reporting Act, the Children\\u2019s Online Privacy Protection Act, the Telephone Consumer Protection Act, and the <span class=\"caps\">CAN</span>-<span class=\"caps\">SPAM</span> Act.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>The Federal Trade Commission\\u2019s concerns relating to use of big data for employment, insurance, or financial purposes</li>\\n<li>Sensitivities concerning the collection and use of health information, children\\u2019s information, and geolocation data</li>\\n<li>How to avoid legal snafus when using targeted advertising, including making telephone calls and sending emails</li>\\n<li>Federal and state obligations relating to the collection of big data and requirements to secure such data</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>With rapid advances in the field of data science and the availability of real-time streaming data, the specter of a data-driven dystopia looms larger than ever. Mainstream media, civil rights advocates, and watchdog groups of all political persuasions are increasingly questioning the legitimacy of proprietary predictive tools that are widely used in areas from law enforcement to healthcare.</p>\\n<p>While some of these concerns are overly sensationalized, many are grounded in legitimate worry about embedded bias and lack of transparency. The market for black box tools is dying fast. The public is demanding greater insight into how and why decisions are made, creating an environment where the use of such software is rapidly becoming politically untenable, particularly in law enforcement. At the same time, competitive markets for predictive tools should encourage transparency into the performance of the underlying analytics in order to ensure optimal performance. So, how can we usher in a future of data-driven decision making that is characterized by more\\u2014not less\\u2014accountability and accessibility?</p>\\n<p>Brett Goldstein discusses the imperative to couple new developments in data science with a renewed commitment to transparency and open source. In particular, Brett focuses on the example of CrimeScape, an open source deployment tool that optimizes policing resources transparently in real time. In contrast to existing crime analytics software, CrimeScape\\u2019s data, model, predictions, and accuracy are published online.</p>\\n<p>Brett explores the benefits and challenges of bringing a sustainable open source approach to predictive analytics, covering issues such as the security of sensitive data, how to develop transparent model performance metrics and evaluation, and how to balance privacy with transparency within an open source framework. Brett concludes with a discussion of how these lessons can be applied more broadly in other contexts beyond crime prediction.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>How are users meant to interpret the influence of big data and personalization in their targeted experiences? What signals show how your data is used and how it improves or constrains your experience? To what degree is this experience based on coarse demographics or the entire data profile of your browsing history? Sara Watson explains that in order to develop normative opinions to shape policy and practice, users need means to guide their experience\\u2014the personalization spectrum.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Examples of user experiences that call into question data uses</li>\\n<li>Examples of early user-facing explanations</li>\\n<li>Overview of the personalization spectrum</li>\\n<li>New opportunities for user intervention through the spectrum</li>\\n<li>Implications for design, user experience, and data use policies</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Enterprises are increasingly demanding real-time analytics and insights. Tony Ng offers an overview of Pulsar, an open source real-time streaming system used at eBay, which can scale to millions of events per second with 4GL <span class=\"caps\">SQL</span>-like language support. Pulsar provides real-time sessionization, multidimensional metrics aggregation over time windows, and custom stream creation through data enrichment, filtering, and stateful processing. Tony explains how Pulsar integrates Kafka, Kylin, and Druid to provide flexibility and scalability in event and metrics consumption.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Real-time analytics and its applications, such as personalization, monitoring, and marketing</li>\\n<li>Pulsar\\u2019s real-time analytics pipeline</li>\\n<li>Pulsar\\u2019s architecture to support high scalability and availability</li>\\n<li>Pulsar\\u2019s event-processing framework and language</li>\\n<li>Integration of Pulsar with Kafka to support replay of unprocessed or undelivered events to avoid data loss</li>\\n<li>Integration of Pulsar with Kylin to provide multidimensional slice and dice of data</li>\\n<li>Integration of Pulsar with Druid to provide real-time metrics and dashboards</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>The landscape for storing your big data is quite complex, with several competing formats and different implementations of each format. Picking the best data format depends on what kind of data you have and how you plan to use it. Depending on your use case, different formats perform very differently. Although you can use a hammer to drive a screw, it isn\\u2019t fast or easy to do so. Owen O\\u2019Malley outlines the performance differences between formats in different use cases and offers an overview of the advantages and disadvantages of each to help you improve the performance of your applications.</p>\\n<p>Use cases include:</p>\\n<ul>\\n<li>Reading all of the columns</li>\\n<li>Reading a few of the columns</li>\\n<li>Filtering using a filter predicate</li>\\n<li>Writing the data</li>\\n</ul>\\n<p>All of the benchmark code will be open source so that the experiments can be replicated. Furthermore, it is important to benchmark on real data rather than synthetic data. You\\u2019ll use the GitHub logs data available freely from<br/>\\n<a href=\"http://githubarchive.org\">the GitHub Archive</a>.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>When it comes to <span class=\"caps\">SQL</span>-on-Hadoop, it is easy to feel overwhelmed with the number of choices available in tools, file formats, schema design, and configurations. However, in reality, making good design choices when you start will help you avoid some of the common pitfalls. Marcel Kornacker and Mostafa Mokhtar simplify the process and cover top performance optimizations for Apache Impala (incubating), from schema design and memory optimization to query tuning.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li><span class=\"caps\">SQL</span>-on-Hadoop: Picking your tool based on the workload and understanding where Hive, Impala, and Spark <span class=\"caps\">SQL</span> are best used</li>\\n<li>Requirements and considerations for BI and <span class=\"caps\">SQL</span> analytic workloads</li>\\n<li>Schema design</li>\\n<li>Memory usage, cluster size, and hardware recommendations</li>\\n<li>Multitenancy best practices</li>\\n<li>Query tuning basics for Impala</li>\\n<li>Impala performance and benchmarking</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Most data centers (and many cloud deployments) are statically partitioned into siloed clusters dedicated to running individual datacenter-scale applications, including web services, databases, and batch/stream processing. This static partitioning model limits overall cluster utilization, decreases flexibility, and poses operational challenges. There is an increasing need to integrate big data applications like Apache Hadoop and Apache Spark with other data center services like Apache Cassandra or Apache Kafka, ideally colocating the data with the services that need it.</p>\\n<p>Adam Bordelon and Mohit Soni demonstrate how projects like Apache Myriad (incubating) can install Hadoop on Mesosphere DC/OS alongside other data center-scale applications, enabling efficient resource sharing and isolation across a variety of distributed applications while sharing the same cluster resources and hence breaking silos. The multitenancy strategy improves overall cluster utilization and operational efficiency while allowing cluster operators to run multiple isolated data services on the same hardware.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Ever since its creation, <span class=\"caps\">HDFS</span> has relied on data replication to shield against most failure scenarios. However, with the explosive growth in data volume, replication is getting quite expensive: the default 3x replication scheme incurs a 200% overhead in storage space and other resources (e.g., network bandwidth when writing the data). Erasure coding (EC) uses far less storage space while still providing the same level of fault tolerance. Under typical configurations, EC reduces the storage cost by ~50% compared with 3x replication.</p>\\n<p>Zhe Zhang and Uma Maheswara Rao G present the first-ever performance study of <span class=\"caps\">HDFS</span>-EC, comprised of 160 performance tests covering four different hardware configurations, three different erasure coders (including a native coder based on Intel\\u2019s <span class=\"caps\">ISA</span>-L library), four I/O concurrency levels, and over 10 different benchmarking workloads (including <span class=\"caps\">TPC</span>-H running with Hive-on-Spark). The test results verify that the superior performance of the <span class=\"caps\">ISA</span>-L coder translates to up to 50x higher I/O throughput. Among other interesting insights, Zhe, Uma, and Rui have observed significant performance gain of EC over the replication mode under sequential I/O workloads. Moreover, the tests have also revealed a number of previously hidden design issues to be optimized in the future.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Several big data graph processing frameworks that have been built to run on large graph datasets have been proposed and are in use at large corporations for applications ranging from social network analysis to machine learning to the PageRank algorithm. However, these libraries can also be put to work to study the nature of cancer.</p>\\n<p>Cancer is a complex disease characterized by defective signaling pathways in the cell. New techniques in cancer genomics research discover the pathways implicated in cancer development by studying large protein-protein interaction networks representing how proteins interact with each other within the cell. Using somatic mutation data from a set of cancer patients and probabilistic graph algorithms, researchers can perform de novo identification of mutated pathways or subnetworks, which can then be used to develop therapies.</p>\\n<p>Crystal Valentine explains how the large graph-processing frameworks that run on Hadoop can be used to detect significantly mutated protein signaling pathways in cancer genomes using techniques similar to those used in social network analysis algorithms and describes an algorithm for de novo discovery of highly-mutated signaling pathways in cancer patients using a big data graph-processing library.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>We\\u2019ve seen significant progress in infrastructure for using data effectively in the last half-decade. But this hasn\\u2019t applied to all types of data equally. Unstructured text, in particular, has been slower to yield to the kinds of analysis that many businesses are starting to take for granted. Rather than being limited by what we can collect, we are now constrained by the tools, time, and techniques to make good use of it. But we are beginning to gain the ability to do remarkable things with unstructured text data.</p>\\n<p>Michael Williams explores text summarization\\u2014taking text in and returning a shorter document that contains the same information\\u2014covering both single document and multidocument summarization. Michael demonstrates ways to solve the summarization problem that range from extremely simple algorithms that date back to the 1950s to the latest recurrent neural networks, explains how to choose between these approaches, and shows working prototype products for each.</p>\\n<p>Summarizing tens or hundreds of thousands of articles at once represents an entirely new capability. But this capability is a solution to a bigger problem: it\\u2019s a gateway to quantified representations of text. The breakthrough capabilities realized by the application of sentence embedding and recurrent neural networks to the semantic meaning of text are poised to transform all the ways in which computers process language.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>As healthcare data becomes more digitized, the opportunity to leverage electronic medical records, prescription data, medical billings, hospital, and other healthcare datasets to help improve health outcomes and lower the cost of care for patients in near real time is becoming a possibility. However, processing terabytes and petabytes of de-identified healthcare data requires the application of complex and ever-changing business rules. This impacts the ability to generate near-real-time insights and conduct research studies that could potentially influence how patients are treated.</p>\\n<p>Today, the analysis of databases of this magnitude can take days or even weeks of processing; to be more effective for improving patient care, researchers need to be able to run processes on demand, returning result sets instantaneously. Navdeep Alam shares his experience at <span class=\"caps\">IMS</span> Health in realizing this opportunity to influence patient health outcomes in minutes to seconds and reviews current and emerging technologies in the marketplace that handle working with unbounded, de-identified patient datasets in the billions of rows in an efficient and scalable way.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Handling heterogeneous and concurrent query workloads in a multiuser environment is a common use scenario for BI analytics over <span class=\"caps\">SQL</span>-on-Hadoop systems. Properly deploying a <span class=\"caps\">SQL</span>-on-Hadoop cluster that provides the best performance in such an environment requires extensive knowledge of the workloads, overall resource utilization, database table design, software stack configurations, and hardware settings. An improperly planned deployment can lead to an underutilized cluster, wasting company assets or failing to meet performance requirements. In one real-world example, a company deployed an 80-node cluster; however, their workloads and data volume required less than half of the nodes to meet their performance requirement. This means more than half of the nodes sat doing nothing but waiting to be depreciated. In another, a company used more expensive SSDs even though, at the software level, operations were single threaded and bottlenecked by <span class=\"caps\">CPU</span> rather than I/O\\u2014thus, HDDs might have been a better choice for deployment. Of course, in some scenarios, SSDs might improve the overall query execution time by more than 50%, so there is really no one-size-fits-all solution.</p>\\n<p>As these examples suggest, many challenges exist in designing an <span class=\"caps\">SQL</span>-on-Hadoop cluster for production in a multiuser environment with heterogeneous and concurrent query workloads. Jun Liu and Zhaojuan Bian draw on their personal experience to address these challenges, explaining how to determine the right size of your cluster with different combinations of hardware and software resources using a simulation-based approach.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>An introduction to Impala and its resource management mechanism</li>\\n<li>Deployment challenges of an Impala cluster: Selecting the best combination of software configurations and choosing hardware settings and sizing</li>\\n<li>A case study: Planning your Impala system in a multiuser environment</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Running real-time data-intensive applications on Apache Hadoop requires complex architectures to store and query data, typically involving multiple independent systems that are tied together through custom-engineered pipelines. A common pattern is to use a NoSQL engine like Apache HBase for caching and later transformations, the results of which are periodically written to <span class=\"caps\">HDFS</span> in one of the popular open columnar file formats as a prerequisite for querying by a <span class=\"caps\">SQL</span> engine.</p>\\n<p>Apache Kudu (incubating), a new scalable distributed storage engine designed for the Hadoop environment, gives the user low-latency single-row access as well as high-throughput bulk data scans. Integrated with Apache Impala (incubating), these capabilities are made available to the user via standard <span class=\"caps\">SQL</span> language elements for updates and querying, combining the flexible update functionality of an <span class=\"caps\">RDBMS</span> with the performance of a parallel analytic database system.</p>\\n<p>Todd Lipcon and Marcel Kornacker explain how to simplify Hadoop-based data-centric applications with the <span class=\"caps\">CRUD</span> (create, read, update, and delete) and interactive analytic functionality of Apache Impala (incubating) and Apache Kudu (incubating), offering an introduction to using Impala + Kudu to power your real-time data-centric applications for use cases like time series analysis (fraud detection, stream market data), machine data analytics, and online reporting.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>One of the ways to drive enterprise adoption of big data in financial services is to have a central standardized, reusable, transparent, and well-governed library of features (or metrics) that will empower data scientists and business analysts across a range of business problems. This is the central idea behind a feature store\\u2014a library of documented features for various analyses based on a shared data model that spans a wide variety of data sources resident within a bank\\u2019s data lake.</p>\\n<p>Kaushik Deka and Phil Jarymiszyn discuss the benefits of a Spark-based feature store, outline three challenges they faced\\u2014semantic data integration within a data lake, high-performance feature engineering, and metadata governance\\u2014and explain how they overcame them.</p>\\n<p>The first challenge of building such a feature store is to project the data in a data lake into a common conceptual data model and then generate features from that model. The combination of data variety, formal analytical models, and long project cycles in financial services suggests that the application of data modeling to data lakes should yield significant advantages both in terms of a shared understanding of the domain-specific semantic ontology and also as an extensible data integration framework. In the discussed use case, the feature store was powered by one such semantically integrated data model for retail banking.</p>\\n<p>The second challenge is to enable high-performance feature engineering at a customer level on top of the conceptual data model. There\\u2019s significant benefit to partitioning data at the customer level so that calculations don\\u2019t incur cross-node chatter on the network. Kaushik and Phil had to provide an <span class=\"caps\">API</span> to access the data model for data scientists to create parameterized features. To accomplish these objectives, they developed an <span class=\"caps\">ETL</span> pipeline in Spark that stored the instance data in Hadoop as a distributed collection of partitioned structured objects per customer. They then provided a parallelizable Spark <span class=\"caps\">API</span> to access these structured customer objects.</p>\\n<p>The third challenge is enforcing business metadata governance on the feature store. The agility of analytics and data democratization that a high-performing feature store can unleash has to be countered with sound metadata governance to prevent complete analytical anarchy. Regulatory pressures make this a necessity. In particular, data lineage, audits, and version control of source code have to be baked into the feature development workflows within the feature store.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Zillow pioneered providing access to unprecedented information about the housing market. Long gone are the days when you needed an agent to get comparables and prior sale and listing data. Enter Zillow, the nation\\u2019s number-one real estate website and mobile app. With more data, data science has enabled more use cases. Jasjeet Thind explores Zillow\\u2019s big data platform, discusses some of its core machine-learning algorithms, and outlines best practices for scaling streaming data ingestion and data processing in Spark.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>How Zillow predicts the owners of 100+ million homes and distinguishes between a buyer, seller, homeowner, and renter</li>\\n<li>How Zillow makes the Zestimate more accurate via text mining</li>\\n<li>How Zillow implemented its own collaborative filtering algorithm to provide personalized real estate recommendations</li>\\n<li>The best time to sell your home</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Bas Geerdink offers an overview of the evolution that the Hadoop ecosystem has taken at <span class=\"caps\">ING</span>. Since 2013, <span class=\"caps\">ING</span> has invested heavily in a central data lake and data management practice. Bas shares historical lessons and best practices for enterprises that are incorporating Hadoop into their infrastructure landscape.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Ram Sriharsha reviews major developments in Apache Spark 2.0 and discusses future directions for the project to make Spark faster and easier to use for a wider array of workloads, with an emphasis on <span class=\"caps\">API</span> evolution, single-node performance (Project Tungsten Phase 3), and Structured Streaming.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>In the world of distributed computing, Spark has simplified development and opened the doors for many to start writing distributed programs. Folks with little to no distributed coding experience can now write just a couple lines of code that will immediately get hundreds or thousands of machines working on creating business value.</p>\\n<p>Even though Spark code is easy to write and read, that doesn\\u2019t mean that users don\\u2019t run into issues of long-running, slow-performing jobs or out-of-memory errors. Thankfully most of the issues with using Spark have nothing to do with Spark but rather the approach taken when using it. Ted Malaska and Mark Grover cover the top five things that prevent Spark developers from getting the most out of their Spark clusters. When these issues are addressed, it is not uncommon to see the same job running 10x or 100x faster with the same clusters and the same data, using just a different approach.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Spark\\u2019s efficiency and speed can help big data administrators reduce the total cost of ownership (<span class=\"caps\">TCO</span>) of their existing clusters. This is because Spark\\u2019s performance advantages allow it to complete processing in drastically shorter batch windows with higher performance per dollar. Raj Krishnamurthy offers a detailed walk-through of an alternating least squares-based matrix factorization workload. Using this methodology, Raj has been able to improve runtimes by a factor of 2.22.</p>\\n<p>Since Spark has a large number of tunables, a bottom-up approach to finding the optimal runtime by varying Spark workers and Spark worker cores can create an explosion of tuning runs for a given workload because of the multiplicative nature of possible configurations. The discussed methodology uses a hybrid top-bottom approach that searches the configuration space carefully and reduces the combinatorial explosion of possible tuning runs. This methodology has even been successfully applied to complex Spark workflows consisting of Spark <span class=\"caps\">SQL</span> and ML Pipelines (and achieved substantial performance improvements) and a variety of other cluster architectures.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Praveen Murugesan explains how Uber leverages Hadoop and Spark as the cornerstones of its data infrastructure. Praveen details the current data architecture at Uber and outlines some of the unique challenges with data processing Uber faced as well as its approach to solving some key issues in order to continue to power Uber\\u2019s real-time marketplace.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Swisscom, the leading mobile service provider in Switzerland, also provides data-driven intelligence through the analysis of the data created by its mobile network. Its Mobility Insights team works to help civil administrators, tourism and marketing professionals, and many others understand the flow of people through their locations of interest. Fran\\xe7ois Garillot outlines the platform, tooling, and choices that help achieve this service and some challenges the team has faced, before exploring in depth the task of understanding the speeds of populations through a path of interest.</p>\\n<p>Fran\\xe7ois offers an overview of the design of Swisscom\\u2019s big data infrastructure, which features Scala, Kafka, and Spark as pivotal tools, focusing on the multiple components that allow unified and reliable access to the high-throughput data flowing from the telecommunication network and lead to a single platform that lets engineers answer heterogeneous questions quickly. Along the way, Fran\\xe7ois explains the technical challenges of moving into real-time analysis and fast data, a key feature of the speed measurement task.</p>\\n<p>Fran\\xe7ois also proposes possible solutions to technical challenges, such as selecting interesting datapoints out of a millions coming in every second, sessionizing when no batch interval seems to make clear sense, and the importance of checking ground truth, and explains how privacy protection\\u2014crucial to Swisscom and the Mobility Insights team\\u2014is constitutive of both the data filtering and the questions the team chooses to tackle.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Apache Spark has been growing in deployments for the past two years. The increasing amount of data being analyzed and processed through the framework is massive, and it continues to push the boundaries of the engine. Drawing on his experiences across 150+ production deployments, Neelesh Srinivas Salian focuses on five common issues observed in a cluster environment setup with Apache Spark (Core, Streaming, and <span class=\"caps\">SQL</span>) to help you improve the usability and supportability of Apache Spark and avoid such issues in future deployments.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Scaling the architecture</li>\\n<li>Memory configurations</li>\\n<li>End-user code</li>\\n<li>Incompatible dependencies</li>\\n<li>Administration- and operation-related issues</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Despite widespread adoption, machine-learning models remain mostly black boxes, making it very difficult to understand the reasons behind a prediction. Such understanding is fundamentally important to assess trust in a model before we take actions based on a prediction or choose to deploy a new ML service. Such understanding further provides insights into the model, which can be used to turn an untrustworthy model or prediction into a trustworthy one.</p>\\n<p>Carlos Guestrin offers an overview of <span class=\"caps\">LIME</span>, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner by learning an interpretable model locally around the prediction, as well as a method to explain models by presenting representative individual predictions and their explanations in a nonredundant way.</p>\\n<p>Carlos demonstrates the flexibility of these methods by explaining different models for text (e.g., random forests) and image classification (e.g., deep neural networks) and explores the usefulness of explanations via novel experiments, both simulated and with human subjects. These explanations empower users in various scenarios that require trust, such as deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and detecting why a classifier should not be trusted.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>eHarmony has been using machine learning for about eight years. During this time, eHarmony has learned a number of lessons about how to implement machine learning at scale that allow it to rapidly address problems accurately. Recently more business units have needed data-driven models. Jonathan Morra introduces Aloha, an open source project that allows the modeling group to quickly deploy type-safe accurate models to production, and explores how eHarmony creates models with Apache Spark and how it uses them.</p>\\n<p>Jonathan first explains why it\\u2019s so important for data scientists and engineers to work together, outlining specific real-world problems that can arise when they don\\u2019t work. Jonathan then builds the case for a unified modeling framework with feature extraction built into the model representation and introduces eHarmony\\u2019s open source modeling framework, Aloha, demonstrating how Aloha lets eHarmony define a common interface between engineering and data science that allows rapid and, more importantly, separate paces on both sides.</p>\\n<p>Jonathan also explores how eHarmony makes use of Apache Spark to rapidly train, validate, test, and deploy models automatically and offers an aside into spotz, the hyperparameter optimization tool eHarmony has created and open sourced, giving the audience a taste of how eHarmony uses engineering on the modeling side to train models using a large amount of data.</p>\\n<p>Finally, Jonathan takes a deep dive into eHarmony\\u2019s matching algorithm and discusses recent advancements in predicting user behavior. Jonathan then goes over how eHarmony uses contextual bandits to help users get the best matching experience everyday and touches on a very recently observed phenomena where eHarmony is able to get a significant lift in matching by training on an intermediate signal. Jonathan will also discuss some open research questions at eHarmony that the team is currently working to address.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Pinterest is a rapidly expanding product that acts as a catalogue of ideas for over 100 million people. Pinterest\\u2019s content contains over 1 billion boards curated from over 50 billion pins. One of the roles data scientists fill at Pinterest is to understand this rapidly changing user base and content corpus. A handy tool for understanding large datasets is to reduce them to smaller datasets via clustering. For this application the workflow of a data scientist is to:</p>\\n<ol>\\n<li>Map each user or pin to a vector of features</li>\\n<li>Run a clustering algorithm over the feature vectors</li>\\n<li>Check the quality of the clusters</li>\\n<li>Iterate over stages 2 and 3, tuning the clustering algorithm\\u2019s parameters until a reasonable result is obtained</li>\\n<li>Label the resulting clusters with human-interpretable descriptions</li>\\n<li>Communicate cluster descriptions to guide product decisions and inform executive strategy</li>\\n</ol>\\n<p>The process is laborious with an inexplicable jump from a machine-learning definition of a cluster to a human-interpretable description. This jump creates a loss in resolution and is open to misinterpretations.</p>\\n<p>June Andrews offers an overview of a new method used at Pinterest and LinkedIn to close the gap between machine and human-interpretable clustering, which has lead to fast, accurate, and pertinent human-readable insights. The method can be both described as a human-in-the-loop iterative supervised clustering and a dance between machine learning and exploratory data analysis. The result is a representative, pertinent, and human interpretable analysis. Along the way, June also explores best practices and Pinterest case studies. By breaking down the barrier between machine-learning output and exploratory analysis, we can create interpretable and accurate results.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Predicting which stories will become popular is an invaluable tool for newsrooms, but people access their news using a variety of different platforms and sites, so identifying what stories are likely to be popular is a challenge. Very few studies have addressed the popularity of news articles specifically, although others have looked at predicting the popularity of other types of online content like tweets and videos. The reasons why a particular story becomes popular are varied and might involve contemporariness, writing quality, and other latent factors. Eui-Hong Han and Shuguang Wang explain how the <em>Washington Post</em> predicts what stories on its site will be popular with readers and share the challenges they faced in developing the tool and metrics on how they refined the tool to increase accuracy.</p>\\n<p>Eui-Hong and Shuguang chose to approach the problem of popularity prediction as a regression task and engineered several classes of features to build models for forecasting popularity. They explain the challenges of building accurate regression models, selecting the right set of features, and deploying the system in the production environment and describe their prediction system, which extracts features from the real-time click stream data and social media data using Kafka and Spark Streaming. Eui-Hong and Shuguang also share the challenges they faced for the production deployment and offer solutions to address those challenges.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Recurrent neural networks (<span class=\"caps\">RNN</span>) and related models represent the state of the art in language modeling. Trained on sufficiently large corpora, RNNs can learn to generate convincing text that obeys rules of syntax and even matches parentheses. What is especially remarkable is that these models can synthesize text from diverse inputs: text in another language (for translation), images and video (for captioning), and scores and categories for creation of personalized product reviews.</p>\\n<p>Josh Patterson and David Kale offer a thorough introduction to context-dependent text generation using conditional RNNs. Traditional <span class=\"caps\">RNN</span>-based language models are utilized in an unsupervised fashion, taking as their inputs only previous tokens in the sequence. In contrast, conditional RNNs (also called context-dependent or concatenative RNNs) augment the input sequence with auxiliary information, enabling the <span class=\"caps\">RNN</span> to model rich and complex interactions between the input and output spaces. Josh and David present an overview of conditional <span class=\"caps\">RNN</span> architectures, both simple (generative concatenative networks) and elaborate (attention models), and discuss diverse potential applications, ranging from automatic generation of personalized user content to video captioning.</p>\\n<p>They then demonstrate a real-world example\\u2014an interactive Twitter bot that dynamically generates text responses based on user requests\\u2014before concluding with a rigorous debate about the implications of such technology, including both the potential business impact (could RNNs be used to generate fake Yelp reviews?) and the philosophical question of whether it represents machine creativity.</p>\\n<p>Josh and David will provide a fully open source implementation using the <a href=\"http://deeplearning4j.org/\">Deep Learning for Java (DL4J) library</a> and make the training dataset freely available.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Much of the success of deep learning in recent years can be attributed to scale\\u2014bigger datasets and more computing power\\u2014but scale can quickly become a problem. Distributed, asynchronous computing in heterogenous environments is complex, hard to debug, and hard to profile and optimize. Martin Wicke demonstrates how to automate or abstract away such complexity, using TensorFlow as an example. Martin covers the sources of complexity for large-scale machine-learning systems, explains how to mitigate such complexity, and touches upon the future avenues for this work, where, unsurprisingly, machine learning will be used to understand and improve machine learning.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Heterogeneous systems: In most cases, we have to contend with at least CPUs and GPUs, but for very large problems, FPGAs and custom hardware are increasingly used.</li>\\n<li>Code complexity: As AI models become more complex, traditional software engineering issues creep into the everyday lives of researchers. Problems with code reuse, proper abstraction, and data isolation become increasingly important.</li>\\n<li>Model complexity: The inner workings of complex models are impossible to understand without custom tooling. Custom visualization and debugging tools are increasingly necessary, but specific approaches can help understand certain classes of models. (For instance, Deepdream is based on a tool to better understand how convolutional neural networks work.)</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Creating production-ready analytical pipelines can be a messy, error-prone undertaking. In the simplest case, connecting a workflow of heterogeneous components, such as databases, feature enrichment and visualization tools, programming languages, and analytical engines, requires maintaining connections between multiple tools. And each of these tools is subject to its own development cycle. In the case of projects involving big data or analytics over real-time streaming data, the difficulties only increase.</p>\\n<p>The Trusted Analytics Platform (<span class=\"caps\">TAP</span>) is an open source-based platform combining elements from popular projects, including Python, Spark Streaming, GearPump, and Docker. <span class=\"caps\">TAP</span> enables data scientists to ask bigger questions of their data and carry out principled data science experiments\\u2014all while engaging in iterative, collaborative development of production solutions with application developers. Since <span class=\"caps\">TAP</span> was introduced in 2015, project contributions have included popular analytics tools and libraries, including the ability to \\u201c bring your own.\" Kyle Ambert offers an overview of these open source project contributions, which include a new Docker-based architecture and improved Spark integration, and explains what they mean to data scientists. Kyle also discusses a healthcare machine-learning-based solution focused on the identification of hospital patients at risk for readmittance.</p>\\n<p><strong>This session is sponsored by Intel.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n          We are moving to real time, and data scientists need a framework to evaluate and architect the right technologies. Darryl Smith shares tested principles for streaming architectures so you can navigate solution options quickly. The mission\\u2014reinforced by the rise of Apache Kafka and the importance of exactly once semantics\\u2014is clear: move from batch to real time. Darryl helps you get there.\\n<p>Topics include:</p>\\n<ul>\\n<li>Starting from the data lake, including an overview of the <span class=\"caps\">EMC</span> data lake architecture</li>\\n<li>The importance of real-time loading and streaming</li>\\n<li>Evaluating fast relational, NoSQL, and NewSQL databases to complement the data lake</li>\\n<li>Bringing real-time and business critical data together</li>\\n<li>Rendering real-time data</li>\\n<li>Supporting an agile application development team</li>\\n</ul>\\n<p><strong>This session is sponsored by MemSQL.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Every day, analysts at Citi, Standard Chartered Bank (<span class=\"caps\">SCB</span>), and Polaris analyze transactions and behaviors for indicators of money laundering, fraud, and human trafficking. The most comprehensive investigation must leverage massive volumes of data from financial institutions, law enforcement and government agencies, social media sites, telecommunication organizations, and enterprises, but it often comes with varied data quality standards or in unusable formats and nonstandard structures. Analysts ability to explore, clean, shape, and integrate the data can\\u2019t be slowed down by time-consuming user compute cycles or resource-intensive <span class=\"caps\">ETL</span> processes. And the entire flow\\u2014from source data to usable information\\u2014must be auditable and fully trusted.</p>\\n<p>Join data experts from Citi, Standard Charter Bank, and Polaris for a panel discussion moderated by Shankar Ganapathy. Learn about the principles, technologies, and processes they have used to design a highly efficient information management pipeline architected around the Hadoop ecosystem, and hear how they eliminate historical obstacles and reduce risk with new methods for data integration and enrichment, data quality, governance, and collaboration.</p>\\n<p><strong>This session is sponsored by Paxata.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Trends driving demand for automated machine learning (<span class=\"caps\">AML</span>) include the growing availability of big data through Hadoop architecture and the shortage of experienced data scientists. Successful big data projects require careful consideration of project definition, success criteria, organizational design, and implementation, and executives contribute vitally to this process. Jeremy Achin teaches executives how to identify opportunities to optimize their business using machine learning. This means radically reducing time to value the total cycle time from data to predictions and broadening the pool of people who can contribute to machine-learning projects without sacrificing quality. Jeremy also introduces DataRobot, <span class=\"caps\">AML</span> software that supports and reflects these best practices, and explains how DataRobot interfaces with <span class=\"caps\">HDFS</span>, <span class=\"caps\">YARN</span>, Apache Spark, and other key components in a Hadoop cluster.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Identifying opportunities to optimize business using machine learning</li>\\n<li>Developing <span class=\"caps\">AML</span> capability in nonprogrammers</li>\\n<li>Integrating data science capability into operational units</li>\\n<li>Defining data science projects in business terms</li>\\n<li>Implementing effective predictive models</li>\\n<li>Assessing and interpreting predictions</li>\\n</ul>\\n<p><strong>This session is sponsored by DataRobot.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>To thrive as a data-driven enterprise, organizations must train their eye for the best opportunities for analytic impact. Creative problem-solving through collaboration enables each individual working with data to utilize their strengths and specialized skills. Hypothesis testing for business impact through analytics requires finding ways to share hypotheses as well as the data assets that are used to test assumptions through data tools like data inventories, data catalogs, code repositories, and data visualization tools.</p>\\n<p>Join moderator John Akred of Silicon Valley Data Science and analytics and data experts from global pharmaceutical company Pfizer, the City of San Diego, and Neustar, the first real-time provider of cloud-based information services, for a lively discussion on how these businesses have tapped into the strengths of a broad team of professionals beyond the boundaries of the analytics specialists and are reimagining how to structure organizations for analytic creative thinking.</p>\\n<p><strong>This session is sponsored by Alation.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Hadoop platforms can be very effective and efficient at analyzing historical data at scale in minutes or smaller-scale data in near real time. Data lakes provide large-scale data processing and storage at low cost but often struggle to deliver real-time analytic response without significant investment in large clusters. Technologies like Apache Spark promise to take the Hadoop stack beyond batch, but even they rely on a \\u201cmicrobatch\\u201d approach instead of truly streaming in real time. Further, the complexities associated with development and ongoing management of a data lake that aims to deliver real-time analytic response can be costly and overwhelming.</p>\\n<p>If your organization truly needs real-time (subsecond) analytic response on live, streaming data, consider the benefits of plugging a <span class=\"caps\">GPU</span>-accelerated database into your data lake. GPUs are often embedded in compute-intensive technologies like video games, cars, and mobile devices. They\\u2019re now gaining traction in the data center. Amit Vij and Mark Brooks explore the <span class=\"caps\">GPU</span>: a single-chip processor that has historically and primarily been used to compute 3D functions such as lighting effects, object transformations, and motion. This unique architecture allows GPUs to process many computations efficiently and quickly, making them ideal for today\\u2019s streaming datasets and IoT use cases. Amit and Mark outline the dramatic performance benefits a <span class=\"caps\">GPU</span> database offers and explain how to integrate it with Hadoop data lakes, discussing connectors for components such as Kafka, Nifi, Spark, Spark Streaming, and Storm in addition to proprietary tools like Tableau.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Massively parallel big data platforms are quickly becoming the industry standard for organizations looking to extract greater value from data. As architectures have shifted, application development paradigms have also changed to reflect growing needs for agility, scale, robustness, efficiency, and ease of collaboration on these new platforms.</p>\\n<p>Companies like Ericsson and Uber are touting the advantages they have realized by implementing microservices applications, including:</p>\\n<ul>\\n<li>Abstraction</li>\\n<li>Continuous, high availability</li>\\n<li>Scalability and extensibility</li>\\n<li>Process isolation</li>\\n<li>Event-based processing</li>\\n<li>Nonblocking functions</li>\\n<li>Application development agility</li>\\n<li>Better hardware utilization</li>\\n</ul>\\n<p>Crystal Valentine draws on lessons learned from companies like Uber and Ericsson to outline the key principles to developing a microservices application. Along the way, Crystal describes how certain next-gen application areas\\u2014such as machine learning\\u2014are particularly well suited to implementation in a microservices architecture rather than a legacy application paradigm.</p>\\n<p><strong>This session is sponsored by MapR.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Data is a company\\u2019s lifeblood, and more data exists than ever before\\u2014in more disparate silos. Getting the insights you need, sifting through data, and answering new questions have all been complex, hairy tasks that only data jocks have been able to do. The entire process is slow and daunting, and businesses are never satisfied with the outcome. Andrew Yeung and Scott Anderson explore new ways to challenge the status quo through automated data blending and smart data discovery across diverse sources to speed insights for business users. See and hear about real customer use cases and learn how to reinvent your organization\\u2019s analytics capability.</p>\\n<p><strong>This session is sponsored by ClearStory Data.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>When building your data stack, the architecture could be your biggest challenge. Yet it could also be the best predictor for success. With so many elements to consider and no proven playbook, where do you begin to assemble best practices for a scalable data architecture? Ben Sharma offers lessons learned from the field to get you started. If you are concerned with building a data architecture that will serve you now and scale for the future, this is a must-attend session.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>A recommended data lake reference architecture</li>\\n<li>Considerations for data lake management and operations</li>\\n<li>Considerations for data lake security and governance</li>\\n<li>Metadata management</li>\\n<li>Logical data lakes to enable ground-to-cloud hybrid architectures</li>\\n<li>Self-service data marketplaces for more democratized data access</li>\\n</ul>\\n<p><strong>This session is sponsored by Zaloni.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Building, running, and governing a data lake and production data applications on Hadoop is often a difficult process filled with slow development cycles and painful operations. Not only are traditional development tools and techniques missing from the Hadoop ecosystem, but mastering data ingestion and data integration, as well as enterprise governance and security, has become a formidable challenge when building big data solutions. The challenge only increases as the Hadoop ecosystem continues to grow, use cases mature, SLAs intensify, and services become customer facing and revenue generating. And while the IT organization owns the task of mitigating these issues, more importantly, it also has an opportunity to enable the business to reduce time to insights and make better decisions faster by providing them with a modern self-service environment for their data.</p>\\n<p>Jonathan Gray proposes a modern, unified integration architecture that helps IT mitigate these issues while enabling businesses to reduce time to insights and make decisions faster through a modern self-service environment. Drawing on his experiences as an early committer on Apache HBase, building real-time systems on Hadoop at Facebook, and working with customers at Cask, Jonathan explores the benefits of the Cask Data Application Platform (<span class=\"caps\">CDAP</span>), the first unified integration platform for big data and the IoT. <span class=\"caps\">CDAP</span> ensures data and process consistency between applications and underlying infrastructure technologies, across multiple environments, and between different parts of the IT organization and provides a single environment for design, operations, data science, and governance for data lakes, data applications, and the IoT. Jonathan discusses the requirements for building and running modern production applications on Hadoop and outlines the architecture <span class=\"caps\">CDAP</span> offers to address the challenges in the context of common use cases.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>More and more frequently, owners of Hadoop deployments find themselves facing the challenge of supporting data science ecosystems like Python and R, both adjacent to and within their Hadoop infrastructure. Although these technologies promise powerful data science insights, they can also be complex to manage and deploy. As people build out data science sandboxes and production environments, they discover a number of challenges ranging from basic package management and data lineage to reproducibility and governance of data science artifacts.</p>\\n<p>Peter Wang distills the vast array of Hadoop and data science tools and architectures down to the essentials that deliver a powerful and lightweight stack quickly so that you can accelerate time to value while meeting your data science, governance, and IT needs. Throughout the discussion, Peter highlights challenges and best practices drawn from real-world customer use cases.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>The next generation of hardware and cloud topologies</li>\\n<li>How Anaconda, an open data science platform, continuously incorporates the latest innovations available in the market for data scientists to do their work while preserving the ability for IT to manage and operate their production environment</li>\\n</ul>\\n<p><strong>This session is sponsored by Continuum Analytics.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>The last 10 years have seen Hadoop move from storage cost killer to contender for the next do-or-die platform in financial services. Fintech organizations have used Hadoop for building advanced scientific, operational data stores, data warehousing and reporting, consumer application development, data visualization, and real-time processing. But what\\u2019s the point at which Hadoop tips from a Swiss-army knife of use cases to a new foundation that rearranges how the financial services marketplace turns data into profit and competitive advantage? This panel of expert practitioners looks into the near future to see if the inflection point is at hand.</p>\\n<p><strong>This session is sponsored by Arcadia Data.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>The emerging Industrial Internet of Things is giving rise to what is predicted to be a sweeping change that will fundamentally transform industries and reconfigure the technology landscape. Sensor data is expected to dwarf the data volumes that defined the first decade of big data, and leading companies will be those that effectively derive value from this next wave of information and opportunity. Yet, the challenges for enterprises remain formidable. The information required to enable breakthrough insights is typically fragmented within the domains of information technology (IT) and operational technology (OT), requiring both technical and cultural changes. Further, organizations are realizing that analytics on sensor data is vastly more diverse and complex than analyzing traditional big datasets like weblogs. Cheryl Wiebe explores how leading companies harness the IoT by putting IoT data in context, fostering collaboration between IT and OT and enabling a new breed of scalable analytics.</p>\\n<p><strong>This session is sponsored by Teradata.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Two of the hottest topics in analytics are data lakes and big data in the cloud. Enterprises like Asurion Services are using Hadoop and Informatica\\u2019s big data management solutions to deliver faster, more flexible, and more repeatable big data projects. By adopting a big data management architecture on top of Hadoop, enterprises can quickly and flexibly ingest, cleanse, master, govern, secure, and deliver all types of data on-premises or in the cloud for business data lake initiatives ranging from marketing effectiveness to fraud detection. Viral Shah explains how enterprises like Asurion Services are leveraging big data management solutions to accelerate enterprise data lake initiatives for business value.</p>\\n<p><strong>This session is sponsored by Informatica.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>A study by HPE\\u2019s security unit found that 70 percent of popular consumer IoT devices are easily hackable. Many of the simplest IoT (or machine-to-machine) devices lack adequate processing power and storage to host endpoint security software. As a result, attackers can exploit vulnerabilities in consumer devices and mobile applications to gain remote access to internal networks and expose users to man-in-the-middle attacks.</p>\\n<p>So can traditional PC security solutions work effectively with embedded devices? With real-time sensor and device data streaming into big data platforms for analytics, security concerns encompass loss of data and intellectual property, potential for physical harm, and risk to brand and reputation. And, right now, data privacy regulations such as the European Union\\u2019s General Data Protection Regulation (<span class=\"caps\">GDPR</span>) are harmonizing legislation across regions and making a global impact. Reiner Kappenberger addresses architectural and developer concerns related to security and privacy: how homomorphic encryption enables nearly all computation to be done directly on encrypted data, how data can be anonymized and protected at scale today, how new standards are defining effective approaches, and how use cases are being transformed.</p>\\n<p><strong>This session is sponsored by <span class=\"caps\">HPE</span> Security \\u2013 Data Security.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Jonathon Whitton details how <span class=\"caps\">PRGX</span> is using Talend and Cloudera to load two million annual client flat files into a Hadoop cluster and perform recovery audit services in order to help clients detect, find, and fix leakage in their procurement and payment processes. Jonathon also explores how <span class=\"caps\">PRGX</span> unzips and decrypts transactional data received from customers so it can analyze the data using a joblet in Talend, shaving hours of time off each job.</p>\\n<p><strong>This session is sponsored by Talend.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>The trend of deploying Hadoop on virtual infrastructure is rapidly increasing. Martin Yip explores the benefits of virtualizing Hadoop through the lens of three real-world examples and walks you through the basics of virtualizing Hadoop, the first step in providing Hadoop on the public or private cloud.</p>\\n<p>All dynamic environments that require rapid, on-demand provisioning of Hadoop clusters need virtualization as the core infrastructure. Martin delves deeply into three different real-world deployments (on small, medium, and large scales) to demonstrate how enterprises are deploying Hadoop differently on virtual machines. Along the way, Martin covers virtual machine placement onto physical host servers, performance, obstacles, and best practices for building your Hadoop cluster on a virtual platform. You\\u2019ll leave with the confidence to deploy your Hadoop clusters using virtualization.</p>\\n<p><strong>This session is sponsored by VMware.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Big data is being thrown around as a silver-bullet solution to enable organizational agility and transformation. All your data in one place sounds great on paper, but is it really? Thomas Place explores the big data journey of the world\\u2019s biggest payment processor, which came dangerously close to building a data swamp before pivoting to embrace governance and quality-first patterns. This case study includes patterns, partners, successes, failures, and lessons learned to date and reviews the journey ahead.</p>\\n<p><strong>This session is sponsored by Ataccama.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Big data and analytics is a team sport empowering companies of all kinds to achieve business outcomes faster and with greater levels of success. Dell <span class=\"caps\">EMC</span> has integrated all the key components for modern digital transformation, taking you on a big data journey that focuses on analytics, integration, and infrastructure. Its portfolio provides the flexibility to buy or build your analytics ecosystem, and offerings range from servers and data lakes to flexible analytics with a turn-key development platform. Carey James explains how the formation of Dell Technologies and Dell <span class=\"caps\">EMC</span> can help you on your data analytics journey and how you can turn actionable insights into new business opportunities.</p>\\n<p><strong>This session is sponsored by Dell <span class=\"caps\">EMC</span>.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>The flux capacitor was the core component that made time travel possible in <em>Back to the Future</em>, processing garbage as a power source. Did you know that you can achieve the same affect in machine learning? Ingo Mierswa, RapidMiner\\u2019s cofounder and <span class=\"caps\">CEO</span>, offers a case study on how he took \\u201cgarbage\\u201d data drawn from 250K data scientist RapidMiner users and through machine learning turned it into Wisdom of Crowds, which helps novice and expert data scientists alike accelerate the creation of their predictive models by delivering expert recommendations about what other scientists would do at every step in their predictive analytics process. Ingo covers the most frequently used machine-learning techniques, what data preparation most experts perform before modeling, and how those behaviors have changed over time, along with other interesting patterns.</p>\\n<p><strong>This session is sponsored by RapidMiner.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>A major challenge in today\\u2019s big data world is getting data into a data lake in a simple, automated way. Many organizations use Python or another language to code their way through these processes. But when the number of data sources increases into the hundreds\\u2014or often thousands\\u2014coding scripts for each source becomes time consuming and extremely difficult to manage and maintain.</p>\\n<p>Developers need the ability to create a simple process that can support many disparate data sources by detecting metadata and passing that metadata through what Pentaho calls \\u201cmetadata injection.\\u201d With this capability, teams can drive hundreds of data ingestion and preparation processes through just a few transformations, reducing development time and risk and speeding time to insights. Chuck Yarbrough outlines this template-driven data ingestion and explains how to simplify and automate your data ingestion processes.</p>\\n<p><strong>This session is sponsored by Pentaho.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Defining the challenges, outlining the goals, identifying the use cases, and tracking <span class=\"caps\">ROI</span> are always important considerations when building a big data strategy, but what about greater behind-the-scenes challenges like security, consumer privacy, fraud detection, governance, and financial investment? Each impacts the business and its brand. Mastercard\\u2019s Nick Curcuru hosts an interactive fireside chat with Anthony Dina from Dell to explore how the flexibility, scalability, and agility of Hadoop big data solutions allow one of the world\\u2019s leading organizations to innovate, enable, and enhance the customer experience while still expanding emerging opportunities. You\\u2019ll also have the chance to discuss the most important considerations for driving big data strategies and implementations.</p>\\n<p><strong>This session is sponsored by Dell.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>As software becomes more free and open, it also is becoming more complex and expensive to operate. How can we in the open source community clarify best practices and recommended operations to model complex, interconnected services so users can focus on their ideas? How can we as developers deliver recommended best practices in our applications so users are free to focus on the science on their choice of substrate (e.g., laptop, cloud, or bare metal/x86, <span class=\"caps\">ARM</span>, ppc64el, or s390x)?</p>\\n<p>Antonio Rosales offers an overview of Juju, an open source method to distill the best practices and operations needed to use interconnected big data solutions, such as modeling a multinode Apache Spark cluster across a diverse set of substrates and adding other services to build additional solutions. Antonio leads a demo of Juju in action, and you\\u2019ll be able to take all software shown to try yourself in a free and open source manner.</p>\\n<p><strong>This session is sponsored by Canonical.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Customers are looking to extend the benefits beyond big data with the power of the deep learning and accelerated analytics ecosystems. Jim McHugh explains how customers are leveraging deep learning and accelerated analytics to turn insights into AI-driven knowledge and covers the growing ecosystem of solutions and technologies that are delivering on this promise, such as the <span class=\"caps\">NVIDIA</span> <span class=\"caps\">DGX</span>-1, which integrates power of deep learning and accelerated analytics together in a single hardware and software system.</p>\\n<p><strong>This session is sponsored by <span class=\"caps\">NVIDIA</span>.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>In-chip analytics blasted onto the scene a few years back, impressing companies with its ability to significantly impact how companies handle complex data (both large and disparate) with less hardware while eliminating the data preparation nightmare. But the real impact of in-chip analytics is only now being realized as companies exploit in-chip\\u2019s extensibility, scaleability, and flexibility to take business intelligence to the next level with new IoT and AI technologies. Guy Levy-Yurista explains the unexpected consequences of making big data processing significantly more agile than ever before and the impact it\\u2019s having on human insight consumption.</p>\\n<p><strong>This session is sponsored by Sisense.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Legacy enterprise data warehouse (<span class=\"caps\">EDW</span>) architecture, geared toward day-to-day workloads associated with operational querying, reporting, and analytics, is often ill-equipped to handle the volume of data or traffic and varied data types associated with modern ad hoc analytics platforms. Faced with the challenges of increasing pipeline speed, aggregation, and visualization in a simplified, self-service fashion, organizations are considering newer technologies such as Apache Spark, Hadoop, Kafka, and columnar databases as key enabling technologies to optimize their <span class=\"caps\">EDW</span> architectures.</p>\\n<p>Jack Gudenkauf explores how organizations have successfully deployed tiered hyperscale architecture for real-time streaming with Spark, Kafka, Hadoop, and Vertica and discusses how advancements in hardware technologies such as nonvolatile memory, SSDs, and accelerators are changing the role of big data and big analytics platforms in an overall enterprise-data-platform strategy.</p>\\n<p><strong>This session is sponsored by Hewlett Packard Enterprise.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Data wrangling has quickly become a hot topic and technology category within the big data analytics industry. Stakeholders across business and IT are hungry to learn the right way to think about applying these new wrangling solutions to the data and analytics efforts of their organization. As with any emerging technology, the leading question from organizations still learning about data wrangling is, \\u201cHow are other organizations wrangling data and what are the benefits they are realizing?\\u201d If this question sounds familiar, then this is the session for you.</p>\\n<p>Connor Carreras offers an in-depth review of the most popular use cases for data wrangling solutions among enterprise organizations, explaining how leading organizations, such as PepsiCo, Royal Bank of Scotland, and Kaiser Permanente, are leveraging data wrangling to accelerate analysis processes and uncover new sources of business value by incorporating new data sources that were previously too difficult to work with. Connor also addresses common questions for data wrangling solutions, including: Where do data wrangling tools fit? Who are the ideal users? How are security and data governance managed?</p>\\n<p><strong>This session is sponsored by Trifacta.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Machine data is growing at an exponential rate, and a key driver for this growth is the Internet of Things (IoT) revolution. Johan Bjerke explains how to make use of and find value in the unstructured machine data that plays an important role in the new connected world.</p>\\n<p>From log files to sensors to wire data to <span class=\"caps\">SCADA</span>, Johan demonstrates how to use IoT data for multiple use cases, including operations, troubleshooting, security and safety, and data analytics, and shows how to use analytics to transform your raw data into insights. Johan also discusses the role of high-speed event collection, Docker containers, and integration with Apache Kafka and <span class=\"caps\">HDFS</span> within the Splunk platform.</p>\\n<p>Johan showcases planes, trains, and automobiles (and power and buildings) through customer stories. Join in to hear how Volkswagen monitors its electric cars, Coca-Cola monitors its freestyle vending machines, and Gatwick Airport speeds passengers through to departures using sensor data from security and check-in.</p>\\n<p><strong>This session is sponsored by Splunk.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Businesses are clamoring to capture all data possible and harness it as a revenue driver. The challenge is bringing the data together. Companies that can capture and harness this data can benefit accordingly.</p>\\n<p>When it comes to data management in Hadoop, the architecture foundation makes all the difference for performance. Jake Dolezal shares research into the performance of data quality and data management workloads on Hadoop clusters. Jake discusses a <span class=\"caps\">YARN</span>-based approach to data management and outlines highly effective IT resource utilization techniques to achieve extreme agility for organizations and performance gains in Hadoop.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Managing complex, high-volume data with identity and entity resolution in the most demanding applications, such as customer data quality</li>\\n<li>Device and entity matching in IoT data</li>\\n<li>Pure <span class=\"caps\">YARN</span> integration</li>\\n<li>How to leverage <span class=\"caps\">MDM</span> capabilities to ensure precise data via business user data access and management</li>\\n</ul>\\n<p><strong>This session is sponsored by RedPoint Global.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Businesses today are driven to adopt big data technologies for their analytics for a number of reasons: There are new types of data sources that are not handled by the existing data warehouses; there is a growth in data velocity and data volumes that becomes prohibitive to process using existing data warehouses; or there are different types of analytics not supported by existing infrastructure, to name a few.</p>\\n<p>Current data warehouse technologies are increasingly challenged to handle the growth in data volume, new data types, and multiple analytics types. Hadoop has the potential to address these issues, but you need to solve several complexities before you can realize its full benefits. Amar Arsikere showcases the business and technical aspects of augmenting and modernizing data warehouses on Hadoop. Amar focuses on a customer\\u2019s journey through business justification and data architecture. Along the way, he shares lessons learned and highlights potential roadblocks and strategies for addressing them, as well as cost/benefit information so that you can justify using Hadoop as a platform to augment data warehouses.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>How to support your business user\\u2019s <span class=\"caps\">EDW</span> applications on Hadoop without losing<br/>\\nperformance and concurrency</li>\\n<li>How to take care of data synchronization to meet your SLAs</li>\\n<li>How to ensure data access control and security</li>\\n<li>How to create a project plan and business justification</li>\\n</ul>\\n<p><strong>This session is sponsored by Infoworks.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the first day of keynotes.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Since its inception, big data solutions have best been known for their ability to master the complexity of the volume, variety, and velocity of data. But as we enter the era of data democratization, there\\u2019s a new set of concerns to consider. Mike Olson discusses the new dynamics of big data and how a renewed approach focused on where, who, and why can lead to cutting-edge solutions.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>During election season, we\\u2019re tasked with considering the next four years and comparing platforms across candidates. What\\u2019s good for the country is good for your data. Consider what the next four years will look like for your organization. How will you lower costs and deliver innovation? Jack Norris reviews the requirements for a winning data platform, such as speed, scale, and agility.</p>\\n<p><strong>This keynote is sponsored by MapR.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Susan Woodward discusses venture outcomes\\u2014what fraction make lots of money, which just barely return capital, and which fraction fail completely. Susan uses updated figures on the fraction of entrepreneurs who succeed, including some interesting details on female founders of venture companies.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>The power of artificial intelligence and advanced analytics emerges from the ability to analyze and compute large, disparate datasets from varied devices and locations, such as predictive medicine and automated cars, at lightning-fast speed. These real-time insights require continued innovation to fuel the changing landscape of AI. Martin Hall explains why collaboration and openness are the key elements driving innovation in AI.</p>\\n<p><strong>This keynote is sponsored by Intel.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>It is no surprise that reducing operational IT expenditures and increasing software capabilities is a top priority for large enterprises. Given its advantages, open source software has proliferated across the globe. While there\\u2019s been much discussion on open source versus commercial, CIOs and CTOs at Global 1000 enterprises are increasingly interested in solutions that blend the benefits of both. However, key challenges must be overcome. Enterprise architecture groups are now faced with the difficult task of selecting the right open source software components from an ever-growing set of options, figuring out how to integrate them, and ensuring they work together.</p>\\n<p>Ron Bodkin explains how Teradata drives open source adoption inside enterprises through a range of initiatives, from direct contributions to open source projects designed to address key enterprise requirements to orchestration software that integrates Hadoop with other components of the analytical ecosystem, providing enterprises with the technical expertise required to fully leverage the benefits of open source in their analytical ecosystem.</p>\\n<p><strong>This keynote is sponsored by Teradata.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>How do we discover what we\\u2019re not looking for? How can we become more serendipitous? In the age of big data and bioinformatics, such questions are more relevant than ever. We develop new tools to help us spot clues in mountains of information, and machines are getting better and better at aiding discovery. And yet, serendipity remains a very human art. Pagan Kennedy discusses the origins of the word serendipity and qualities of mind that lead to successful searches in the deep unknown.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>It is clear that we\\u2019re at a critical inflection point in the industry\\u2014organizations are realizing that they must quickly adapt in order to keep pace in today\\u2019s ever changing digital economy. Data, your most precious commodity, is increasing at an alarming rate. At the same time, an emerging business imperative has made this data a component of your deepest insights, allowing you to focus on your business outcomes. Patricia Florissi explains why the recent formation of Dell <span class=\"caps\">EMC</span> ensures that your analytics capabilities will be stronger than ever.</p>\\n<p><strong>This keynote is sponsored by Dell <span class=\"caps\">EMC</span>.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Healthcare, a $3 trillion industry, is ripe for disruption through data science. However, there are many challenges in the journey to make healthcare a truly transparent, consumer-centric, data-driven industry. Sriram Vishwanath explains where data science can have massive impact in healthcare and dispels myths where its apparent use contradicts realities within the healthcare ecosystem.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\" style=\"overflow: visible;\">\\n<p>American politics is adrift in a sea of polls. This year, that sea is deeper than ever before\\u2014and darker. Data science is upending the public opinion industry. But to what end? In a brief, illustrated history of the field, Jill Lepore demonstrates how pollsters rose to prominence by claiming that measuring public opinion is good for democracy and asks, \\u201cBut what if it\\u2019s bad?\\u201d</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Quench your thirst with vendor-hosted libations (and snacks) while you check out all the exhibitors in the Expo Hall. It\\u2019s also a great time to meet and mingle with fellow attendees and Strata + Hadoop World speakers and authors.</p>\\n<p>The Booth Crawl is happening immediately after the afternoon sessions on Wednesday.</p>\\n<div class=\"event-photo-cluster\">\\n<img alt=\"Booth Crawl\" src=\"http://cdn.oreillystatic.com/oreilly/conferences/events/stratany2015/booth-crawl1.jpg\"/>\\n<img alt=\"Booth Crawl\" src=\"http://cdn.oreillystatic.com/oreilly/conferences/events/stratany2015/booth-crawl2.jpg\"/>\\n<img alt=\"Booth Crawl\" src=\"http://cdn.oreillystatic.com/oreilly/conferences/events/stratany2015/booth-crawl3.jpg\"/>\\n</div>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Birds of a Feather (BoF) discussions are a great way to informally network with people in similar industries or interested in the same topics.</p>\\n<p>BoFs will happen during lunch on Wednesday and Thursday. <strong>Create your own topic</strong> at the sign-up board near Registration <strong>or join one of the industry tables</strong> below.</p>\\n<div class=\"event-photo-cluster\">\\n<img alt=\"Birds of a Feather\" src=\"http://cdn.oreillystatic.com/oreilly/conferences/events/stratany2015/bof1.png\"/>\\n<img alt=\"Birds of a Feather\" src=\"http://cdn.oreillystatic.com/oreilly/conferences/events/stratany2015/bof2.jpg\"/>\\n<img alt=\"Birds of a Feather\" src=\"http://cdn.oreillystatic.com/oreilly/conferences/events/stratany2015/bof3.jpg\"/>\\n</div>\\n<p>This year\\u2019s Industry Birds of a Feather discussion topics include:</p>\\n<ul>\\n<li>Advertising &amp; Marketing</li>\\n<li>Energy</li>\\n<li>Finance</li>\\n<li>Government &amp; Policy</li>\\n<li>Healthcare</li>\\n<li>Media &amp; Entertainment</li>\\n<li>Retail &amp; Ecommerce</li>\\n<li>Telecommunications</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Calling all data enthusiasts.</p>\\n<p>The Data Dash is a great opportunity for you to network and connect with other data enthusiasts while supporting the local community.</p>\\n<div class=\"event-photo-cluster\">\\n<img alt=\"Data Dash\" src=\"http://cdn.oreillystatic.com/oreilly/conferences/events/stratany2015/dd1.jpg\"/>\\n<img alt=\"Data Dash\" src=\"http://cdn.oreillystatic.com/oreilly/conferences/events/stratany2015/dd2.jpg\"/>\\n<img alt=\"Data Dash\" src=\"http://cdn.oreillystatic.com/oreilly/conferences/events/stratany2015/dd3.jpg\"/>\\n</div>\\n<p><strong><span class=\"caps\">WHAT</span></strong><br/>\\nJoin Cloudera and O\\u2019Reilly Media for the Data Dash run/walk, held in conjunction with Strata + Hadoop World in New York. <a class=\"external\" href=\"http://www.meetup.com/Cloudera-Cares/events/228593529/\">Register free for the Data Dash</a>.</p>\\n<p><strong><span class=\"caps\">GIVING</span></strong><br/>\\nWe have partnered with <a class=\"external\" href=\"https://www.wearethorn.org/\">Thorn</a>, which will receive 100% of the proceeds of this year\\u2019s Data Dash. Thorn\\u2019s mission is to drive technology innovation to fight the sexual exploitation of children.</p>\\n<p><strong><span class=\"caps\">WHERE</span></strong><br/>\\nMeet us at 6:30am in <a href=\"https://www.google.com/maps/place/555+12th+Ave,+New+York,+NY+10019/@40.7640271,-74.0037473,17z/data=!3m1!4b1!4m2!3m1!1s0x89c2584938fb3d9d:0x35b8967164319e7f?hl=en\">Hudson River Park</a> at Pier 84. The start marker is an 8\\u2019 checkered flag.</p>\\n<p><strong><span class=\"caps\">WHY</span></strong><br/>\\nMeet fellow data enthusiasts, find a new pal to run with, and enjoy the fresh air. Finish at the same location around 7:30am and receive a swag bag with gifts from our sponsors.</p>\\n<p><strong><span class=\"caps\">ROUTE</span></strong><br/>\\nDuring the 2.7-mile route, you\\u2019ll run along the river and catch the beautiful sunrise over New Jersey and New York City. You\\u2019ll start and end your run at the same location.</p>\\n<p><a class=\"external\" href=\"http://www.strava.com/routes/2622058\">Map of route via Strava</a></p>\\n<p>We look forward to seeing you there.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<div class=\"event-photo-cluster\">\\n<img alt=\"Data After Dark\" src=\"//cdn.oreillystatic.com/en/assets/1/event/160/stratany2016_dad1.jpg\"/>\\n<img alt=\"Data After Dark\" src=\"//cdn.oreillystatic.com/en/assets/1/event/160/stratany2016_dad2.jpg\"/>\\n<img alt=\"Data After Dark\" src=\"//cdn.oreillystatic.com/en/assets/1/event/160/stratany2016_dad3.jpg\"/>\\n</div>\\n<div class=\"right\" id=\"sponsor_box\" style=\"width: 340px; margin-left: 20px;\">\\n<div style=\"float: right; width: 310px; padding: 0 10px; border: 1px solid #ccc; margin: 0; background-color: #fff;\">\\n<p><strong>Sponsored by:</strong> <br/></p>\\n<p><a class=\"external\" href=\"http://www.cisco.com/\"><img alt=\"cisco.com\" border=\"0\" src=\"//cdn.oreillystatic.com/conferences/images/logos/cisco-3.png\" style=\"margin: 10px;\"/></a> <a class=\"external\" href=\"http://www.cloudera.com/\"><img alt=\"cloudera\" border=\"0\" src=\"//cdn.oreillystatic.com/conferences/images/logos/cloudera_2015.png\" style=\"margin: 10px 0 10px 10px;\"/></a> <a class=\"external\" href=\"http://www.thoughtspot.com/\"><img alt=\"thoughtspot\" border=\"0\" src=\"//cdn.oreillystatic.com/conferences/images/logos/thought-spot.png\" style=\"margin: 10px;\"/></a><a class=\"external\" href=\"http://oreilly.com/\"><img alt=\"O'Reilly Media\" border=\"0\" src=\"//cdn.oreillystatic.com/conferences/images/logos/oreilly_logo_box.png\" style=\"margin: 10px 0 10px 10px;\"/></a> <a class=\"external\" href=\"http://www.emc.com\"><img alt=\"Dell EMC\" border=\"0\" src=\"//cdn.oreillystatic.com/conferences/images/logos/dell-emc-120.jpg\" style=\"margin: 10px;\"/></a></p>\\n</div>\\n</div>\\n<div class=\"left\" id=\"dad_logo\">\\n<p><img alt=\"Data After Dark\" height=\"140\" src=\"http://cdn.oreillystatic.com/en/assets/1/event/160/stratany2016_dad_image.png\"/></p>\\n</div>\\n<p>Join us aboard the City at Sea, the Intrepid Sea, Air &amp; Space Museum. While exploring life at sea on the Intrepid, enjoy an XD theater simulator, a lounge and DJ, and much more, as you wander through different areas with themed cocktails, bites of food, and entertainment. </p>\\n<p><strong>Note: This event is open to Strata + Hadoop World attendees only\\u2014bring your conference badge to get in. Expo Plus pass holders do not have access to this event.</strong></p>\\n<p><strong>Shuttle Service</strong>: For Data After Dark, there will be continuous pick-up and drop off from the Hilton Midtown, Marriott Marquis and Renaissance Midtown hotels and the Intrepid from 7:30PM-11:30PM. Attendees staying at the Renaissance Times Square can take the shuttle from the Renaissance Midtown hotel. For shuttle service between Javits and the hotels, please refer to the <a href=\"http://conferences.oreilly.com/strata/hadoop-big-data-ny/public/content/onsite#shuttle_service\">onsite page</a>.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Increasing demand for more and higher-granularity data continues to push the boundaries of what is possible to process using big data technologies. Netflix\\u2019s Big Data Platform team manages a highly organized and curated data warehouse in Amazon S3 with over 40 petabytes of data. At this scale, we are reaching the limits of partitioning, with thousands of tables and millions of partitions per table.</p>\\n<p>To work around the diminishing returns of additional partition layers, the team increasingly relies on the Parquet file format and recently made additions to Presto that resulted in an over 100x performance improvement for some real-world queries over Parquet data. The team is currently adding similar functionality to other processing engines like Spark, Hive, and Pig. Data written in Parquet is not optimized by default for these newer features, so the team is tuning how they write Parquet to maximize the benefit.</p>\\n<p>Ryan Blue explains how Netflix is building on Parquet to enhance its 40+ petabyte warehouse, combining Parquet\\u2019s features with Presto and Spark to boost <span class=\"caps\">ETL</span> and interactive queries. Information about tuning Parquet is hard to find. Ryan shares what he\\u2019s learned, creating the missing guide you need.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>The tools and techniques Netflix uses to analyze Parquet tables</li>\\n<li>How to spot common problems</li>\\n<li>Recommendations for Parquet configuration settings to get the best performance out of your processing platform</li>\\n<li>The impact of this work in speeding up applications like Netflix\\u2019s telemetry service and A/B testing platform</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Tyler Akidau offers a whirlwind tour of the evolution of massive-scale data processing at Google, from the original <a href=\"http://static.googleusercontent.com/media/research.google.com/en/us/archive/mapreduce-osdi04.pdf\">MapReduce</a> paradigm to the high-level pipelines of <a href=\"http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35650.pdf\">Flume</a> to the streaming approach of <a href=\"http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/41378.pdf\">MillWheel</a> to the portable, unified streaming/batch model of <a href=\"https://cloud.google.com/dataflow/\">Google Cloud Dataflow</a> and <a href=\"http://beam.incubator.apache.org\">Apache Beam</a> (incubating). Tyler examines in detail the basic architectural concepts that underlie these four models, highlights their similarities, contrasts their differences (particularly regarding traditional batch versus streaming), and provides insight into the use cases the drove the progression of the designs to what exists today. He also highlights similarities and differences with related open source systems such as <a href=\"https://flink.apache.org/\">Flink</a>, <a href=\"https://spark.apache.org/\">Spark</a>, <a href=\"https://storm.apache.org/\">Storm</a>, and <a href=\"http://gearpump.io\">Gearpump</a>, calling out ways in which they\\u2019re converging on and diverging from the <a href=\"http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf\">Beam model</a> and what that means when running Beam pipelines on their respective runners.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Today Metamarkets processes over 300 billion events per day, representing over 100 TB going through a single pipeline built entirely on open source technologies including Druid, Kafka, and Samza. Growing to such a scale presents engineering challenges on many levels, not just in design but also with operations, especially when downtime is not an option.</p>\\n<p>Xavier L\\xe9aut\\xe9 explores how Metamarkets used Kafka and Samza to build a multitenant pipeline to perform streaming joins and transformations of varying degrees of complexity, which then pushes data into Druid to make it available for immediate, interactive analysis at a rate of several hundreds of concurrent queries per second. Xavier describes how his team overcame the challenges around scaling this stack. With data growing an order of magnitude in the span of a few months, all systems involved started to show their limits. Xavier explains how Metamarkets uses extensive metric collection to manage both performance and costs and how it handles very heterogeneous processing workloads while keeping down operational complexity.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>In the last decade, fire services around the world have started to notice both the challenges and opportunities of our information society. More and more information about their operating environment has been made available, either as open data or by in-government data-sharing initiatives, which has made clear fire services\\u2019 poor information position as well as the resulting potential unnecessary risks their firefighters take. The fear in the current information climate is that if a firefighter is injured or killed on the job, the investigation might show that the fire service knew in advance (but didn\\u2019t or couldn\\u2019t share) vital information that could have prevented the incident.</p>\\n<p>By collecting more and more information, fire departments are able to gain insight into fire safety in their operating environments. And if the fire service can tell that certain behavior results in a higher risk for fire fatalities, isn\\u2019t it their moral obligation to inform the public? To help achieve these goals, various initiatives have started in order to gain better understanding of fire behavior through fire investigation and research, share the knowledge and, more importantly, the definitions that come with it, and improve the understanding of the data and its usability in the context of fire service operations.</p>\\n<p>For the last seven years, Bart van Leeuwen has been working on linked data research in the fire service, which has resulted in some groundbreaking initiatives and research opportunities. Bart shares his experiences as a firefighter, explaining where the problems are, outlining where current solutions fall short, and demonstrating the potential of using publicly available data to inform the public and firefighters better. At every step, Bart emphasizes a pragmatic and self-reflective approach to data quality.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Cluster computing frameworks such as Hadoop or Spark are tremendously beneficial in processing and deriving insights from data. However, long query latencies make these frameworks suboptimal choices to power interactive applications. Organizations frequently rely on dedicated query layers such as relational databases and key-value stores for faster query latencies, but these technologies suffer many drawbacks for analytic use cases.</p>\\n<p>User-facing applications are replacing traditional reporting interfaces as the preferred means for organizations to derive value from their datasets. In order to provide an interactive user experience, user interactions with analytic applications must complete in an order of milliseconds. To meet these needs, organizations often struggle with selecting a proper serving layer; many select serving layers because of their general popularity without understanding the possible architecture limitations.</p>\\n<p>Druid is an analytics data store designed for analytic (<span class=\"caps\">OLAP</span>) queries on event data. It draws inspiration from Google\\u2019s Dremel, Google\\u2019s PowerDrill, and search infrastructure, and many large technology companies are switching to Druid for analytics. Fangjin Yang discusses using Druid for analytics and explains why the architecture is well suited to power analytic dashboards.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Airbnb developed <a href=\"https://github.com/airbnb/caravel\">Caravel</a> to provide all employees with interactive access to data while minimizing friction. Caravel provides a quick way to intuitively visualize datasets by allowing users to create and share interactive dashboards; a rich set of visualizations to analyze your data, as well as a flexible way to extend the capabilities; an extensible, high-granularity security model allowing intricate rules on who can access which features and integration with major authentication providers (database, OpenID, <span class=\"caps\">LDAP</span>, OAuth, and REMOTE_USER through Flask AppBuilder); a simple semantic layer, allowing you to control how data sources are displayed in the UI by defining which fields should show up in which drop-down and which aggregation and function (metrics) are made available to the user; and deep integration with Druid that allows for Caravel to stay blazing fast while working with large, real-time datasets.</p>\\n<p>Caravel\\u2019s main goal is to make it easy to slice, dice, and visualize data. Maxime Beauchemin explains how Caravel empowers each and every employee to perform analytics at the speed of thought.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Uma Raghavan explains why you\\u2019re about to see companies whose business models depend on using their customers\\u2019 data, like Facebook, Google, and many others, scramble to keep up with the flood of new and evolving laws on data privacy. Whether using your customers\\u2019 data, buying third-party data, or mashing it up to make derivative data to better market to customers, create better products and services, or provide customer support, you could be in violation of emerging data privacy laws from around the world that carry stiff fines (up to 5% of revenue) or even jail time for violations of the use of people\\u2019s personal data. Ultimately, using customer data is a balance between what your business needs to do to run efficiently and effectively and what the brand, regulatory, and legal risks are if you get caught in violation of the law. Join Uma to learn what\\u2019s needed to manage your data risk effectively.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Data science is a process of abstraction. In order to explain or to predict a real phenomena, the process starts with acquiring and refining the data. It then moves between the three layers of abstraction: transformations (data abstraction), visualizations (visual abstraction), and modeling (symbolic abstraction). All three layers of abstraction together build a truer (or closer) representation of the real phenomena.</p>\\n<p>Data visualization (data-vis) helps us to understand the portrait and the shape of the data. The science of data-vis for exploratory data analysis is well developed for both static graphics (scatter-plot matrices, glyph-based approaches, geometric transforms like parallel coordinates) and interactive graphics (layering, brushing and linking, projections and tours). (For more information, see Amit Kapoor\\u2019s Strata + Hadoop World Singapore talk, <a href=\"https://www.youtube.com/watch?v=GtAe_UZGr28\">Visualizing Multidimensional Data</a>.) Though visualization is used in data science to understand the shape of the data, it\\u2019s not widely used for statistical models, which are evaluated based on numerical summaries.</p>\\n<p>Amit Kapoor demonstrates extending visualization to the statistical model (model-vis), which aids in understanding the shape of the model, the impact of parameters and input data on the model, the fit of the model, and where it can be improved. Model visualization can help us to understand the shape of the model and compare it to the shape of the data. It allows us to see the fit of the model and understand where the fit can be improved. It also allows us to better understand the parameters in the model and how the model changes when the parameters change as well as how the parameters changes when the input data changes.</p>\\n<p>The science and tools for model-vis are still very underdeveloped. Amit looks at practical examples of doing model-vis in regression (linear, lasso), classification (logistic, trees, <span class=\"caps\">LDA</span>), and clustering (hierarchical) problems that can help us better understand the model. This includes exploring model-vis approaches that:</p>\\n<ul>\\n<li>Visualize the model in data space as opposed to data in model space</li>\\n<li>Visualize the entire space of models</li>\\n<li>Visualize the same model with varying tuning parameters</li>\\n<li>Visualize the same model with different input datasets</li>\\n<li>Visualize the process of model fitting as opposed to final result</li>\\n</ul>\\n<p>Integrating these approaches for model-vis as a part of model evaluation strengthens a data scientist\\u2019s understanding of the model and leads to better model building, complementing data-vis for fitting better models as well as communicating the insight from the data science process.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>AI is moving from consumer applications to the enterprise and will soon affect all parts of operations from the customer to the product to the enterprise. Stephen Pratt, the <span class=\"caps\">CEO</span> of Noodle.ai and former head of Watson for <span class=\"caps\">IBM</span> <span class=\"caps\">GBS</span>, presents a shareholder value perspective on why enterprise artificial intelligence (eAI) will be the single largest competitive differentiator in business over the next five years\\u2014and what you can do to end up on top.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>A framework for why AI will be key to creating shareholder value</li>\\n<li>How to determine where to start and how to progress (with case studies)</li>\\n<li>How to manage spread of AI in your enterprise (with lessons from the past)</li>\\n<li>How to ensure proper adoption of AI solutions (implementing organizational change)</li>\\n<li>A future vision of the algorithmic enterprise</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Data should be something you can see, feel, hear, taste, and touch. Cameron Turner, Brad Sarsfield, Hanna Kang-Brown, and Evan Macmillan cover the emerging field of sensory data visualization, including data sonification. In an anecdotal survey, they explore real-life examples of solutions deployed to production in industries spanning from consumer goods to heavy industrial and large-scale manufacturing to the IoT that take advantage of auditory, touch, and other senses as alternative means of what has traditionally been called data visualization. They then investigate the hypothesis that we might better consume information by moving beyond words, numbers, and pictures and start using sound, smell, and even taste as a means to better understand the state of the world. Topics will tie into <a href=\"https://www.oreilly.com/ideas/cameron-turner-on-the-sound-of-data\">Cameron\\u2019s recent interview on the <em>O\\u2019Reilly Hardware Podcast</em></a>, which focused on data sonification, extending these topics into the future of sensory data collection and consumption.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Agility is king in the world of finance, and a message-driven architecture is a mechanism for building and managing discrete business functionality to enable agility. In order to accommodate rapid innovation, data pipelines must evolve. However, implementing microservices can create management problems, like the number of instances running in an environment.</p>\\n<p>Microservices can be leveraged on a message-driven architecture, but the concept must be thoughtfully implemented to show the true value. Jim Scott outlines the core tenets of a message-driven architecture and explains its importance in real-time big data-enabled distributed systems within the realm of finance. Along the way, Jim covers financial use cases dealing with securities management and fraud\\u2014starting with ingestion of data from potentially hundreds of data sources to the required fan-out of that data without sacrificing performance\\u2014and discusses the pros and cons around operational capabilities and using the same data pipeline to support development and quality assurance practices.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>To manage the ever-increasing volume and velocity of data within your company, you may have successfully made the transition from single machines and one-off solutions to large, distributed stream infrastructures in your data center powered by Apache Kafka. But what\\u2019s to be done if one data center is not enough?</p>\\n<p>Ewen Cheslack-Postava explores resilient multi-data-center architecture with Apache Kafka, sharing best practices for data replication and mirroring as well as disaster scenarios and failure handling. Ewen covers four scenarios\\u2014replication and failover for disaster recovery, data produced in one location but consumed in another, aggregate cluster for data analysis, and bidirection relication\\u2014discussing the requirements for each, providing a proven architecture, and explaining the benefits and limitations of the solution.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Digital consumer companies are disrupting the old guard and changing the way we do business in fundamental ways; for example, Uber, Airbnb, and Zipcar have disrupted the traditional businesses of taxis, hotels, and car rental companies by leveraging software capabilities to create new business models. Opportunities in the industrial world are expected to outpace consumer business cases. Time series data is growing exponentially as new machines around the world get connected. Venkatesh Sivasubramanian and Luis Ramos explain how GE makes it faster and easier for systems to access (using a common layer) and perform analytics on a massive volume of time series data by taking what they\\u2019ve learned from Apache Arrow and applying it today for highly efficient time series storage using Apache Apex, Spark, and Kudu.</p>\\n<p>At the heart of GE\\u2019s digital portfolio is the Predix platform, a cloud-based platform as a service (PaaS) for the Industrial IoT. Predix provides the tools, framework, guidelines, and best practices to enable you to create solutions to run industrial-scale analytics. Distributed processing is a de facto standard when dealing with a lot of data. But as there are many heterogenous data processing systems geared for different work loads, the need to agree and standardize the communication layer becomes paramount. Apache Arrow is working with several products in an attempt to do just that and agree on a common in-memory columnar storage layer to avoid serialization of data between different systems. Venkat and Luis discuss GE\\u2019s approach, which uses similar concepts to time series-centric data.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Complexities with industrial use cases (e.g., aviation and oil and gas)</li>\\n<li>Why GE chose Apache Apex (incubating) and how it simplifies real-time streaming ingestion and processing</li>\\n<li>Apache Spark for running in-stream analytics and machine-learning algorithms</li>\\n<li>Experiments with Apache Kudu (incubating) and lessons learned</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>IoT and financial trading platforms share some commonality: they intercept massive amounts of sensor or event data and must provide insights and actions in real time. The technology challenge is huge since we need to combine fast event streams, historical state, and consistent data update transactions (e.g., time series data and statistical aggregators) with data science and machine learning and present results through real-time dashboards or drive immediate corrective actions.</p>\\n<p>Traditional solutions like the Lambda Architecture or stream processing can\\u2019t meet the requirement, but with the latest advancements in Spark 2.0 coupled with Data Frames coprocessing in external real-time engines, we can analyze millions of complex events per second and deliver true real-time dashboards or actionable insights.</p>\\n<p>Yaron Haviv explains how to design real-time IoT and <span class=\"caps\">FSI</span> applications, leveraging Spark with advanced data frame acceleration. Yaron then presents a detailed, practical use case, diving deep into the architectural paradigm shift that makes the powerful processing of millions of events both efficient and simple to program.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Recent years have seen significant evolution of the Internet of Things. It has become increasingly easy to connect devices to the Internet and send sensory data to the public cloud.  However, the adoption of IoT platforms and stream analytics within the enterprise is lagging and less prevalent, an effect of the lack of skilled developers required to deploy an on-premises platform and the limited demonstration of high value in real-life use cases.</p>\\n<p>Intel IT has addressed these challenges by implementing an internal IoT platform, with the goal of allowing users and organizations to gain insights and business value from real-time analytics. The platform is based on several open source technologies including Akka, Kafka, and Spark Streaming with a full stack of algorithms including multisensor change detection and anomaly detection. To enable stream analytics at scale, Intel implemented a smart data pipe/stream processing framework, Pigeon, that implements a cluster capable of processing topologies that process the data according to any arbitrary logic determined by the users and is optimized to be easily deployed with Docker and CoreOS, which cuts down development by enabling a single developer to deploy a massive real time, elastic processing cluster with a click of a button. And unlike other IoT analytics implementations that settle for basic statistics or make many assumptions on the collected data, Intel implemented a generic analytics layer that uses machine learning and advanced statistical tests to provide meaningful insights to users in different use cases and business domains.</p>\\n<p>Moty Fania explains how Intel identified the set of characteristics and needs common to many IoT scenarios and made them available in one single reusable platform, offers a thorough overview of the platform\\u2019s architecture and related technologies (Akka, Kafka, Spark, Hadoop, etc.), demonstrates how Docker and CoreOS made the on-premises deployment easy, and reviews the generic analytics layer that uses machine learning to provide meaningful insights in different use cases and business domains. Moty concludes by discussing how Intel is using this platform to address problems that are not classical IoT use cases but can benefit from real-time analytics to achieve proactivity and operational excellence.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>The history of the digital age is being written in photographs. Today, for better or worse, everyone is a both a photographer and a subject. We need to start thinking about visual content in a radically different way, as both organizations and individuals. To innovate in the visual age, we have to crack the visual code. This means learning as much as we can, not only about how we see but also about how computers see so we can teach them to discover hidden opportunities and disregard hidden biases. If we try hard enough, maybe they\\u2019ll teach us to do the same.</p>\\n<p>Susan Etlinger explores why the ability to understand why one photo resonates and one doesn\\u2019t can make or break reputations, spark new products or lines of business, and make or save millions of dollars. But Susan won\\u2019t stop there, because the real value of these types of technologies isn\\u2019t just to see and analyze; it\\u2019s to help us make decisions and, sometimes, even make them for us. We haven\\u2019t even scratched the surface of what computer vision will be able to do with video and virtual reality, not to mention technology that hasn\\u2019t even been invented yet. Join Susan for a curious, cautionary, and perhaps hopeful view on sight itself and the strange world where every image is data with consequences.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Otto is the world\\u2019s second-largest online retailer in a highly competitive market space. Superior customer experience in terms of higher empathy, relevance, and speed is key to positive customer experience, and this is where AI comes into play. Rupert Steffner explores the cornerstones retailers have to focus when building their customers\\u2019 experience on artificial intelligence. It starts with having clear goals and a value system that finds the right balance between customer retention and revenue optimization. Even if AI is hereby built from the seller\\u2019s perspective, retailers will need a \\u201cgood AI\\u201d approach that treats consumers fairly and as partners to optimize long-term customer equity.</p>\\n<p>Technology talent is needed to organize the AI system for implementing agents that receive percepts from customer\\u2019s touchpoints and perform actions. Rupert outlines Otto\\u2019s system, which is built on a Dockerized microservice architecture and consists of reactive real-time bots working as a little army, and offers a pattern of how to orchestrate the topology. Although far from a generalized AI approach of managing customers in real-time, all dedicated bots are acting on the same belief states by now. The higher the scope of business functions, the more essential the need to act on conformed states to assure decision conformity.</p>\\n<p>Rupert explains the necessity of implementing coherence between AI\\u2019s capabilities and the business functions, perception to build empathy based on behavioral sensor data, and reasoning and learning to manage customer\\u2019s risks and opportunities in real time. Rupert then discusses the need for data locality for speed, achieved by implementing in-memory data grids and sharding by customer ID. Rupert concludes by shedding a light on the algorithms executed in real time\\u2014ranging from neural networks to random forest. Apart from narrow-task models, Rupert covers how Otto attacks a more generalized model to calculate the real-time conversion propensity along the overall site journey with every click. Analytic models perform within 3\\u20134 ms, which allows for parallel models operating the same task. The design pattern for automated decisioning splits the model execution from the cutoff and the callback/action trigger. This enables a history-versus-real-time negotiation and the proposition of a more generalized next-best action compared to a single-purpose optimization.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>At Strata + Hadoop World 2012, Amy O\\u2019Connor and her daughter Danielle Dean shared how they learned and built data science skills at Nokia. (At the time, Amy led the Big Data team at Nokia where Danielle was an intern still working on her PhD.) In the past four years, the landscape of data science has changed drastically. In 2012, most data science skills had to be learned organically; today, there have been major advances in tools, education, and the general culture in organizations taking on data science work. This year, Amy and Danielle explore how the landscape in the world of data science has changed and explain how to be successful deriving value from data today. Along the way, they outline the innovative methods they\\u2019ve used to find and build a data science skill set within their teams and for those in their customer base.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Most people will agree that interviewing is one of the most difficult and least enjoyable professional activities. Given the recent demand for data analytics and data science skills, it has become an increasingly daunting task for managers to adequately test and qualify candidates.</p>\\n<p>Interviewing hundreds of individuals of varying backgrounds requires a more efficient way of quantifying technical and cultural fit. This demand led to the creation of a deceptively simple data exercise, which reveals a surprising amount of information about interviewees. This test has been administered to dozens of candidates of varying experience levels and formal backgrounds. Data science is a highly integrated discipline. The variance in solutions provided by a physicist compared to a computer scientist is fascinating.</p>\\n<p>Tanya Cashorali digs deeper into these approaches and provides recommendations on how to administer and review test results for each type of candidate. You will receive a link to the publicly available dataset, the test questions, and the scoring rubric and learn how to save time vetting candidates, move away from unrealistic white-boarding interviews, and start hiring data scientists who will quickly provide business value.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>The electrical utility industry, an industry accustomed to gathering customer usage data on a monthly basis, now has access to a regular stream of data from smart meters and other smart sensors. Analyzing these new streams of data has given utilities the opportunity to understand their customer usage patterns, perform preventative maintenance, detect fraud, exercise demand management, and allocate resources more effectively.</p>\\n<p>Outages cost US businesses up to $150 billion a year. Due to aging infrastructure, the number of outages in the US has increased 285% since 1984. Utilities need improved data-driven methods for determining which infrastructure is most critically in need of replacement. Improving maintenance procedures for key pieces of equipment such as transformers, feeder cables, and reclosers can substantially reduce the risk of an outage. Kim Montgomery discusses some ways that analysis of smart grid sensor data can lead to better methods for replacing equipment before catastrophic failures occur.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Machine-learning tools promise to help solve data curation problems. While the principles are well understood, the engineering details in configuring and deploying ML techniques are the biggest hurdle. Ihab Ilyas explains why leveraging data semantics and domain-specific knowledge is key in delivering the optimizations necessary for truly scalable ML curation solutions.</p>\\n<p>Ihab focuses on entity consolidation, which is arguably the most difficult data curation challenge because it is notoriously complex and hard to scale. The problem statement sounds deceptively simple: find all the records from a collection of multiple data sources that refer to the same real-world entity. The problem has been traditionally modeled as a record clustering problem, followed by a mechanism to merge these resulting clusters into a unified representation.</p>\\n<p>There\\u2019s a large body of work on this problem by both academia and industry. Techniques have included human curation, automatic discovery of clusters using predefined thresholds on record similarity, and machine-learning techniques to discover classifiers able to label pairs of records as matches. Unfortunately, none of these techniques alone has been able to provide sufficient accuracy and scalability. Ihab provides deeper insight into the entity consolidation problem and discusses how machine learning, human expertise, and problem semantics collectively can deliver a scalable, high-accuracy solution.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Twitter generates billions and billions of events per day. Analyzing these events in real time presents a massive challenge. Karthik Ramasamy offers an overview of the end-to-end real-time stack Twitter designed in order to meet this challenge, consisting of DistributedLog (the distributed and replicated messaging system) and Heron (the streaming system for real-time computation).</p>\\n<p>DistributedLog\\u2014a replicated log service built on top of Apache BookKeeper that provides infinite, ordered, append-only streams that can be used for building robust real-time systems\\u2014is the foundation of Twitter\\u2019s publish-subscribe system. Heron is Twitter\\u2019s next-generation streaming system built from ground up to address its scalability and reliability needs. Both systems have been in production for nearly two years and are widely used at Twitter in a range of diverse applications, such as the search ingestion pipeline, ad analytics, image classification, and more.</p>\\n<p>Karthik describes Heron and DistributedLog in detail, covering use cases and sharing the operating experiences and challenges of running large-scale real-time systems at scale.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Many initiatives for running applications inside containers have been scoped to run on a single host. Using Docker containers for large-scale production environments poses interesting challenges, especially when deploying distributed big data applications like Apache Hadoop and Apache Spark.</p>\\n<p>Some of these challenges include container life-cycle management, smart scheduling for optimal resource utilization, network configuration and security, and performance. BlueData is \"all in\\u201d on Docker containers\\u2014with a specific focus on big data applications. BlueData has learned firsthand how to address these challenges for Fortune 500 enterprises and government organizations that want to deploy big data workloads using Docker.</p>\\n<p>BlueData\\u2019s Thomas Phelan demonstrates how to securely network Docker containers across multiple hosts and discusses ways to achieve high availability across distributed big data applications and hosts in your data center. Since we\\u2019re talking about very large volumes of data, performance is a key factor, so Thomas shares some of the storage options implemented at BlueData to achieve near bare-metal I/O performance for Hadoop and Spark using Docker as well as lessons learned and some tips and tricks on how to Dockerize your big data applications in a reliable, scalable, and high-performance environment.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>The Hearst Corporation monitors trending content on all of its 300+ sites worldwide, providing metrics to editors and promoting cross-platform content sharing. To facilitate this, Hearst has built a clickstream analytics pipeline entirely in the cloud that transmits and processes over 30 TB of data a day.</p>\\n<p>Rick McFarland offers an overview of Hearst\\u2019s clickstream analytics pipeline built with Spark, Kinesis, and Elasticsearch and demonstrates how you can use the same architecture for your clickstream data. Rick dives into how to do Spark Streaming from an Amazon Kinesis stream, use timestamps to cleanse and validate data coming from diverse sources, and make the system robust to changes in data types. Rick then explains how Hearst\\u2019s data scientists use cleansed data provided by the platform to perform ad hoc analyses, develop home-grown algorithms, and create visualizations and dashboards that support Hearst\\u2019s business stakeholders.</p>\\n<p>Rick also demos one of the major products powered by this pipeline: Buzzing@Hearst. This collaborative product enables thousands of editors across Hearst to monitor their content in real time, identify trends in articles, and syndicate articles across the entire network. Because of this tool, syndication has increased 50 percent, resulting in 25 percent more pageviews on Hearst properties.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Society is standing at the gates of what promises to be a profound transformation in the nature of work, the role of data, and the future of the world\\u2019s major industries. Intelligent machines will play a variety of roles in every sector of the economy, from the energy supply chain to legal services and manufacturing.</p>\\n<p>The common trends underlying these developments belie a far more complicated story of adoption, as each industry absorbs AI in its own way. Each industry\\u2019s idiosyncrasies suggest a unique forecast for its movement along the adoption curve.</p>\\n<p>David Beyer introduces a general framework for modeling the adoption of AI-related technologies. This model then serves to focus a discussion around the adoption of intelligent applications in the context of the law, agriculture, and manufacturing.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>According to the Identity Theft Resource Center database, more than 169,068,506 records were exposed in 2015. That stolen data has a much longer shelf life than most realize and will be used to continue the cycle of theft, deception, and fraud through one of the fastest-growing and most lucrative businesses for criminals: account takeover (<span class=\"caps\">ATO</span>) attacks.</p>\\n<p>Organized crime rings are performing account takeover at scale by using sophisticated attack techniques such as phishing, password cracking tools, and botnets. Once compromised, these user accounts are difficult to detect using traditional security solutions because they have all the attributes of legitimate accounts. These accounts are then sold in the underground Web to other crime rings, which perform a variety of downstream attacks involving retailers, financial services, reward programs, mobile games, and other consumer-facing online services.</p>\\n<p>Renowned security expert Fang Yu describes the anatomy of ATOs, detailing the scope and breadth of mass <span class=\"caps\">ATO</span> attacks using real-world examples the DataVisor team recently discovered in the wild, and explains how using real-time big data security analytics can help detect them.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>The modern threat landscape for consumer-facing websites and mobile apps, including how the underground fraud economy works and the sophisticated attack techniques fraudsters employ to compromise and exploit legitimate user accounts</li>\\n<li>Real-life case studies that describe the anatomy of ATOs and the damage they inflict on a variety of consumer-facing online services in financial, ecommerce, social networking, gaming, and other market verticals</li>\\n<li>How advances in big data technology such as Spark, HBase, and <span class=\"caps\">AWS</span> have made it possible to run advanced algorithms in real time to stop <span class=\"caps\">ATO</span> attacks before they do any damage</li>\\n<li>Why you need big data technology to do unsupervised detection algorithms at web scale to continuously analyze millions of users and billions of events per hour to pick up on <span class=\"caps\">ATO</span> attacks attempts in real time</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Traditional security tools like security information and event managers (SIEMs) are struggling to keep up with the terabytes of event data (250M to 2B events) being generated each day from an ever-growing number of devices. Cybersecurity has become a data problem, and enterprises need to reply with scalable solutions to enable effective hunting and combat evolving attacks. Rethinking the cybersecurity problem as a data-centric problem led Accenture Labs\\u2019s Cybersecurity team to use emerging big data tools along with new approaches such as graph databases and analysis to exploit the connected nature of the data to its advantage. Joshua Patterson, Michael Wendt, and Keith Kraus explain how Accenture Labs\\u2019s Cybersecurity team is using Apache Kafka, Spark, and Flink to stream data into Blazegraph and Datastax Graph to accelerate cyber defense.</p>\\n<p>Leveraging Datastax Graph and Blazegraph allows Accenture Labs to greatly accelerate query and analysis performance compared to traditional security tools like <span class=\"caps\">SIEM</span>. Josh, Michael, and Keith share the challenges of fitting cybersecurity data into each of the graph structures, as well as the ways they exploited the connectedness of events to discover new threats that would have been missed in traditional <span class=\"caps\">SIEM</span> tools. In addition, they explain how they use GPUs to accelerate graph analysis by using Blazegraph <span class=\"caps\">DASL</span>. Josh, Michael, and Keith end by demonstrating how to efficiently and effectively stream data into these graph databases using best-in-breed technologies such as Apache Kafka, Spark, and Flink and touch on why Kudu is becoming an integral part of Accenture\\u2019s technology stack. Utilizing these technologies, clients have supercharged their security analysts\\u2019 cyber-hunting abilities and are uncovering threats faster.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Kafka, developed at LinkedIn in 2010, was originally an open system to encourage adoption; developers could easily create new data streams, add data to the pipeline, and read data as it was created. It succeeded brilliantly at encouraging developers to build new data applications, improved the reliability of systems and applications, and helped LinkedIn scale its logging and data infrastructure.</p>\\n<p>Unfortunately, as Kafka usage grew at LinkedIn (and at other sites), the problems with a totally open system became apparent. Developers might inadvertently cause production problems when creating new Kafka streams, engineers might change the configuration of critical systems, and employees might get access to sensitive data. As Kafka has been adopted by larger enterprises with more complex security requirements, the Kafka community has had to rethink its architecture.</p>\\n<p>With Apache Kakfa 0.9, the community has introduced a number of features to make data streams secure. Jun Rao explains the motivation for making these changes and the threats that Kafka Security mitigates, discusses the design of Kafka security, and demonstrates how to secure a Kafka cluster. Jun also covers common pitfalls in securing Kafka and talks about ongoing security work.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>New security features in Kafka 0.9</li>\\n<li>The common usage pattern of the security feature</li>\\n<li>The access control model for Kafka</li>\\n<li>Configuring authentication, access control, and encryption</li>\\n<li>Using a secure Kafka cluster with other secure (and insecure) systems</li>\\n<li>Testing, monitoring, and tuning a secure Kafka cluster</li>\\n<li>Future work in Kafka security</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Hadoop in the cloud is becoming an increasingly common use case, as the cloud provides rapid access to flexible and low-cost IT resources. Similar to traditional on-premises Hadoop clusters, data authorization becomes more crucial than ever for the multitenant cloud. A transparent solution that decouples compute and storage is required for a simple and smooth transition. And since the underlying data is shared across the components, a unified authorization policy should be enforced to adapt the flexibility of Hadoop ecosystem.</p>\\n<p>Li Li and Hao Hao explore Apache Sentry and RecordService as a solution to address this problem. Apache Sentry is a framework to provide fine-grained authorization as a service, and RecordService is an abstraction layer between computing frameworks and data storage, which can leverage and enforce the Sentry centralized authorization policies.</p>\\n<p>Li and Hao discuss the architecture of Apache Sentry and RecordService and how the fine-grained access control policies are uniformly enforced in different Hadoop components in the cloud, such as Hive, Solr, Impala, Kafka, Sqoop2, Spark, Pig, and MapReduce, with no performance loss. They also explain how Apache Sentry can leverage the benefits of both role-based access control (<span class=\"caps\">RBAC</span>) and attribute-based access control (<span class=\"caps\">ABAC</span>).</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Apache Kudu was first announced as a public beta release at Strata <span class=\"caps\">NYC</span> 2015 and recently reached 1.0. This conference marks its one year anniversary as a public open source project. Todd Lipcon offers a very brief refresher on the goals and feature set of the Kudu storage engine, covering the development that has taken place over the last year, including new features such as improved support for time series workloads, performance improvements, Spark integration, and highly available replicated masters. Along the way, Todd explores real-world production deployments and some of the tools that have been built to help operators manage a Kudu cluster. He ends with a view of the road map of the Kudu project for the upcoming year, including plans for security and other new features.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>At P&amp;G, the Global Business Services organization delivers many shared services, including the core data infrastructure and applications from data warehouses to business intelligence and advanced analytics. Terry Mcfadden and Priyank Patel discuss Procter and Gamble\\u2019s three-year journey to enable production applications with on-cluster BI technology, exploring in detail the architecture challenges and choices made by the team along this journey, evaluation criteria, and how the final choice (Arcadia) fit with the big data infrastructure.</p>\\n<p>For the past three years, IT within the <span class=\"caps\">GBS</span> team has been working with Hadoop (Cloudera on Oracle appliances). The first year\\u2019s efforts focused on exploratory analytics and proof of concepts while the second year\\u2019s activities moved closer toward big data-based applications. Along this journey, the team identified a need for a BI tool more suited for the Hadoop ecosystem. Last year, the team embarked on an ambitious project to enable ~300 users globally to start deriving insights from 200 TB of raw data stored in Hadoop. The specific application was built using Arcadia\\u2019s Hadoop-converged BI technology that enabled the team to build an application meeting their goals.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Since its introduction in Spark 1.4, SparkR has received contributions from both the Spark community and the R community. Xiangrui Meng explores recent community efforts to extend SparkR for scalable advanced analytics\\u2014including summary statistics, single-pass approximate algorithms, and machine-learning algorithms ported from Spark MLlib\\u2014and shows how to integrate existing R packages with SparkR to accelerate existing R workflows.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Deep learning\\u2014the most significant innovation in data science in recent years\\u2014presents amazing improvements in the modeling results. However, most data scientists don\\u2019t yet use deep learning for several reasons: the relative complexity of customizing deep learning models for their own problems, challenges in installing and using the required frameworks, and low performance of open source deep learning frameworks on standard CPUs.</p>\\n<p>Amitai Armon and Nir Lotan outline a new, free software tool that enables the creation of deep learning models quickly and easily. The tool is based on existing deep learning frameworks and incorporates extensive optimizations that provide high performance on standard CPUs.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Many areas of applied machine learning require models optimized for rare occurrences, such as class imbalances, and users actively attempting to subvert the system (adversaries). The Data Innovation Lab at Capital One has explored advanced modeling techniques for just these challenges. The lab\\u2019s use case necessitated that it survey the many related fields that deal with these issues and perform many of the suggested modeling techniques. It has also introduced a few novel variations of its own.</p>\\n<p>Brendan Herger offers an introduction to the problem space and a brief overview of the modeling frameworks the Data Innovation Lab has chosen to work with, outlines the lab\\u2019s approaches, discusses the lessons learned along the way, and explores proposed future work.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Ensemble models</li>\\n<li>Deep learning</li>\\n<li>Genetic algorithms</li>\\n<li>Outlier detection via dimensionally reduction (<span class=\"caps\">PCA</span> and neural network auto-encoders)</li>\\n<li>Time-decay weighting</li>\\n<li>The synthetic minority over-sampling technique (<span class=\"caps\">SMOTE</span> sampling)</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Choice Hotels International is in the midst of a multiyear transformation that is changing key elements of its IT enterprise\\u2014replacing its monolithic central reservation system with a cloud-based, microservice-style architecture using Cassandra as the backend. A parallel project is replacing its enterprise data warehouse and reporting systems with an advanced analytics platform based on Spark and Kafka.</p>\\n<p>Jeff Carpenter describes the key role that data modeling played in helping to define the architectures of these new systems. Along the way, Jeff highlights several of the challenges Choice Hotels faced, including achieving transactional behavior across distributed services, accessing historical data from online systems, and maintaining an extensible data design when new features are added.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Using bounded contexts to identify microservices and data ownership</li>\\n<li>Defining data types for RESTful APIs and asynchronous events</li>\\n<li>Creating extensible Cassandra schema designs</li>\\n<li>Incorporating common metadata into APIs, logs, metrics, and events to enable diagnostics and analytics</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Modern data science is the creative application of scientific principles to design new tools and processes in areas where a scientific approach has been previously infeasible due to the difficulty or expense of collecting data. That\\u2019s a mouthful, but if you see data science that way, we\\u2019re likely just at the beginning. The people and things that are starting to be equipped with sensors will create data that will enable entirely new classes of problems to be approached more scientifically.</p>\\n<p>Mike Stringer looks into the future and outlines some of the issues related to data science that may arise for business, for data scientists, and for society.</p>\\n<p><strong>For businesses</strong><br/>\\nIf you\\u2019ve got a problem that you suspect can be approached scientifically, off-the-shelf solutions probably don\\u2019t exist yet. Depending on your problem and how common it is, they may never exist. Data scientist isn\\u2019t a meaningful title that most will put on their r\\xe9sum\\xe9\\u2014your future data scientists are more likely to be physicists, chemical engineers, statisticians, or economists. The most important characteristic of a data scientist is curiosity and an aptitude for the creative and rigorous application of scientific principles to new problems. First, ask yourself if there are any aspects of the new connected world that are generating data that may be relevant to your pressing problems. If the answer is yes, ask yourself whether the benefit of developing a scientific approach to solve it is worth the cost. If so, you need data science.</p>\\n<p><strong>For data scientists</strong><br/>\\nData scientists must be comfortable identifying new problems where the benefit of a data-driven approach exceeds the cost. Much of this explosion of data is nonexperimental and requires data scientists to tread extremely carefully to avoid incorrect conclusions. You also have to be able to build your own tools. In today\\u2019s climate, this means you need to be able to write code.</p>\\n<p><strong>For society</strong><br/>\\nThe same standards of ethics practiced by <a href=\"http://embor.embopress.org/content/2/9/747\">scientists</a> and <a href=\"http://www.nspe.org/resources/ethics/code-ethics\">engineers</a> still apply. Just because it\\u2019s easier to collect data now doesn\\u2019t necessarily mean it\\u2019s ethical.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Data visualizations are interactive stories that can powerfully engage audiences, giving them insight into the meanings and trends behind numbers. They often begin with a massive spreadsheet of data that has no meaning to the average person. Building an effective visualization begins by asking how data can be transformed into a compelling narrative and a dynamic user experience.</p>\\n<p>Alana Range and Brian Kahn offer a behind-the-scenes overview of the collaboration between Radish Lab and Climate Central that crunched temperature data from 1,001 US cities and created an addictive, shockingly simple interactive data visualization that engaged more than 1 million users within three days of launch. Alana and Brian explore the process of creating a successful, viral, interactive data visualization and share key perspectives from both the team that collected and collated the data (Climate Central) and the team that curated the data and reimagined it as a visualized narrative (Radish Lab) to offer insights into project dynamics from conception to launch. From data collection to data organization, from story concept to design, from development to rollout and analytics, you\\u2019ll learn how to turn your data into shareable, engaging interactive visualizations that get the traction you want.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>What kinds of datasets translate well into visualizations</li>\\n<li>How to decide which data is meaningful and which data to throw out</li>\\n<li>How to reconfigure and counterpoint datasets to create a compelling narrative</li>\\n<li>How to portray things like scale, proportionality, acceleration, geographic trends, etc.</li>\\n<li>How illustration, photography, and maps fit into visualizations</li>\\n<li>Is your visualization objective, or does it have an opinion?</li>\\n<li>How to find the most surprising story in the data</li>\\n<li>Does your visualization make users think differently about the world or mobilize them to take an action?</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Apache Flink has seen incredible growth during the last year, both in development and usage, driven, to a large extent, by the fundamental shift in the enterprise from batch to stream processing. A streaming-first architecture enables continuous processing on data that is continuously produced (as it is in most interesting datasets), enabling real-time decisions but also a simplified architecture that can subsume batch processing. Kostas Tzoumas dives into the benefits of using Flink as the central piece of such architecture. In addition, Kostas covers the latest developments in the project and the future roadmap, such as the ability to query the state in the stream processor, new libraries (e.g., <span class=\"caps\">SQL</span> and <span class=\"caps\">CEP</span>), dynamic scaling, seamless application and Flink updates, and integration between batch and streaming, which leads to radically simplified architecture and deployment. Kostas concludes with a sample of what production users of Flink are currently achieving with the system.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Simplifying the data processing stack with a microservices-based architecture while at the same time enabling real-time decisions</li>\\n<li>Processing very-high-volume streams (millions of events per second) with very low latency efficiently (in small clusters)</li>\\n<li>How Flink is unique in its ability to enable accurate, fault-tolerant, repeatable, and practical production deployment of stateful streaming applications with its support for exactly once processing, event time, and savepoints</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>The largest challenge for deep learning is scalability. With a single <span class=\"caps\">GPU</span> server, it takes hours or days to finish training. This doesn\\u2019t scale for production service; eventually you\\u2019ll need distributed training in the cloud. Google has been working on a large-scale neural network in the cloud for years and has now started sharing the power with developers.</p>\\n<p>Kazunori Sato introduces pretrained ML services, such as the Cloud Vision <span class=\"caps\">API</span> and the Speech <span class=\"caps\">API</span>, and explores how TensorFlow and Cloud Machine Learning can accelerate custom model training 10\\u201340x with Google\\u2019s distributed training infrastructure.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Amazon Kinesis is a fully managed, cloud-based service for real-time data processing over large, distributed data streams. Customers who use Amazon Kinesis can continuously capture and process real-time data such as IoT sensor data, website clickstreams, financial transactions, social media feeds, IT logs, location-tracking events, and more. Roy Ben-Alta explores the Amazon Kinesis platform in detail and discusses best practices for scaling your core streaming data ingestion pipeline as well as real-world customer use cases and design pattern integration with Amazon Elasticsearch, <span class=\"caps\">AWS</span> Lambda, and Apache Spark.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Structured Streaming is a new effort in Apache Spark to make stream processing simple without the need to learn a new programming paradigm or system. Ram Sriharsha offers an overview of Structured Streaming, discussing its support for event-time, out-of-order/delayed data, sessionization, and integration with the batch data stack to show how it simplifies building powerful continuous applications.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>There is a growing trend to use modern advanced technology in the finance industry. Information is often obtained on much larger scales, in various modalities, and from multiple dimensions, which greatly enriches the profiles of financial entities and leads to a rapid increase in the complexity of financial analytics. In the meantime, there\\u2019s increasing demand for automating the process of data statistics, feature engineering, and model tuning.</p>\\n<p>Through collaboration with some of the top payments companies around the world, Intel has developed an end-to-end solution for building fraud detection applications. Yuhao Yang explains how Intel used and extended Spark DataFrames and ML Pipelines to build the tool chain for financial fraud detection and shares the lessons learned during development.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>An overview of the overall system architecture</li>\\n<li>How to build a powerful and fast pipeline for feature derivation, selection, and transform</li>\\n<li>A deep dive into the algorithm, based on an ensemble of neural networks, which resolves difficulty from unbalanced data and outperforms other algorithms for fraud detection</li>\\n<li>Other insights and experience learned during the development and deployment</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Streaming machine learning is being integrated in Spark 2.1, but you don\\u2019t need to wait. Holden Karau and Seth Hendrickson demonstrate how to do streaming machine learning using Spark\\u2019s new Structured Streaming and walk you through creating your own streaming model. Holden and Seth will also cover how to use structured machine-learning algorithms (if they are merged by the talk). By the end of this session, you\\u2019ll have a better understanding of Spark\\u2019s Structured Streaming <span class=\"caps\">API</span> as well as how machine learning works in Spark.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Business and franchise users need access to data to generate reports and dashboards, perform analytics, and create customer-centric predictive/personalization models that assist with managing demand at Choice Hotel properties, but making data available in an accurate, timely, and reliable manner to anyone who is authorized to consume it is no easy task.</p>\\n<p>Narasimhan Sampath and Avinash Ramineni share how Choice Hotels International used Spark Streaming, Kafka, Spark, and Spark <span class=\"caps\">SQL</span> to create an advanced analytics platform that enables business users to be self-reliant by accessing the data they need from a variety of sources to generate customer insights and property dashboards and enable data-driven decisions with minimal IT engagement. Narasimhan and Avinash highlight the architecture, lessons learned, and the challenges that were overcome on both the business and technology fronts.</p>\\n<p>The analytics platform is designed as a framework to enable self-service data intake, data processing, and report/model generation by the business users. The data-driven framework consists of a distributed hybrid-cloud data ingestor for data intake and a Cloudera <span class=\"caps\">CDH</span> cluster with Spark as the distributed compute engine. The solution is built in such a way that storage and compute have been decoupled and encourages the concept of <span class=\"caps\">BYOC</span> (bring your own compute). The platform uses EC2 instances to run <span class=\"caps\">CDH</span> and leverages Amazon S3 as a data warehouse storage layer (data lake), Spark as an <span class=\"caps\">ETL</span> engine, and Spark <span class=\"caps\">SQL</span> as a distributed query engine. Results (computations/derived tables) are exposed to the end users via Spark <span class=\"caps\">SQL</span> and are discovered via Tableau. The platform supports both batch and streaming use cases and is built on the following technology stack: <span class=\"caps\">AWS</span> (S3, EC2, <span class=\"caps\">SQS</span>, <span class=\"caps\">SNS</span>), Cloudera <span class=\"caps\">CDH</span> (<span class=\"caps\">YARN</span>, Navigator, Sentry), Spark, Kafka, Spark <span class=\"caps\">SQL</span>, and Spark Streaming.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Most people are surprised to know that Spark works with Java. Maybe they saw the initial Java code that used anonymous classes and dismissed it as an ungainly mess. They were right\\u2014plain Java and Spark are ugly together. Then Java 8\\u2019s lambdas came along. Now, instead of an ungainly mess, we get the tight syntax of lambda expressions offering code that is readable and testable. Best of all, it uses Java.</p>\\n<p>Jesse Anderson demonstrates how to create Java lambdas and integrate them with Spark to process data. Then Jesse explains how to find Java resources about Spark and outlines the pros and cons for a Java developer to learn Scala.</p>\\n<p>Learning two big concepts at once is often a nonstarter. People learning Spark are often learning Scala at the same time. If we can remove one big concept (learning Scala), people will be more successful at learning Spark.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Deep learning has taken us a few steps further toward achieving AI for a man-machine interface. However, deep learning technologies like speech recognition and natural language processing remain a mystery to many. Yishay Carmiel reviews the history of deep learning, the impact it\\u2019s made, recent breakthroughs, interesting solved and open problems, and what\\u2019s in store for the future.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>A text-mining system must go way beyond indexing and search to appear truly intelligent. First, it should understand language beyond keyword matching. (For example, distinguishing between \\u201cJane has the flu,\\u201d \\u201cJane may have the flu,\\u201d \\u201cJane is concerned about the flu,\" \\u201cJane\\u2019s sister has the flu, but she doesn\\u2019t,\\u201d or \\u201cJane had the flu when she was 9\\u201d is of critical importance.) This is a natural language processing problem. Second, it should \\u201cread between the lines\\u201d and make likely inferences even if they\\u2019re not explicitly written. (For example, if Jane has had a fever, a headache, fatigue, and a runny nose for three days, not as part of an ongoing condition, then she likely has the flu.) This is a semi-supervised machine-learning problem. Third, it should automatically learn the right contextual inferences to make. (For example, learning on its own that fatigue is sometimes a flu symptom\\u2014only because it appears in many diagnosed patients\\u2014without a human ever explicitly stating that rule.) This is an association-mining problem, which can be tackled via deep learning or via more guided machine-learning techniques.</p>\\n<p>David Talby and Claudiu Branzan lead a live demo of an end-to-end system that makes nontrivial clinical inferences from free-text patient records and provides real-time inferencing at scale. The architecture is built out of open source big data components: Kafka and Spark Streaming for real-time data ingestion and processing, Spark for modeling, and Elasticsearch for enabling low-latency access to results. The data science components include a <span class=\"caps\">UIMA</span> pipeline with custom annotators, machine-learning models for implicit inferences, and dynamic ontologies for representing and learning new relationships between concepts. Source code will be made available after the talk to enable you to hack away on your own.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\" style=\"overflow: visible;\">\\n<h2>Description</h2>\\n<p>Political analysts may once have depended entirely on subjective attributes, such as ethics, charisma, and nonscientific impressions of the electorate to forecast elections, but with the rise of data generated from human daily interaction with software systems, it\\u2019s possible to add meaningful data-driven attributes to political forecasting alongside all of the demographic information available to today\\u2019s political consultants.</p>\\n<p>Amir Hajian, Khaled Ammar, and Alex Constandache offer a data scientific approach to mining more than 100 million responses to survey questions related to public opinion collected by <a href=\"http://polling.reuters.com/\">Thomson Reuters\\u2019s polling website</a> over the past five years. The method uses machine learning, natural language processing, and deep learning on an infrastructure that includes Spark and Elasticsearch, which serves as the backbone of the mobile game <em>White House Run</em> that is aimed at increasing political awareness of the US political context. The game allows players to participate as their own candidate in the current US presidential race by answering questions about certain topics. (Domain experts in politics and journalism have picked the topics and designed wording of the questions to ensure accuracy of the input data.) Using these answers, the method performs a statistical analysis of comparisons between a given opinion and the mined opinion data. As a result, the app returns the electability of the user as an election candidate, based on the stances they have taken as well as demographic analysis of their supporters.</p>\\n<p>Amir, Khaled, and Alex briefly introduce the infrastructure behind collecting poll answers, describe the science behind the <em>White House Run</em> app as well as the methods behind their data-driven approach, and share the lessons learned along the way. They conclude with a discussion of the model for computing the electability score and characterizing the statistical populations of supporters based on various similarity and distance measures and the applicability of the techniques described beyond the current US presidential contest.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Predictive maintenance is about anticipating a failure and taking preemptive action. With the recent advances in accessible machine learning and cloud storage, there is tremendous opportunity to utilize the entire gamut of data coming from factories, buildings, machines, and sensors to not only monitor the health of equipment but also predict when it is likely to malfunction or fail. However, as simple as it sounds in principle, in reality the data required to actually make a prediction in advance and in a timely manner is hard to come by. The data that is collected is often incomplete, partial, or just not enough, making it unsuitable for modeling.</p>\\n<p>In the realm of predictive maintenance, the event of interest is an equipment failure. In real scenarios, this is usually a rare event. Ideally, the data should have hundreds or even thousands of failures. However, unless the data collection has been taking place over a long period of time, the data will have very few of these events or, in the worst case, none at all. But even in these cases, the distribution or the ratio of failure to nonfailure data is highly skewed.</p>\\n<p>Modeling for failure thus often falls under the classic problem of modeling with imbalanced data when only a fraction of the data constitutes failure. Standard methods for feature selection and feature construction do not work so well for imbalanced data. Moreover, the metrics used to evaluate the model can be misleading. Danielle Dean and Shaheen Gauher discuss the best ways to build and evaluate models, offering examples that reference sample code in regular open source R as well as Microsoft R Server, which allows the computations to be done on big data. Danielle and Shaheen explain why a clear understanding of business requirements and tolerance to false negative and false positives is necessary. For example, for some businesses, failure to predict a malfunction can be extremely detrimental (e.g., aircraft engine failure) or exorbitantly expensive (e.g., production shutdown in a factory), while for others falsely predicting a failure when there is none leads to a significant loss of time and resources. In the language of statistics, this is what we call misclassification cost. Danielle and Shaheen conclude by illustrating how to deal with imbalanced data through two predictive maintenance example case studies.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>For M&amp;A speculators, intellectual property managers, and attorneys, the \\u201choly grail\\u201d for patents is to be able to accurately assess the value of an individual invention and aggregate up to portfolios. Ignoring the softer side of a patent\\u2019s value (i.e., its power to halt competitive development and/or neutralize the threat of enforcement), there are few liquidity events\\u2014specifically litigation damages/settlements, portfolio acquisitions, and licensing fees\\u2014that allow assignees to monetize individual patents; none of these typically make the associated data available to the research community.</p>\\n<p>How can the value of a patent be quantified? Josh Lemaitre explores how Thomson Reuters Labs approached this problem by applying machine learning to the patent corpus in an effort to predict those most likely to be enforced via litigation, diving into how TR Labs trained a backtesting-inspired Bayesian classifier to predict future litigation events of individual patents using the currently in-force US patent corpus combined with the litigation dockets of all US patent infringement cases with an open source technology stack.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Data procurement</li>\\n<li>Infrastructure selection</li>\\n<li>Feature extraction and extraction</li>\\n<li>Model selection and assessment</li>\\n<li>The presentation layer</li>\\n<li>Project exhaust</li>\\n</ul>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Martin Wicke and Josh Gordon field questions related to their tutorial, Deep Learning with TensorFlow. Ask them about TensorFlow, machine learning (and deep learning in particular), deploying ML in products, and their favorite resources to learn more about ML or any other question you might have.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Mark Grover, Jonathan Seidman, and Ted Malaska, the authors of <a href=\"http://shop.oreilly.com/product/0636920033196.do\"><em>Hadoop Application Architectures</em></a>, participate in an open Q&amp;A session on considerations and recommendations for the architecture and design of applications using Hadoop. Come with questions about your use case and its big data architecture or just listen in on the conversation.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Join Apache Beam and Google Cloud Dataflow engineers to ask all of your questions about stream processing. They\\u2019ll answer everything from general streaming questions about concepts, semantics, capabilities, limitations, etc. to questions specifically related to Apache Beam, Google Cloud Dataflow, and other common streaming systems (Flink, Spark, Storm, etc.).</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Join Xiangrui Meng and Ram Sriharsha to discuss the state of Spark.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>John Akred, Stephen O\\u2019Sullivan, and Julie Steele will field a wide range of detailed questions about developing a modern data strategy, architecting a data platform, and best practices for <span class=\"caps\">CDO</span> and its evolving role. Even if you don\\u2019t have a specific question, join in to hear what others are asking.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>VoltDB promises full <span class=\"caps\">ACID</span> with strong serializability in a fault-tolerant, distributed <span class=\"caps\">SQL</span> platform, as well as higher throughput than other systems that promise much less. But why should users believe this?</p>\\n<p>VoltDB has built a culture where data safety, correctness, and consistency are the highest priorities. John Hugg covers some specific scenarios where trade-offs have been made in support of these goals. Anyone can say their product makes no compromises, but it\\u2019s precisely the compromises that expose engineering values. John discusses VoltDB\\u2019s internal development, support, and testing processes and explores how VoltDB has improved over the years, how it tries to identify blindspots, and what kind it has planned for the future.</p>\\n<p>John concludes by diving into third-party validation, with a focus on Kyle Kingsbury\\u2019s recent Jepsen test, which analyzed VoltDB and published the results in a nonbiased report. John offers an overview of this process and its ramifications.</p>\\n<p><strong>This session is sponsored by VoltDB.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Using Hadoop and other big data technologies, the YP Analytics application allows advertisers and media and advertising consultants to understand their digital presence and <span class=\"caps\">ROI</span>. Richard Langlois explains how Yellow Pages (YP) used this expertise for an internal use case that delivers real-time analytics with Tableau, using <span class=\"caps\">OLAP</span> on Hadoop and enabled by its stack (<span class=\"caps\">HDFS</span>, Parquet, Hive, Impala, and AtScale).</p>\\n<p>Yellow Pages\\u2019 first big data analytics use case, the YP Analytics application, uses Hadoop (Cloudera) and other big data technologies to help YP\\u2019s 244,000 advertisers understand their digital presence (ranking) and <span class=\"caps\">ROI</span> with regard to the products and services they use with YP. With the delivery of YP Analytics, YP realized that its nationwide media and advertising consultants (<span class=\"caps\">MAC</span>) needed the same information when meeting the advertisers. The MACs were and are still using a different sales application called Compass. In order to ensure information consistencies between these two applications, built by different teams and technologies, the YP team created a series of data services that can be used by any consuming applications, such as YP Analytics and Compass.</p>\\n<p>The successes of these applications led YP\\u2019s internal teams to ask, \\u201cWhat about us?\\u201d For YP Analytics and Compass, all queries were known in advance and always in the context of a merchant or an account, which allowed the team to do multiple optimizations. However, these optimizations were not great for different internal ad hoc queries with other contexts than a merchant or an account, so YP decided to use <span class=\"caps\">OLAP</span> on Hadoop. Richard offers an overview of the stack that has enabled <span class=\"caps\">OLAP</span> on Hadoop (with more than 75 billion rows in production). The stack includes <span class=\"caps\">HDFS</span>, Parquet, Hive, Impala, and AtScale for incredibly fast, real-time analytics and data exploration through Tableau, the tool chosen by YP\\u2019s end users. Richard also describes other recent use cases in advanced analytics for marketing campaign automation and sales recommendation engines using Spark, as well as recent work on reducing data analytics silos and experiments with search-based analytics.</p>\\n<p><strong>This session is sponsored by Tableau Software.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>We all hear stories about the potential value of big data analytics, but it\\u2019s important to understand that big data analytics is a journey of introspection, operational excellence, and new, winning strategies. John Morrell leads a panel of practitioners from Dell, National Instruments, and Citi\\u2014companies that are gaining real value from big data analytics\\u2014as they explore their companies\\u2019 big data journeys, offering lessons learned and best practices. Join John to hear real-world stories from organizations that are seeing concrete results and learn how analytics can answer groundbreaking new questions about business and create a path to becoming a data-driven organization.</p>\\n<p><strong>This session is sponsored by Datameer.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Haoyuan Li offers an overview of Alluxio (formerly Tachyon), a memory-speed virtual distributed storage system. The Alluxio open source community is one of the fastest growing open source communities in big data history with more than 250 developers from over 50 organizations around the world, and the Alluxio system has been deployed at a number of companies, including Alibaba, Baidu, Barclays, Intel, Huawei, and Qunar. In some of these deployments, Alluxio has been running in production for over a year, managing PBs of data.</p>\\n<p>In the past year, the Alluxio project experienced a tremendous improvement in performance and scalability and was extended with key new features including tiered storage, transparent naming, and unified namespace. At the same time, the Alluxio ecosystem has expanded to include support for more under storage systems and computation frameworks, including Amazon S3, Google Cloud Storage, Gluster, Ceph, <span class=\"caps\">HDFS</span>, <span class=\"caps\">NFS</span>, and OpenStack Swift. These integrations make it possible to leverage Alluxio in many different environments.</p>\\n<p>This year, the goal is to make Alluxio accessible to an even wider set of users through a focus on security, new language bindings, and further increased stability. In addition, the team is working on new APIs to allow applications to access data more efficiently and manage data across different under storage systems.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Operational data stores (<span class=\"caps\">ODS</span>) serve as a data staging area between transactional databases and data warehouses. Data from multiple sources are integrated, cleansed, and prepped in the <span class=\"caps\">ODS</span> before populating a data warehouse for long-term storage and analytics. Traditional <span class=\"caps\">ODS</span> systems encounter severe challenges when it comes to dealing with the wide variety and massive volume of data common to data warehouses built on top of the Hadoop platform. It\\u2019s time to rethink the requirements and the architecture for the next generation of an <span class=\"caps\">ODS</span> on top of Hadoop. Starting from first principles, Vinayak Borkar defines the requirements for a modern operational data store and explores some possible architectures to support those requirements.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>The amount of cutting-edge technology that Azure puts at your fingertips is incredible. Tasks like building a web server or integration workflow that used to take weeks can now be accomplished in seconds, and bundling Azure services allows developers to create solutions that, even a few months ago, would have seemed out of reach. Artificial intelligence is no exception. Azure enables sophisticated capabilities in artificial intelligence, machine learning, deep learning, cognitive services, and advanced analytics. Rimma Nehme explains why Azure is the next AI supercomputer and how this vision is being implemented in reality. Along the way, Rimma explains how to use Azure to process data at any scale and how to compose tools such as Spark and R and do scale-out querying on demand at massive scale.</p>\\n<p><strong>This session is sponsored by Microsoft.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>BigQuery provides petabyte-scale data warehousing with consistently high performance for all users. However, users coming from traditional enterprise data warehousing platforms often have questions about how best to adapt their workloads for BigQuery. Chad Jennings explores best practices and integration with BigQuery with special emphasis on loading and transforming data for BigQuery, as well as how BigQuery integrates with the rest of the Google Cloud Platform\\u2014including Apache Spark on Google Cloud Dataproc. Chad also discusses Big Query\\u2019s <span class=\"caps\">SQL</span> dialect and its ability to handle the industry\\u2019s most common queries.</p>\\n<p><strong>This session is sponsored by Google.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Organizations from small startups to large enterprises are increasingly using open source frameworks such as Apache Hadoop, Spark, and Presto to address a broad range of analytic use cases, including business intelligence, stream processing, and machine learning. However, with any big data project comes the risk of uncapped costs, delayed timelines, expensive infrastructure, and difficult choices about where to focus in the open source toolset.<br/>\\n\\xa0<br/>\\nJonathan Fritz explains how organizations are deploying these and other big data frameworks with Amazon Web Services (<span class=\"caps\">AWS</span>) and how you too can quickly and securely run Spark and Presto on <span class=\"caps\">AWS</span>. Jonathan demonstrates how to lower costs and accelerate deployment of big data applications, using Amazon <span class=\"caps\">EMR</span> to easily create a Hadoop cluster running Spark and Presto and querying data in Amazon S3 using <span class=\"caps\">ANSI</span> <span class=\"caps\">SQL</span>. Jonathan then explores how you can use Amazon S3 as a highly scalable, durable, and secure data lake by decoupling compute from storage, before outlining best practices to lower costs using Amazon EC2 Spot Instances and discussing how to secure your clusters using AWS\\u2019s extensive security capabilities.</p>\\n<p><strong>This session is sponsored by Amazon.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Join Max Shron, former consultant on data science and current head of Warby Parker\\u2019s data science team, for a Q&amp;A all about data science consulting. Bring your questions about getting into the data science consulting business (or your questions about how to transition from consulting to something new). Even if you don\\u2019t have questions, join in to hear what others are asking.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Join Apache Kafka cocreator and <span class=\"caps\">PMC</span> chair Jun Rao and Apache Kafka committer and architect of Kafka Connect Ewen Cheslack-Postava for a Q&amp;A session about Apache Kafka. Bring your questions about Kafka internals or key considerations for developing your data pipeline and architecture, designing your applications, and running in production with Kafka. Even if you don\\u2019t have a specific question, join in to hear what others are asking.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Rajesh Shroff reviews the big data and analytics landscape, lessons learned in enterprise over the last few years, and some of the key considerations while designing a big data systems. Rajesh also highlights differentiations in the Cisco Unified Computing Systems (<span class=\"caps\">UCS</span>) platform, outlines Cisco\\u2019s big data ecosystem partnerships and certified Cisco-validated designs (CVDs), and demonstrates what <span class=\"caps\">UCS</span> Director Express can bring to managing your big data environment by significantly increasing productivity and reducing operational cost.</p>\\n<p><strong>This session is sponsored by Cisco.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Big data and Hadoop are a critical part of the data fabric of companies; as such, proper information governance is key in order to support data-driven applications that extend line-of-business processes, radically transforming industry-specific solutions. With big data and Hadoop a general-purpose, reusable resource, developers and administrators need to meet the critical enterprise adoption criteria of correctness, quality, consistency, compliance, and traceability. Big data solutions and the quality of data in data lakes should not generate additional risk to the business or be a roadblock to application development and user adoption. These solutions must meet the highest levels of enterprise information governance, compliance, and regulation without stifling the democratization, agility, and openness promised by big data.</p>\\n<p>Michael Eacrett discusses the convergence of enterprise information governance and big data and demonstrates how developers and business users are deploying and using <span class=\"caps\">SAP</span> solutions for enterprise information management. Mike explores how customers are integrating enterprise data stores, systems of record transactional systems, and master data management systems together, asynchronously and in real time, with big data infrastructures to actively solve these information governance and data-quality challenges. Join Mike and turn your data swamp back into a beautiful trusted enterprise data lake.</p>\\n<p><strong>This session is sponsored by <span class=\"caps\">SAP</span>.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>In this new world order, data collection must come with a corporate responsibility to protect data. Sometimes this is a legal requirement, as in the EU\\u2019s data protection regulation (aka <span class=\"caps\">GDPR</span>), Russia\\u2019s federal law on personal data, and Germany\\u2019s Bundesdatenschutzgesetz (<span class=\"caps\">BDSG</span>), but many times, it\\u2019s only a social responsibility, a quite complicated and gray area\\u2014it\\u2019s all about what you feel is \\u201cright.\\u201d</p>\\n<p>While sharing or selling your valuable data can be risky due to data privacy regulations and IP considerations, it can also generate revenue or help nonprofits succeed at world-changing missions. Steve Touw explores real-world examples of how a proper data architecture enables philanthropic missions and offers ideas for how to better share your data.</p>\\n<p>A well-built governance strategy creates a workflow for the creation of advanced analytics with data privacy at the core of the design. Steve explains why up-front data governance is key; designing models/analytics and then going back to add data privacy controls is much more difficult, sometimes impossible, and always very risky.</p>\\n<p>Topics include:</p>\\n<ul>\\n<li>Dynamic policies built into disparate and changing data sources</li>\\n<li>A low barrier to entry and common access layer to enforce policies, control data access, and audit all actions</li>\\n<li>Using technology to enable scaling of \\u201cdata knowledge\\u201d</li>\\n</ul>\\n<p><strong>This session is sponsored by Immuta.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Join DJ Patil and Lynn Overmann to ask your questions about data science at the White House.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>With so much variance across Hadoop distributions, ODPi was established to create standards for both Hadoop components and testing applications on those components. The Linux Foundation\\u2019s John Mertic offers an overview of ODPi and its benefits before IBM\\u2019s Berni Schiefer discusses applications such as <span class=\"caps\">IBM</span> Big <span class=\"caps\">SQL</span> that have been designed to work with ODPi-compliant Hadoop distributions. Join in to learn how application developers and companies considering Hadoop can benefit from ODPi.</p>\\n<p><strong>This session is sponsored by <span class=\"caps\">IBM</span>.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Big data offers the possibility of deep insights into marketing programs and business operations, as well as a 360-degree view of customers and competitors. For many enterprises, being able to achieve such results means combining new and traditional data sources and modernizing <span class=\"caps\">ETL</span> and data warehouse applications. These tasks seem easy during research and proof-of-concept phases, but complexity grows exponentially when moving toward enterprise-grade implementations.</p>\\n<p>Using real-world examples, Joe Goldberg explains why freeware isn\\u2019t free when it comes to managing Hadoop workflows for enterprise implementations. Joe explores how companies like GoPro, Produban, Navistar, and others have taken a platform approach to managing their workflows; how they are using workflows to power data ingest, <span class=\"caps\">ETL</span>, and data integration processing; how an end-to-end view of workflows has reduced issue resolution time; and how these companies are achieving success in their data warehouse modernization projects.</p>\\n<p><strong>This session is sponsored by <span class=\"caps\">BMC</span>.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Scott Gnau provides unique insights into the tipping point for data, how enterprises are now rethinking everything from their IT architecture and software strategies to data governance and security, and the cultural shifts CIOs must grapple with when supporting a business using real-time data to scale and grow.</p>\\n<p><strong>This session is sponsored by Hortonworks.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Ready to take a deeper look at how Hadoop and its ecosystem has a widespread impact on analytics? Douglas Liming explains where <span class=\"caps\">SAS</span> fits into the open ecosystem, why you no longer have to choose between analytics languages like Python, R, or <span class=\"caps\">SAS</span>, and how a single, unified open analytics architecture empowers you to literally have it all.</p>\\n<p>Doug details a case study involving one of the world\\u2019s largest health agencies. Dependent on donations and fundraising efforts, this agency suffered under the weight of data silos and data quality issues. Answering relevant business questions was both difficult and time consuming. With help from <span class=\"caps\">SAS</span> and an open analytics ecosystem, the company now makes data-driven decisions based on previously undetectable patterns in data; as a result, employees spend less time generating reports, allowing more staff to work directly with patients, researchers, donors, and participants.</p>\\n<p>Doug also explores how embracing Docker container technology can help you provide a flexible infrastructure with all the tools and methods you need in one place. Join in to learn how strategic placement of containers helps push work inside the Hadoop cluster, minimizing data movement, minimizing time to insight, and maximizing the value of robust analytics.</p>\\n<p><strong>This session is sponsored by <span class=\"caps\">SAS</span>.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Launched in late 2015, Cigna\\u2019s enterprise data lake project is taking the company on a data governance journey. Using Podium as the software platform for its data lake, Cigna\\u2019s data Management and Governance teams are eliminating silos of activity and creating a common thread of activity\\u2014based on a shared platform of metadata\\u2014to connect and align activities across technical teams and business processes.</p>\\n<p>For the first time ever, all stakeholders are providing true data governance. By empowering Cigna\\u2019s Data Architecture, Data Modeling, and Data Governance teams to efficiently and transparently work together around a shared set of information, the data lake is reducing risk and cost in the data management process. Activities like data profiling entire sets of data and matching data definitions to over 18,000 data elements, which historically would take 40+ days to complete, are now finished inside of a week. Similarly the task of joining data from multiple legacy sources, which previously took three weeks, can now happen in the data lake in less than an hour.</p>\\n<p>Sherri Adame offers an overview of the project, providing insights into some of the business pain points and key drivers, how it has led to organizational change, and the best practices associated with Cigna\\u2019s new data governance process. Along the way, Sherri highlights some of the lessons learned and shares the next-step deployment plans for the data lake to Cigna\\u2019s business community.</p>\\n<p><strong>This session is sponsored by Podium Data.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>While traditional methods may be proficient to collect and analyze uniform data, utilizing multiple structured and unstructured external data sources can be challenging. Joe Caserta explains how one of the largest membership interests groups in the country makes sense of the influx of information from streaming external data sources. This challenge is exciting because aside from collecting data from its ~40 million members, the  group also needs to monitor digital and traditional interactions cohesively to predict and optimize a member\\u2019s path to purchase.</p>\\n<p>Path-to-purchase analytics is at the core of the solution to segment and individualize potential member interactions on- and offline and increase high-value member loyalty. Joe outlines the architecture of the ingestion, data lake, data science, and data warehouse components built on <span class=\"caps\">AWS</span> and Spark and discusses how his team designed and implemented a data lake in S3, <span class=\"caps\">ETL</span> in Spark, member matching with GraphFrames, and a DW in Redshift to help revolutionize the way this membership interest group uses its data to become an analytics-driven company. You\\u2019ll learn how organize data within the lake to encourage data science experimentation and create models to increase a lasting engagement with your members.</p>\\n<p><strong>This session is sponsored by Caserta Concepts.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<h2>Description</h2>\\n<p>Mariusz G\\u0105darowski offers an overview of Neptune, deepsense.io\\u2019s new IT platform-based machine-learning experiment management solution for data scientists. <a href=\"http://deepsense.io/\">Deepsense.io</a> uses technologies such as Theano, TensorFlow, Lasagne, scikit-learn, and Apache Spark to carry out machine-learning tasks. Neptune seamlessly integrates with these technologies and makes them easier to use. Neptune enhances the management of machine-learning tasks such as dependent computational processes, code versioning, comparing achieved results, monitoring tasks and progress, sharing infrastructure among teammates, and many others.</p>\\n<p>Deepsense.io began working on Neptune as part of the Kaggle machine-learning competition Automated Right Whale Recognition\\u2014a computer vision contest which it won earlier this year. Following the competition, the team discovered that its data scientists frequently face similar problems and in response developed the Neptune technology. Now, deepsense.io wants to make Neptune available to everyone. Mariusz explores Neptune\\u2019s functionalities, shows how to use it, and outlines the benefits it can bring to you and your team of data scientists. Join Mariusz to learn more about Neptune and maybe even become an early adopter of this exciting new technology.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Strata + Hadoop World program chairs Roger Magoulas, Doug Cutting, and Alistair Croll welcome you to the second day of keynotes.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>In a few short years, cloud has risen from an aspirational concept to an enterprise mandate. As workloads move to the cloud, a new set of concerns in the data center are emerging, including data security, portability, and governance. Cloudera <span class=\"caps\">CEO</span> Tom Reilly and James Powell, global <span class=\"caps\">CTO</span> of Nielsen, discuss the dynamics of Hadoop in the cloud, what to consider at the start of the journey, and how to implement a solution that delivers flexibility while meeting key enterprise requirements.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>When Hollywood portrays artificial intelligence, it\\u2019s either a demon or a savior. But the reality is that AI is far more likely to be an extension of ourselves. Strata program chair Alistair Croll looks at the sometimes surprising ways that machine learning is insinuating itself into our everyday lives.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Will machine learning give us better eyesight? Join Joseph Sirosh for a surprising story about how machine learning, population data, and the cloud are coming together to fundamentally reimagine eye care in one of the world\\u2019s most populous countries, India.</p>\\n<p><strong>This keynote is sponsored by Microsoft.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>The Panama Papers investigation revealed the offshore holdings and connections of dozens of politicians and prominent public figures around the world and led to high-profile resignations, police raids, and official investigations. Almost 500 journalists, coordinated by the International Consortium of Investigative Journalists and S\\xfcddeutsche Zeitung, had to sift through 2.6 terabytes of data\\u2014the biggest leak in the history of journalism. Mar Cabra explains how technology made it all possible.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>The need to quickly acquire, process, prepare, store, and analyze data has never been greater. The need for performance crosses the big data ecosystem too\\u2014from the edge to the server to the analytics software, speed matters. Raghunath Nambiar shares a few use cases that have had significant organizational impact where performance was key.</p>\\n<p><strong>This keynote is sponsored by Cisco.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Chad W. Jennings demonstrates the power of BigQuery through an exciting demo and announces several new features that will make BigQuery a better home for your enterprise big data workloads.</p>\\n<p><strong>This keynote is sponsored by Google.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Keynote by DJ Patil and Lynn Overmann</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Data has long stopped being structured and flat, but the results of our analysis are still rendered as flat bar charts and scatter plots. We live in a 3D world, and we need to be able to enable data interaction from all perspectives. Robert Thomas offers an overview of Immersive Visualization\\u2014integrated with notebooks and powered by Spark\\u2014which helps bring insights to life.</p>\\n<p><strong>This keynote is sponsored by <span class=\"caps\">IBM</span>.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>We have more data than ever before, by many orders of magnitude, yet \\u201cstrong\\u201d artificial intelligence remains elusive. Some notorious difficult problems like speech recognition and the game of Go have recently seen spectacular advances, yet no machine can understand language as well as three-year-old child. Gary Marcus explores the gap between what machines do well and what people do well and what needs to happen before machines can match the flexibility and power of human cognition.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Hadoop and its ecosystem mean new possibilities for analytics. We\\u2019ve shifted from sample-based assessments to all-inclusive investigations. The result? Deeper insights and more relevant actions. Paul Kent offers an overview of SAS\\u2019s participation in open platforms and introduces <span class=\"caps\">SAS</span> Viya, a new unified and open analytics architecture that lets you scale analytics in the cloud and code as you choose.</p>\\n<p><strong>This keynote is sponsored by <span class=\"caps\">SAS</span>.</strong></p>\\n</div>],\n",
       " [<div class=\"en_session_description description\" style=\"overflow: visible;\">\\n<p>Although 2016 is a highly unusual political year, elections and public opinion follow predictable statistical properties. Sam Wang explains how the presidential, Senate, and House races can be tracked and forecast from freely available polling data using tools from statistics and machine learning. These approaches offer a deeper understanding of the US political scene, even under extreme circumstances.</p>\\n</div>],\n",
       " [<div class=\"en_session_description description\">\\n<p>Birds of a Feather (BoF) discussions are a great way to informally network with people in similar industries or interested in the same topics.</p>\\n<p>BoFs will happen during lunch on Wednesday and Thursday. <strong>Create your own topic</strong> at the sign-up board near Registration <strong>or join one of the industry tables</strong> below.</p>\\n<div class=\"event-photo-cluster\">\\n<img alt=\"Birds of a Feather\" src=\"http://cdn.oreillystatic.com/oreilly/conferences/events/stratany2015/bof1.png\"/>\\n<img alt=\"Birds of a Feather\" src=\"http://cdn.oreillystatic.com/oreilly/conferences/events/stratany2015/bof2.jpg\"/>\\n<img alt=\"Birds of a Feather\" src=\"http://cdn.oreillystatic.com/oreilly/conferences/events/stratany2015/bof3.jpg\"/>\\n</div>\\n<p>This year\\u2019s Industry Birds of a Feather discussion topics include:</p>\\n<ul>\\n<li>Advertising &amp; Marketing</li>\\n<li>Energy</li>\\n<li>Finance</li>\\n<li>Government &amp; Policy</li>\\n<li>Healthcare</li>\\n<li>Media &amp; Entertainment</li>\\n<li>Retail &amp; Ecommerce</li>\\n<li>Telecommunications</li>\\n</ul>\\n</div>]]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize list\n",
    "div_desc = []\n",
    "\n",
    "# initialize web driver to get dynamic page source\n",
    "driver = webdriver.Chrome(\"C:/Users/ffarmer/Downloads/chromedriver.exe\")\n",
    "\n",
    "# add session descriptions to list\n",
    "for path in sessions_df['link']: # for each session link\n",
    "    driver.get(domain + path) # open link\n",
    "    page_html = driver.page_source.encode('utf-8') # save page source\n",
    "    soup = bs4.BeautifulSoup(page_html, 'lxml') # parse html\n",
    "    div_desc.append(soup.find_all('div', class_='en_session_description description')) #save descriptions, append to list\n",
    "\n",
    "# kill driver\n",
    "driver.quit()\n",
    "\n",
    "# confirm results\n",
    "div_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 193 descriptions\n"
     ]
    }
   ],
   "source": [
    "# initialize list\n",
    "descriptions = []\n",
    "\n",
    "# loop through description divs to get <p> contents\n",
    "# Note: some of the descriptions are broken up over multiple <p> tags\n",
    "# so have to combine them, checking for embedded tags within <p>\n",
    "for res_set in div_desc: # loop through ResultSets\n",
    "    phrases = res_set[0].p.contents # get the <p> contents from the div\n",
    "\n",
    "    desc = '' # initialize string for description\n",
    "    if len(phrases) > 1: # if description has multiple <p>\n",
    "        for i in phrases: # loop through each <p>\n",
    "            if isinstance(i, bs4.element.Tag) == True: # if <p> has embedded tags\n",
    "                if len(i.contents) == 0: # check if embedded tag has contents\n",
    "                    desc += '' # if so, add nothing\n",
    "                else:\n",
    "                    desc += str(i.contents[0].encode('utf-8')) # otherwise, add contents of embedded tag\n",
    "            else:\n",
    "                desc += str(i.encode('utf-8')) # add contents\n",
    "    else:\n",
    "        desc += str(phrases[0].encode('utf-8')) # add contents\n",
    "\n",
    "    descriptions.append(desc) # add combined description to list\n",
    "    \n",
    "# confirm results, should be 193 (total sessions)\n",
    "print 'Parsed ' + str(len(descriptions)) + ' descriptions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>topic</th>\n",
       "      <th>descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Parallel SQL and analytics with Solr</td>\n",
       "      <td>Analytics has increasingly become a major focu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>JupyterLab: The evolution of the Jupyter Notebook</td>\n",
       "      <td>Project Jupyter provides building blocks for i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Designing a location intelligence platform for...</td>\n",
       "      <td>CartoDB has enabled hundreds of thousands of u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>The future of column-oriented data processing ...</td>\n",
       "      <td>In pursuit of speed and efficiency, big data p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/strata/hadoop-big-data-ny/public/schedule/det...</td>\n",
       "      <td>Beyond Hadoop at Yahoo: Interactive analytics ...</td>\n",
       "      <td>Yahoo initially built Hadoop as an answer to a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "1  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "2  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "3  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "4  /strata/hadoop-big-data-ny/public/schedule/det...   \n",
       "\n",
       "                                               topic  \\\n",
       "0               Parallel SQL and analytics with Solr   \n",
       "1  JupyterLab: The evolution of the Jupyter Notebook   \n",
       "2  Designing a location intelligence platform for...   \n",
       "3  The future of column-oriented data processing ...   \n",
       "4  Beyond Hadoop at Yahoo: Interactive analytics ...   \n",
       "\n",
       "                                        descriptions  \n",
       "0  Analytics has increasingly become a major focu...  \n",
       "1  Project Jupyter provides building blocks for i...  \n",
       "2  CartoDB has enabled hundreds of thousands of u...  \n",
       "3  In pursuit of speed and efficiency, big data p...  \n",
       "4  Yahoo initially built Hadoop as an answer to a...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add descriptions to dataframe\n",
    "sessions_df['descriptions'] = descriptions\n",
    "\n",
    "# confirm results\n",
    "sessions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ffarmer\\\\Documents\\\\Code\\\\strata-notes_text-analysis\\\\data_train'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change directory to save file\n",
    "os.chdir('data_train')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save dataframe as pickle\n",
    "sessions_df.to_pickle('strata_sessions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
